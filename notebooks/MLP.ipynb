{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_in_the_prediction = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    " '41','42','44','45','46','47','48','49','50','51','53','54','55','56']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_df = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations.csv\")\n",
    "truth_df = truth_df[truth_df['date'] >= '2022-01-01']\n",
    "truth_df = truth_df[truth_df['location'] != 'US']\n",
    "truth_df = truth_df[truth_df['location'].isin(states_in_the_prediction)]\n",
    "truth_df.sort_values(by=['date', 'location'], inplace=True)\n",
    "unique_dates = truth_df['date'].unique()\n",
    "unique_states = truth_df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardize the data individually for each state\n",
    "mean_arr = []\n",
    "std_arr = []\n",
    "for state in unique_states:\n",
    "    state_values = truth_df[truth_df['location'] == state]['value'].values\n",
    "    mean = state_values.mean()\n",
    "    mean_arr.append(mean)\n",
    "    std = state_values.std()\n",
    "    std_arr.append(std)\n",
    "    truth_df.loc[truth_df['location'] == state, 'norm_value'] = (state_values - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = np.zeros([len(unique_dates),len(unique_states)])\n",
    "for id1,i in enumerate(unique_dates):\n",
    "    for id2,j in enumerate(unique_states):\n",
    "        weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['value'].values\n",
    "        # weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['norm_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "loader_temp = weeks.copy().transpose(1,0)\n",
    "train_data = loader_temp[:,:-2].copy() #train_data = loader_temp[:,:-4].copy()\n",
    "train_data = np.concatenate([train_data[:,i:i+10] for i in range(0, 46, 1)], axis = 0)\n",
    "np.random.shuffle(train_data)\n",
    "val_data = train_data[:500]\n",
    "train_data = train_data[500:]\n",
    "test_data = loader_temp[:,-8:].copy() # test_data = loader_temp[:,-10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to PyTorch tensors\n",
    "# train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "# train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "# val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "# test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "mean = train_inputs.mean()\n",
    "std = train_inputs.std()\n",
    "\n",
    "train_inputs = (train_inputs - mean) / std\n",
    "val_inputs = (val_inputs - mean) / std\n",
    "train_labels = (train_labels - mean) / std\n",
    "val_labels = (val_labels - mean) / std\n",
    "test_inputs = (test_inputs  - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
    "val_dataset =torch.utils.data.TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "# create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save\n",
    "# with open('../data/shuffled_data/MLP_flu_train.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_dataset, handle)\n",
    "# with open('../data/shuffled_data/MLP_flu_val.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_dataset, handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP on residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMLP(nn.Module):\n",
    "    def __init__(self,input_length, output_length,hidden_length):\n",
    "        super(AutoMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, output_length),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 0.789 Validation loss : 1.689\n",
      "Epoch 11 Training loss: 0.524 Validation loss : 0.655\n",
      "Epoch 21 Training loss: 0.477 Validation loss : 0.650\n",
      "Epoch 31 Training loss: 0.467 Validation loss : 0.674\n",
      "Epoch 41 Training loss: 0.426 Validation loss : 0.647\n",
      "Epoch 51 Training loss: 0.390 Validation loss : 0.664\n",
      "Epoch 61 Training loss: 0.348 Validation loss : 0.592\n",
      "Epoch 71 Training loss: 0.333 Validation loss : 0.631\n",
      "Epoch 81 Training loss: 0.286 Validation loss : 0.615\n",
      "Epoch 91 Training loss: 0.259 Validation loss : 0.635\n",
      "Epoch 101 Training loss: 0.251 Validation loss : 0.699\n",
      "Epoch 111 Training loss: 0.217 Validation loss : 0.878\n",
      "Epoch 121 Training loss: 0.202 Validation loss : 0.742\n",
      "Epoch 131 Training loss: 0.195 Validation loss : 0.775\n",
      "Epoch 141 Training loss: 0.181 Validation loss : 0.796\n",
      "Epoch 151 Training loss: 0.168 Validation loss : 0.957\n",
      "Epoch 161 Training loss: 0.164 Validation loss : 0.987\n",
      "Epoch 171 Training loss: 0.157 Validation loss : 0.955\n",
      "Epoch 181 Training loss: 0.149 Validation loss : 0.990\n",
      "Epoch 191 Training loss: 0.147 Validation loss : 1.043\n",
      "Epoch 201 Training loss: 0.144 Validation loss : 1.048\n",
      "Epoch 211 Training loss: 0.139 Validation loss : 1.126\n",
      "Epoch 221 Training loss: 0.136 Validation loss : 1.053\n",
      "Epoch 231 Training loss: 0.134 Validation loss : 1.108\n",
      "Epoch 241 Training loss: 0.130 Validation loss : 1.174\n",
      "Epoch 251 Training loss: 0.129 Validation loss : 1.135\n",
      "Epoch 261 Training loss: 0.126 Validation loss : 1.156\n",
      "Epoch 271 Training loss: 0.125 Validation loss : 1.124\n",
      "Epoch 281 Training loss: 0.124 Validation loss : 1.157\n",
      "Epoch 291 Training loss: 0.122 Validation loss : 1.179\n"
     ]
    }
   ],
   "source": [
    "best_val = 1e6\n",
    "for epoch in range(300):\n",
    "    running_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    \n",
    "    # validate the model after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = []\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss.append(loss.item())\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "    scheduler.step()\n",
    "    if np.mean(val_running_loss) < best_val:\n",
    "        best_val = np.mean(val_running_loss)\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tryput_1 = (best_model(test_inputs.to(device))* std + mean)\n",
    "# np.mean(np.abs(np.abs([item for sublist in tryput_1.detach().numpy() for item in sublist]) - np.array(truth_df[truth_df['date'] == '2023-01-28']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = best_model(test_inputs.to(device))[:,np.array([False, False, False,True,])]\n",
    "# result = []\n",
    "# for i in range(len(pred)):\n",
    "#     result.append(((pred[i] * std_arr[i])+ mean_arr[i]).tolist())\n",
    "# flat_list = [item for sublist in result for item in sublist]\n",
    "# p.mean(np.abs(flat_list - np.array(truth_df[truth_df['date'] == '2023-02-04']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "# test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "# print(\"test error:\", torch.mean(torch.abs(test_preds.cpu() - test_labels)))\n",
    "\n",
    "temp = test_preds[:,np.array([True, False, False,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_1_pred = np.abs(flat_list)\n",
    "\n",
    "temp = test_preds[:,np.array([False, True, False,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_2_pred = np.abs(flat_list)\n",
    "\n",
    "temp = test_preds[:,np.array([False, False, True,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_3_pred = np.abs(flat_list)\n",
    "\n",
    "\n",
    "temp = test_preds[:,np.array([False, False, False,True,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_4_pred = np.abs(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_0128 = np.array(truth_df[truth_df['date'] == '2023-01-28']['value'])\n",
    "groundtruth_0204 = np.array(truth_df[truth_df['date'] == '2023-02-04']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 34])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([groundtruth_0128,groundtruth_0204])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z5/qndjgmp94tgckrl38gcbw_9h0000gn/T/ipykernel_13066/679233685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load in the updated truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/CDC/truth-Incident Hospitalizations 2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'US'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_in_the_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load in the updated truth \n",
    "new_truth = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations 2.csv\")\n",
    "new_truth = new_truth[new_truth['location'] != 'US']\n",
    "new_truth = new_truth[new_truth['location'].isin(states_in_the_prediction)]\n",
    "new_truth.sort_values(by=['date', 'location'], inplace=True)\n",
    "groundtruth_0128 = np.array(new_truth[new_truth['date'] == '2023-01-28']['value'])\n",
    "groundtruth_0204 = np.array(new_truth[new_truth['date'] == '2023-02-04']['value'])\n",
    "groundtruth_0211 = np.array(new_truth[new_truth['date'] == '2023-02-11']['value'])\n",
    "groundtruth_0218 = np.array(new_truth[new_truth['date'] == '2023-02-18']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 week ahead:  21.5026469039917\n",
      "2 week ahead:  22.64774673461914\n",
      "3 week ahead:  29.41070137023926\n",
      "4 week ahead:  43.41663303375244\n"
     ]
    }
   ],
   "source": [
    "print('1 week ahead: ',np.mean(np.abs(week_1_pred - groundtruth_0128)))\n",
    "print('2 week ahead: ',np.mean(np.abs(week_2_pred - groundtruth_0204)))\n",
    "print('3 week ahead: ',np.mean(np.abs(week_3_pred - groundtruth_0128)))\n",
    "print('4 week ahead: ',np.mean(np.abs(week_4_pred - groundtruth_0204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLEAM 1 week ahead:  18.984057145383236\n",
      "GLEAM 2 week ahead:  15.82940462261393\n"
     ]
    }
   ],
   "source": [
    "GLEAM_01_23_pd = pd.read_csv(\"../data/GLEAM/2023-01-23-MOBS-GLEAM_FLUH.csv\")\n",
    "GLEAM_01_28_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '1 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "GLEAM_02_04_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '2 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "print('GLEAM 1 week ahead: ',np.mean(np.abs(GLEAM_01_28_pred - groundtruth_0128)))\n",
    "print('GLEAM 2 week ahead: ',np.mean(np.abs(GLEAM_02_04_pred - groundtruth_0204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    x = np.arange(1,5)\n",
    "    for i in range(50):\n",
    "        plt.plot(np.arange(1,3), np.abs(test_preds[i,:].detach().numpy())[:2],label=\"Prediction\", marker = \"*\")\n",
    "        plt.plot(np.arange(1,3), np.stack([GLEAM_01_28_pred,GLEAM_02_04_pred])[:,i],label=\"GLEAM Prediction\", marker = \"v\")\n",
    "        plt.plot(np.arange(1,3), np.abs(test_preds[i,:].detach().numpy())[2:],label=\"Prediction Shifted\", marker = \"*\")\n",
    "        # plt.plot(x, week_4_pred[i],label=\"Truth Used for Prediction\", marker = \".\")\n",
    "        plt.plot(np.arange(1,3),np.stack([groundtruth_0128,groundtruth_0204])[:,i],label=\"Truth\", marker = \"o\")\n",
    "        plt.title(states_in_the_prediction[i])\n",
    "        plt.xlabel(\"Num of Weeks Ahead\")\n",
    "        plt.ylabel(\"Number of Flu Cases\")\n",
    "        plt.legend(loc='best')\n",
    "        # plt.fill_between(x, pred_reshaped[i,:,0] + ground_truth[i,:],pred_reshaped[i,:,2] + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        #plt.fill_between(x, pred_reshaped[i,:,0]  + ground_truth[i,:],pred_reshaped[i,:,2]  + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportions = [.90, .10]\n",
    "# lengths = [int(p * len(train_loader)) for p in proportions]\n",
    "# lengths[-1] = len(train_loader) - sum(lengths[:-1])\n",
    "# tr_dataset, vl_dataset = torch.utils.data.random_split(train_loader, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AutoMLP(nn.Module):\n",
    "#     def __init__(self,input_length, hidden_length, output_length):\n",
    "#         super(AutoMLP, self).__init__()\n",
    "#         self.input_length = input_length\n",
    "#         self.hidden_length = hidden_length\n",
    "#         self.output_length = output_length\n",
    "#         self.fc1 = nn.Linear(self.input_length, self.hidden_length)\n",
    "#         self.fc2 = nn.Linear(self.hidden_length, self.output_length)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots_quantile_prediction(npz_file_location):\n",
    "    npz = np.load(npz_file_location)\n",
    "    pred = npz['prediction']\n",
    "    \n",
    "    # truth = npz['truth']\n",
    "    ground_truth = np.resize(dfg_date_filtered_truth_filtered[dfg_date_filtered_truth_filtered['date'] == '2023-01-21']['value'].to_numpy(), (4,50)).T\n",
    "\n",
    "    # print(pred.shape,truth.shape)\n",
    "    # if \"quantile_model\" in npz_file_location:\n",
    "    #     pred_reshaped = pred.reshape(50, 4 ,3)\n",
    "    #     truth_reshaped = truth.reshape(50, 4)\n",
    "    # elif \"dropout_model\" in npz_file_location:\n",
    "    #     pred_reshaped = pred.reshape(50, 4)\n",
    "    #     truth_reshaped = truth.reshape(50, 4)\n",
    "    pred_reshaped = pred.reshape(50, 4 ,3)\n",
    "    pred_actual_ground_truth = np.resize(dfg_date_filtered_truth_filtered[dfg_date_filtered_truth_filtered['date'].isin(pd.date_range('2023-01-21', '2023-01-28', freq='W-SAT'))]['value'].to_numpy(), (4,50)).T\n",
    "    # truth_reshaped = truth.reshape(50, 4)\n",
    "\n",
    "    x = np.arange(1,5)\n",
    "    # states = list(states_data[1])\n",
    "    states = dfg_date_filtered_truth_filtered['location_name'].unique()\n",
    "    # states = states_in_the_prediction\n",
    "    for i in range(50):\n",
    "        plt.plot(x, pred_reshaped[i,:,1] + ground_truth[i,:],label=\"Prediction\", marker = \"*\")\n",
    "        plt.plot(x, pred_reshaped[i,:,0]  + ground_truth[i,:],label=\"Lower Bound\", marker = \"v\")\n",
    "        plt.plot(x, pred_reshaped[i,:,2]  + ground_truth[i,:],label=\"Upper Bound\", marker = \"^\")\n",
    "        plt.plot(x, ground_truth[i,:],label=\"Truth Used for Prediction\", marker = \".\")\n",
    "        plt.plot(np.arange(1,3), pred_actual_ground_truth[i,:2],label=\"Truth\", marker = \"o\")\n",
    "        plt.title(states[i])\n",
    "        plt.xlabel(\"Num of Weeks Ahead\")\n",
    "        plt.ylabel(\"Number of Flu Cases\")\n",
    "        plt.legend(loc='best')\n",
    "        # plt.fill_between(x, pred_reshaped[i,:,0] + ground_truth[i,:],pred_reshaped[i,:,2] + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        plt.fill_between(x, pred_reshaped[i,:,0]  + ground_truth[i,:],pred_reshaped[i,:,2]  + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0662545c6fd30eceb00c46289e22b5a22aef9c4ebb29470f344626a3bc8eec96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
