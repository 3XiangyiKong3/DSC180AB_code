{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_in_the_prediction = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    " '41','42','44','45','46','47','48','49','50','51','53','54','55','56']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_df = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations 3.csv\")\n",
    "truth_df = truth_df[truth_df['date'] >= '2022-01-01']\n",
    "#truth_df = truth_df[(truth_df['date'] >= '2022-01-01') & (truth_df['date'] <= '2023-01-28')] #truth_df = truth_df[truth_df['date'] >= '2022-01-01']\n",
    "truth_df = truth_df[truth_df['date'] >= '2022-01-01']\n",
    "truth_df = truth_df[truth_df['location'] != 'US']\n",
    "truth_df = truth_df[truth_df['location'].isin(states_in_the_prediction)]\n",
    "truth_df.sort_values(by=['date', 'location'], inplace=True)\n",
    "unique_dates = truth_df['date'].unique()\n",
    "unique_states = truth_df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022-01-01', '2022-01-08', '2022-01-15', '2022-01-22',\n",
       "       '2022-01-29', '2022-02-05', '2022-02-12', '2022-02-19',\n",
       "       '2022-02-26', '2022-03-05', '2022-03-12', '2022-03-19',\n",
       "       '2022-03-26', '2022-04-02', '2022-04-09', '2022-04-16',\n",
       "       '2022-04-23', '2022-04-30', '2022-05-07', '2022-05-14',\n",
       "       '2022-05-21', '2022-05-28', '2022-06-04', '2022-06-11',\n",
       "       '2022-06-18', '2022-06-25', '2022-07-02', '2022-07-09',\n",
       "       '2022-07-16', '2022-07-23', '2022-07-30', '2022-08-06',\n",
       "       '2022-08-13', '2022-08-20', '2022-08-27', '2022-09-03',\n",
       "       '2022-09-10', '2022-09-17', '2022-09-24', '2022-10-01',\n",
       "       '2022-10-08', '2022-10-15', '2022-10-22', '2022-10-29',\n",
       "       '2022-11-05', '2022-11-12', '2022-11-19', '2022-11-26',\n",
       "       '2022-12-03', '2022-12-10', '2022-12-17', '2022-12-24',\n",
       "       '2022-12-31', '2023-01-07', '2023-01-14', '2023-01-21',\n",
       "       '2023-01-28', '2023-02-04', '2023-02-11', '2023-02-18',\n",
       "       '2023-02-25'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardize the data individually for each state\n",
    "# mean_arr = []\n",
    "# std_arr = []\n",
    "# for state in unique_states:\n",
    "#     state_values = truth_df[truth_df['location'] == state]['value'].values\n",
    "#     mean = state_values.mean()\n",
    "#     mean_arr.append(mean)\n",
    "#     std = state_values.std()\n",
    "#     std_arr.append(std)\n",
    "#     truth_df.loc[truth_df['location'] == state, 'norm_value'] = (state_values - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = np.zeros([len(unique_dates),len(unique_states)])\n",
    "for id1,i in enumerate(unique_dates):\n",
    "    for id2,j in enumerate(unique_states):\n",
    "        weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['value'].values\n",
    "        # weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['norm_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 50)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_temp = weeks.copy().transpose(1,0)\n",
    "# loader_temp[0]\n",
    "# loader_temp = weeks.copy().transpose(1,0)\n",
    "# train_data = loader_temp[:,:-2].copy() #train_data = loader_temp[:,:-4].copy()\n",
    "# np.concatenate([train_data[:,i:i+10] for i in range(0, 46, 1)], axis = 0)[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "loader_temp = weeks.copy().transpose(1,0)\n",
    "train_data = loader_temp[:,:-2].copy() #train_data = loader_temp[:,:-4].copy()\n",
    "train_data = np.concatenate([train_data[:,i:i+10] for i in range(0, 46, 1)], axis = 0)\n",
    "np.random.shuffle(train_data)\n",
    "val_data = train_data[:500]\n",
    "train_data = train_data[500:]\n",
    "test_data = loader_temp[:,-8:].copy() # test_data = loader_temp[:,-10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 10.,  2.,  9.,  1.,  1.,  2.,  2.,  5.,  5.,  4.,  4.,  6.,\n",
       "        4.,  8.,  8.,  1.,  5.,  6.,  4.,  4.,  6.,  9.,  6.,  3.,  0.,\n",
       "        2.,  2.,  2.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  3.,  1.,  5.,  8., 13., 47., 45., 73., 52., 32., 50.,\n",
       "       50., 33., 16., 11.,  5.,  1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50., 33., 16., 11.,  5.,  1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[-1][-6:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous 6 weeks for the next 1 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to PyTorch tensors\n",
    "# train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "# train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "# val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "# test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_data[:, 8:9], dtype=torch.float32) #train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_data[:, 8:9], dtype=torch.float32) #val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "test_inputs = torch.tensor(test_data[:, -6:], dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_data[:, 6:7], dtype=torch.float32) # test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "mean = train_inputs.mean()\n",
    "std = train_inputs.std()\n",
    "\n",
    "train_inputs = (train_inputs - mean) / std\n",
    "val_inputs = (val_inputs - mean) / std\n",
    "train_labels = (train_labels - mean) / std\n",
    "val_labels = (val_labels - mean) / std\n",
    "test_inputs = (test_inputs  - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
    "val_dataset =torch.utils.data.TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "# create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save\n",
    "# with open('../data/shuffled_data/MLP_flu_train.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_dataset, handle)\n",
    "# with open('../data/shuffled_data/MLP_flu_val.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtruth_df = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations 3.csv\")\n",
    "# truth_df = truth_df[(truth_df['date'] >= '2022-02-26') & (truth_df['date'] < '2023-01-28')]\n",
    "newtruth_df = newtruth_df[newtruth_df['location'] != 'US']\n",
    "newtruth_df = newtruth_df[newtruth_df['location'].isin(states_in_the_prediction)]\n",
    "newtruth_df.sort_values(by=['date', 'location'], inplace=True)\n",
    "groundtruth_0128 = np.array(newtruth_df[newtruth_df['date'] == '2023-01-28']['value'])\n",
    "groundtruth_0204 = np.array(newtruth_df[newtruth_df['date'] == '2023-02-04']['value'])\n",
    "groundtruth_0211 = np.array(newtruth_df[newtruth_df['date'] == '2023-02-11']['value'])\n",
    "groundtruth_0218 = np.array(newtruth_df[newtruth_df['date'] == '2023-02-18']['value'])\n",
    "groundtruth_0225 = np.array(newtruth_df[newtruth_df['date'] == '2023-02-25']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMLP(nn.Module):\n",
    "    def __init__(self,input_length, output_length,hidden_length):\n",
    "        super(AutoMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, output_length),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoMLP(6, 1, 128).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week1():\n",
    "    result_dict_1 = {}\n",
    "    week_1_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_1_pred = np.abs(flat_list)\n",
    "            week_1_pred_dict[(length,lrate)] = week_1_pred\n",
    "            result_dict_1[(length,lrate)] = np.mean(np.abs(week_1_pred - groundtruth_0128))\n",
    "        print(result_dict_1)\n",
    "    return result_dict_1, week_1_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pred_1 = week_1_pred_dict.copy()\n",
    "temp_result_1 = result_dict_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 30.90858528137207,\n",
       " (8, 0.001): 38.28237796783447,\n",
       " (8, 0.0001): 35.79594268798828,\n",
       " (16, 0.01): 21.833219985961914,\n",
       " (16, 0.001): 21.186963806152345,\n",
       " (16, 0.0001): 19.860978775024414,\n",
       " (32, 0.01): 125.61238632202148,\n",
       " (32, 0.001): 37.04625598907471,\n",
       " (32, 0.0001): 17.256411781311034,\n",
       " (64, 0.01): 31.7813041305542,\n",
       " (64, 0.001): 21.980377502441407,\n",
       " (64, 0.0001): 25.619525413513184,\n",
       " (128, 0.01): 21.802515449523927,\n",
       " (128, 0.001): 18.710785522460938,\n",
       " (128, 0.0001): 23.489974174499512,\n",
       " (256, 0.01): 38.044397811889645,\n",
       " (256, 0.001): 24.890918807983397,\n",
       " (256, 0.0001): 17.519793395996093}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 23.575529823303224,\n",
       " (8, 0.001): 21.975960998535157,\n",
       " (8, 0.0001): 19.39921875,\n",
       " (16, 0.01): 25.01174301147461,\n",
       " (16, 0.001): 11.461907119750977,\n",
       " (16, 0.0001): 40.021451454162595,\n",
       " (32, 0.01): 17.59495834350586,\n",
       " (32, 0.001): 37.26073196411133,\n",
       " (32, 0.0001): 22.543455963134765,\n",
       " (64, 0.01): 15.459693565368653,\n",
       " (64, 0.001): 29.50738105773926,\n",
       " (64, 0.0001): 14.658164291381835,\n",
       " (128, 0.01): 12.145687408447266,\n",
       " (128, 0.001): 24.456424598693847,\n",
       " (128, 0.0001): 15.56952480316162,\n",
       " (256, 0.01): 15.825345840454101,\n",
       " (256, 0.001): 15.539810409545899,\n",
       " (256, 0.0001): 16.675416831970214}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2():\n",
    "    result_dict_2 = {}\n",
    "    week_2_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_2_pred = np.abs(flat_list)\n",
    "            week_2_pred_dict[(length,lrate)] = week_2_pred\n",
    "            result_dict_2[(length,lrate)] = np.mean(np.abs(week_2_pred - groundtruth_0204))\n",
    "        print(result_dict_2)\n",
    "    return result_dict_2, week_2_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week3():\n",
    "    result_dict_3 = {}\n",
    "    week_3_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_3_pred = np.abs(flat_list)\n",
    "            week_3_pred_dict[(length,lrate)] = week_3_pred\n",
    "            result_dict_3[(length,lrate)] = np.mean(np.abs(week_3_pred - groundtruth_0211))\n",
    "        print(result_dict_3)\n",
    "    return result_dict_3, week_3_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week4():\n",
    "    result_dict_4 = {}\n",
    "    week_4_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_4_pred = np.abs(flat_list)\n",
    "            week_4_pred_dict[(length,lrate)] = week_4_pred\n",
    "            result_dict_4[(length,lrate)] = np.mean(np.abs(week_4_pred - groundtruth_0204))\n",
    "        print(result_dict_4)\n",
    "    return result_dict_4, week_4_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_2 = result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 31.279174613952637,\n",
       " (8, 0.001): 34.62915599822998,\n",
       " (8, 0.0001): 84.24005493164063,\n",
       " (16, 0.01): 28.474387435913087,\n",
       " (16, 0.001): 25.021390647888182,\n",
       " (16, 0.0001): 33.25743797302246,\n",
       " (32, 0.01): 23.506855926513673,\n",
       " (32, 0.001): 26.89193504333496,\n",
       " (32, 0.0001): 28.327368278503418,\n",
       " (64, 0.01): 22.06661148071289,\n",
       " (64, 0.001): 27.043925132751465,\n",
       " (64, 0.0001): 20.53235019683838,\n",
       " (128, 0.01): 31.234378204345703,\n",
       " (128, 0.001): 23.605826377868652,\n",
       " (128, 0.0001): 24.16839130401611,\n",
       " (256, 0.01): 21.985442123413087,\n",
       " (256, 0.001): 23.902833671569823,\n",
       " (256, 0.0001): 62.7376868057251}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 33.38484477996826,\n",
       " (8, 0.001): 38.13541572570801,\n",
       " (8, 0.0001): 29.91273036956787,\n",
       " (16, 0.01): 23.280872650146485,\n",
       " (16, 0.001): 33.50578121185303,\n",
       " (16, 0.0001): 32.05569496154785,\n",
       " (32, 0.01): 18.157356834411623,\n",
       " (32, 0.001): 25.96607925415039,\n",
       " (32, 0.0001): 33.00566864013672,\n",
       " (64, 0.01): 30.06999069213867,\n",
       " (64, 0.001): 43.26408473968506,\n",
       " (64, 0.0001): 16.76365566253662,\n",
       " (128, 0.01): 52.16321346282959,\n",
       " (128, 0.001): 23.476780471801757,\n",
       " (128, 0.0001): 29.579945182800294,\n",
       " (256, 0.01): 65.37937660217285,\n",
       " (256, 0.001): 22.718031806945802,\n",
       " (256, 0.0001): 52.36312831878662}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 36.262850799560546,\n",
       " (8, 0.001): 20.73842296600342,\n",
       " (8, 0.0001): 103.26102088928222,\n",
       " (16, 0.01): 36.05392253875733,\n",
       " (16, 0.001): 25.70744213104248,\n",
       " (16, 0.0001): 27.64268928527832,\n",
       " (32, 0.01): 27.38308036804199,\n",
       " (32, 0.001): 25.030885543823242,\n",
       " (32, 0.0001): 32.5325424194336,\n",
       " (64, 0.01): 48.99851676940918,\n",
       " (64, 0.001): 31.491930809020996,\n",
       " (64, 0.0001): 20.152099800109863,\n",
       " (128, 0.01): 32.94138061523437,\n",
       " (128, 0.001): 29.512160263061524,\n",
       " (128, 0.0001): 28.101052551269532,\n",
       " (256, 0.01): 30.134797286987304,\n",
       " (256, 0.001): 21.791844749450682,\n",
       " (256, 0.0001): 27.589387321472167}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.314999999999998"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13.26+20+15+17)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the updated truth \n",
    "new_truth = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations 2.csv\")\n",
    "new_truth = new_truth[new_truth['location'] != 'US']\n",
    "new_truth = new_truth[new_truth['location'].isin(states_in_the_prediction)]\n",
    "new_truth.sort_values(by=['date', 'location'], inplace=True)\n",
    "groundtruth_0128 = np.array(new_truth[new_truth['date'] == '2023-01-28']['value'])\n",
    "groundtruth_0204 = np.array(new_truth[new_truth['date'] == '2023-02-04']['value'])\n",
    "groundtruth_0211 = np.array(new_truth[new_truth['date'] == '2023-02-11']['value'])\n",
    "groundtruth_0218 = np.array(new_truth[new_truth['date'] == '2023-02-18']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 0.01) 26.53727779388428\n",
      "(8, 0.001) 42.19153205871582\n",
      "(8, 0.0001) 30.798382225036622\n",
      "(16, 0.01) 26.973972930908204\n",
      "(16, 0.001) 34.70088481903076\n",
      "(16, 0.0001) 22.362245864868164\n",
      "(32, 0.01) 19.21786144256592\n",
      "(32, 0.001) 20.663228912353517\n",
      "(32, 0.0001) 35.254453887939455\n",
      "(64, 0.01) 27.425651397705078\n",
      "(64, 0.001) 54.687352867126464\n",
      "(64, 0.0001) 20.334604606628417\n",
      "(128, 0.01) 62.8641361618042\n",
      "(128, 0.001) 15.658547439575194\n",
      "(128, 0.0001) 38.08612743377685\n",
      "(256, 0.01) 75.00898101806641\n",
      "(256, 0.001) 17.497203788757325\n",
      "(256, 0.0001) 63.88647518157959\n"
     ]
    }
   ],
   "source": [
    "for item in week_3_pred_dict.items():\n",
    "    print(item[0], np.mean(np.abs(item[1] - groundtruth_0211)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 0.01) 21.0478023147583\n",
      "(8, 0.001) 40.64174816131592\n",
      "(8, 0.0001) 88.46328689575195\n",
      "(16, 0.01) 26.694155349731446\n",
      "(16, 0.001) 19.77414150238037\n",
      "(16, 0.0001) 30.32465919494629\n",
      "(32, 0.01) 24.555188293457032\n",
      "(32, 0.001) 32.745373306274416\n",
      "(32, 0.0001) 21.276368827819823\n",
      "(64, 0.01) 18.176981811523437\n",
      "(64, 0.001) 23.638693962097168\n",
      "(64, 0.0001) 17.372859001159668\n",
      "(128, 0.01) 35.10382652282715\n",
      "(128, 0.001) 21.371487464904785\n",
      "(128, 0.0001) 24.67693058013916\n",
      "(256, 0.01) 21.69838691711426\n",
      "(256, 0.001) 23.185288505554198\n",
      "(256, 0.0001) 67.91990558624268\n"
     ]
    }
   ],
   "source": [
    "for item in week_4_pred_dict.items():\n",
    "    print(item[0], np.mean(np.abs(item[1] - groundtruth_0218)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLEAM 1 week ahead:  18.702641530396498\n",
      "GLEAM 2 week ahead:  12.972103543433212\n",
      "GLEAM 3 week ahead:  12.099102270072143\n",
      "GLEAM 4 week ahead:  12.208814474872613\n",
      "Average GLEAM:  13.995665454693617\n"
     ]
    }
   ],
   "source": [
    "GLEAM_01_23_pd = pd.read_csv(\"../data/GLEAM/2023-01-23-MOBS-GLEAM_FLUH.csv\")\n",
    "GLEAM_01_28_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '1 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "GLEAM_02_04_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '2 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "GLEAM_02_11_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '3 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "GLEAM_02_18_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '4 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()\n",
    "print('GLEAM 1 week ahead: ',np.mean(np.abs(GLEAM_01_28_pred - groundtruth_0128)))\n",
    "print('GLEAM 2 week ahead: ',np.mean(np.abs(GLEAM_02_04_pred - groundtruth_0204)))\n",
    "print('GLEAM 3 week ahead: ',np.mean(np.abs(GLEAM_02_11_pred - groundtruth_0211)))\n",
    "print('GLEAM 4 week ahead: ',np.mean(np.abs(GLEAM_02_18_pred - groundtruth_0218)))\n",
    "print('Average GLEAM: ',np.mean([np.mean(np.abs(GLEAM_01_28_pred - groundtruth_0128)),np.mean(np.abs(GLEAM_02_04_pred - groundtruth_0204)),np.mean(np.abs(GLEAM_02_11_pred - groundtruth_0211)),np.mean(np.abs(GLEAM_02_18_pred - groundtruth_0218))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29,   5,  35,  24,  80,  17,  16,   1, 216,  44,   4,   8,  26,\n",
       "        35,   9,   4,  20,  28,   5,  25,  30,  57,   8,  30,  45,   3,\n",
       "         5,  19,   4,  57,   3,  86,  43,  10,  40,  42,   4, 188,   2,\n",
       "        42,  10,  48, 232,   4,   2,  54,  40,  10,   5,   1])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth_0218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16: 1 week ahead:  19.93516399383545 lr=0.0001; 16: 1 week ahead:  19.870430908203126 lr = 0.01; <br>\n",
    "64: 1 week ahead:  21.80642028808594 lr=0.0001; 64: 1 week ahead:  17.2224405670166 lr = 0.01; <br>\n",
    "128: 1 week ahead:  18.330050354003905 lr=0.0001; 128: 1 week ahead:  14.73071044921875 lr = 0.01; <br>\n",
    "256: 2 week ahead:  22.322580184936523 lr=0.001; 256: 2 week ahead:   lr = 0.01; <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tryput_1 = (best_model(test_inputs.to(device))* std + mean)\n",
    "# np.mean(np.abs(np.abs([item for sublist in tryput_1.detach().numpy() for item in sublist]) - np.array(truth_df[truth_df['date'] == '2023-01-28']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = best_model(test_inputs.to(device))[:,np.array([False, False, False,True,])]\n",
    "# result = []\n",
    "# for i in range(len(pred)):\n",
    "#     result.append(((pred[i] * std_arr[i])+ mean_arr[i]).tolist())\n",
    "# flat_list = [item for sublist in result for item in sublist]\n",
    "# p.mean(np.abs(flat_list - np.array(truth_df[truth_df['date'] == '2023-02-04']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 week ahead:  78.5850626373291\n"
     ]
    }
   ],
   "source": [
    "test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "week_1_pred = np.abs(flat_list)\n",
    "print('1 week ahead: ',np.mean(np.abs(week_1_pred - groundtruth_0204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "# test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "# print(\"test error:\", torch.mean(torch.abs(test_preds.cpu() - test_labels)))\n",
    "\n",
    "temp = test_preds[:,np.array([True, False, False,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_1_pred = np.abs(flat_list)\n",
    "\n",
    "temp = test_preds[:,np.array([False, True, False,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_2_pred = np.abs(flat_list)\n",
    "\n",
    "temp = test_preds[:,np.array([False, False, True,False,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_3_pred = np.abs(flat_list)\n",
    "\n",
    "\n",
    "temp = test_preds[:,np.array([False, False, False,True,])]\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "week_4_pred = np.abs(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 34])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([groundtruth_0128,groundtruth_0204])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 week ahead:  21.5026469039917\n",
      "2 week ahead:  22.64774673461914\n",
      "3 week ahead:  29.41070137023926\n",
      "4 week ahead:  43.41663303375244\n"
     ]
    }
   ],
   "source": [
    "print('1 week ahead: ',np.mean(np.abs(week_1_pred - groundtruth_0128)))\n",
    "print('2 week ahead: ',np.mean(np.abs(week_2_pred - groundtruth_0204)))\n",
    "print('3 week ahead: ',np.mean(np.abs(week_3_pred - groundtruth_0128)))\n",
    "print('4 week ahead: ',np.mean(np.abs(week_4_pred - groundtruth_0204)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportions = [.90, .10]\n",
    "# lengths = [int(p * len(train_loader)) for p in proportions]\n",
    "# lengths[-1] = len(train_loader) - sum(lengths[:-1])\n",
    "# tr_dataset, vl_dataset = torch.utils.data.random_split(train_loader, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AutoMLP(nn.Module):\n",
    "#     def __init__(self,input_length, hidden_length, output_length):\n",
    "#         super(AutoMLP, self).__init__()\n",
    "#         self.input_length = input_length\n",
    "#         self.hidden_length = hidden_length\n",
    "#         self.output_length = output_length\n",
    "#         self.fc1 = nn.Linear(self.input_length, self.hidden_length)\n",
    "#         self.fc2 = nn.Linear(self.hidden_length, self.output_length)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots_quantile_prediction(npz_file_location):\n",
    "    npz = np.load(npz_file_location)\n",
    "    pred = npz['prediction']\n",
    "    \n",
    "    # truth = npz['truth']\n",
    "    ground_truth = np.resize(dfg_date_filtered_truth_filtered[dfg_date_filtered_truth_filtered['date'] == '2023-01-21']['value'].to_numpy(), (4,50)).T\n",
    "\n",
    "    # print(pred.shape,truth.shape)\n",
    "    # if \"quantile_model\" in npz_file_location:\n",
    "    #     pred_reshaped = pred.reshape(50, 4 ,3)\n",
    "    #     truth_reshaped = truth.reshape(50, 4)\n",
    "    # elif \"dropout_model\" in npz_file_location:\n",
    "    #     pred_reshaped = pred.reshape(50, 4)\n",
    "    #     truth_reshaped = truth.reshape(50, 4)\n",
    "    pred_reshaped = pred.reshape(50, 4 ,3)\n",
    "    pred_actual_ground_truth = np.resize(dfg_date_filtered_truth_filtered[dfg_date_filtered_truth_filtered['date'].isin(pd.date_range('2023-01-21', '2023-01-28', freq='W-SAT'))]['value'].to_numpy(), (4,50)).T\n",
    "    # truth_reshaped = truth.reshape(50, 4)\n",
    "\n",
    "    x = np.arange(1,5)\n",
    "    # states = list(states_data[1])\n",
    "    states = dfg_date_filtered_truth_filtered['location_name'].unique()\n",
    "    # states = states_in_the_prediction\n",
    "    for i in range(50):\n",
    "        plt.plot(x, pred_reshaped[i,:,1] + ground_truth[i,:],label=\"Prediction\", marker = \"*\")\n",
    "        plt.plot(x, pred_reshaped[i,:,0]  + ground_truth[i,:],label=\"Lower Bound\", marker = \"v\")\n",
    "        plt.plot(x, pred_reshaped[i,:,2]  + ground_truth[i,:],label=\"Upper Bound\", marker = \"^\")\n",
    "        plt.plot(x, ground_truth[i,:],label=\"Truth Used for Prediction\", marker = \".\")\n",
    "        plt.plot(np.arange(1,3), pred_actual_ground_truth[i,:2],label=\"Truth\", marker = \"o\")\n",
    "        plt.title(states[i])\n",
    "        plt.xlabel(\"Num of Weeks Ahead\")\n",
    "        plt.ylabel(\"Number of Flu Cases\")\n",
    "        plt.legend(loc='best')\n",
    "        # plt.fill_between(x, pred_reshaped[i,:,0] + ground_truth[i,:],pred_reshaped[i,:,2] + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        plt.fill_between(x, pred_reshaped[i,:,0]  + ground_truth[i,:],pred_reshaped[i,:,2]  + ground_truth[i,:],color = 'green',alpha = 0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2023 Feb 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "loader_temp = weeks.copy().transpose(1,0)\n",
    "train_data = loader_temp[:,:-4].copy() #train_data = loader_temp[:,:-4].copy()\n",
    "train_data = np.concatenate([train_data[:,i:i+10] for i in range(0, 48, 1)], axis = 0)\n",
    "np.random.shuffle(train_data)\n",
    "val_data = train_data[:700]\n",
    "train_data = train_data[700:]\n",
    "test_data = loader_temp[:,-10:].copy() # test_data = loader_temp[:,-10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_data[:, 9:10], dtype=torch.float32) #train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_data[:, 9:10], dtype=torch.float32) #val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_data[:, 8:9], dtype=torch.float32) # test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "mean = train_inputs.mean()\n",
    "std = train_inputs.std()\n",
    "\n",
    "train_inputs = (train_inputs - mean) / std\n",
    "val_inputs = (val_inputs - mean) / std\n",
    "train_labels = (train_labels - mean) / std\n",
    "val_labels = (val_labels - mean) / std\n",
    "test_inputs = (test_inputs  - mean) / std\n",
    "\n",
    "# create the datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
    "val_dataset =torch.utils.data.TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "# create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week1():\n",
    "    result_dict_1 = {}\n",
    "    week_1_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_1_pred = np.abs(flat_list)\n",
    "            week_1_pred_dict[(length,lrate)] = week_1_pred\n",
    "            result_dict_1[(length,lrate)] = np.mean(np.abs(week_1_pred - groundtruth_0204))\n",
    "        print(result_dict_1)\n",
    "    return result_dict_1, week_1_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2():\n",
    "    result_dict_2 = {}\n",
    "    week_2_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_2_pred = np.abs(flat_list)\n",
    "            week_2_pred_dict[(length,lrate)] = week_2_pred\n",
    "            result_dict_2[(length,lrate)] = np.mean(np.abs(week_2_pred - groundtruth_0211))\n",
    "        print(result_dict_2)\n",
    "    return result_dict_2, week_2_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week3():\n",
    "    result_dict_3 = {}\n",
    "    week_3_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_3_pred = np.abs(flat_list)\n",
    "            week_3_pred_dict[(length,lrate)] = week_3_pred\n",
    "            result_dict_3[(length,lrate)] = np.mean(np.abs(week_3_pred - groundtruth_0218))\n",
    "        print(result_dict_3)\n",
    "    return result_dict_3, week_3_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week4():\n",
    "    result_dict_4 = {}\n",
    "    week_4_pred_dict = {}\n",
    "    for length in [8,16,32,64,128,256]:\n",
    "        for lrate in [0.01,0.001,0.0001]:\n",
    "            model = AutoMLP(6, 1, length).to(device) # 8, 16, 32, 64, 128, 256 model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lrate) #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay\n",
    "            best_val = 1e6\n",
    "            for epoch in range(300):\n",
    "                running_loss = []\n",
    "                for i, data in enumerate(train_loader, 0):\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss.append(loss.item())\n",
    "                # validate the model after each epoch\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_running_loss = []\n",
    "                    for i, data in enumerate(val_loader, 0):\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_running_loss.append(loss.item())\n",
    "                if epoch % 10 == 0:\n",
    "                    print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "                scheduler.step()\n",
    "                if np.mean(val_running_loss) < best_val:\n",
    "                    best_val = np.mean(val_running_loss)\n",
    "                    best_model = model\n",
    "\n",
    "            test_pred_1 = best_model(test_inputs.to(device)) * std + mean\n",
    "            flat_list = [item for sublist in test_pred_1.detach().numpy() for item in sublist]\n",
    "            week_4_pred = np.abs(flat_list)\n",
    "            week_4_pred_dict[(length,lrate)] = week_4_pred\n",
    "            result_dict_4[(length,lrate)] = np.mean(np.abs(week_4_pred - groundtruth_0225))\n",
    "        print(result_dict_4)\n",
    "    return result_dict_4, week_4_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 0.480 Validation loss : 0.702\n",
      "Epoch 11 Training loss: 0.169 Validation loss : 0.076\n",
      "Epoch 21 Training loss: 0.130 Validation loss : 0.108\n",
      "Epoch 31 Training loss: 0.126 Validation loss : 0.099\n",
      "Epoch 41 Training loss: 0.140 Validation loss : 0.069\n",
      "Epoch 51 Training loss: 0.104 Validation loss : 0.069\n",
      "Epoch 61 Training loss: 0.106 Validation loss : 0.101\n",
      "Epoch 71 Training loss: 0.110 Validation loss : 0.087\n",
      "Epoch 81 Training loss: 0.112 Validation loss : 0.071\n",
      "Epoch 91 Training loss: 0.101 Validation loss : 0.065\n",
      "Epoch 101 Training loss: 0.096 Validation loss : 0.071\n",
      "Epoch 111 Training loss: 0.098 Validation loss : 0.095\n",
      "Epoch 121 Training loss: 0.093 Validation loss : 0.075\n",
      "Epoch 131 Training loss: 0.095 Validation loss : 0.072\n",
      "Epoch 141 Training loss: 0.088 Validation loss : 0.090\n",
      "Epoch 151 Training loss: 0.085 Validation loss : 0.138\n",
      "Epoch 161 Training loss: 0.087 Validation loss : 0.102\n",
      "Epoch 171 Training loss: 0.086 Validation loss : 0.074\n",
      "Epoch 181 Training loss: 0.087 Validation loss : 0.112\n",
      "Epoch 191 Training loss: 0.086 Validation loss : 0.082\n",
      "Epoch 201 Training loss: 0.083 Validation loss : 0.090\n",
      "Epoch 211 Training loss: 0.081 Validation loss : 0.081\n",
      "Epoch 221 Training loss: 0.081 Validation loss : 0.119\n",
      "Epoch 231 Training loss: 0.080 Validation loss : 0.080\n",
      "Epoch 241 Training loss: 0.080 Validation loss : 0.099\n",
      "Epoch 251 Training loss: 0.080 Validation loss : 0.108\n",
      "Epoch 261 Training loss: 0.080 Validation loss : 0.097\n",
      "Epoch 271 Training loss: 0.078 Validation loss : 0.101\n",
      "Epoch 281 Training loss: 0.079 Validation loss : 0.099\n",
      "Epoch 291 Training loss: 0.078 Validation loss : 0.091\n",
      "Epoch 1 Training loss: 0.876 Validation loss : 0.310\n",
      "Epoch 11 Training loss: 0.205 Validation loss : 0.142\n",
      "Epoch 21 Training loss: 0.154 Validation loss : 0.116\n",
      "Epoch 31 Training loss: 0.136 Validation loss : 0.100\n",
      "Epoch 41 Training loss: 0.128 Validation loss : 0.096\n",
      "Epoch 51 Training loss: 0.119 Validation loss : 0.088\n",
      "Epoch 61 Training loss: 0.118 Validation loss : 0.084\n",
      "Epoch 71 Training loss: 0.113 Validation loss : 0.084\n",
      "Epoch 81 Training loss: 0.112 Validation loss : 0.079\n",
      "Epoch 91 Training loss: 0.110 Validation loss : 0.077\n",
      "Epoch 101 Training loss: 0.107 Validation loss : 0.075\n",
      "Epoch 111 Training loss: 0.106 Validation loss : 0.079\n",
      "Epoch 121 Training loss: 0.105 Validation loss : 0.074\n",
      "Epoch 131 Training loss: 0.104 Validation loss : 0.076\n",
      "Epoch 141 Training loss: 0.103 Validation loss : 0.076\n",
      "Epoch 151 Training loss: 0.106 Validation loss : 0.076\n",
      "Epoch 161 Training loss: 0.101 Validation loss : 0.074\n",
      "Epoch 171 Training loss: 0.101 Validation loss : 0.075\n",
      "Epoch 181 Training loss: 0.101 Validation loss : 0.074\n",
      "Epoch 191 Training loss: 0.100 Validation loss : 0.072\n",
      "Epoch 201 Training loss: 0.099 Validation loss : 0.074\n",
      "Epoch 211 Training loss: 0.099 Validation loss : 0.074\n",
      "Epoch 221 Training loss: 0.098 Validation loss : 0.073\n",
      "Epoch 231 Training loss: 0.098 Validation loss : 0.073\n",
      "Epoch 241 Training loss: 0.098 Validation loss : 0.073\n",
      "Epoch 251 Training loss: 0.098 Validation loss : 0.073\n",
      "Epoch 261 Training loss: 0.097 Validation loss : 0.073\n",
      "Epoch 271 Training loss: 0.098 Validation loss : 0.073\n",
      "Epoch 281 Training loss: 0.097 Validation loss : 0.073\n",
      "Epoch 291 Training loss: 0.097 Validation loss : 0.073\n",
      "Epoch 1 Training loss: 1.755 Validation loss : 1.367\n",
      "Epoch 11 Training loss: 1.385 Validation loss : 1.007\n",
      "Epoch 21 Training loss: 1.005 Validation loss : 0.623\n",
      "Epoch 31 Training loss: 0.653 Validation loss : 0.359\n",
      "Epoch 41 Training loss: 0.453 Validation loss : 0.241\n",
      "Epoch 51 Training loss: 0.321 Validation loss : 0.181\n",
      "Epoch 61 Training loss: 0.259 Validation loss : 0.165\n",
      "Epoch 71 Training loss: 0.230 Validation loss : 0.161\n",
      "Epoch 81 Training loss: 0.211 Validation loss : 0.158\n",
      "Epoch 91 Training loss: 0.199 Validation loss : 0.158\n",
      "Epoch 101 Training loss: 0.189 Validation loss : 0.157\n",
      "Epoch 111 Training loss: 0.182 Validation loss : 0.158\n",
      "Epoch 121 Training loss: 0.176 Validation loss : 0.159\n",
      "Epoch 131 Training loss: 0.171 Validation loss : 0.159\n",
      "Epoch 141 Training loss: 0.168 Validation loss : 0.159\n",
      "Epoch 151 Training loss: 0.165 Validation loss : 0.159\n",
      "Epoch 161 Training loss: 0.163 Validation loss : 0.159\n",
      "Epoch 171 Training loss: 0.161 Validation loss : 0.160\n",
      "Epoch 181 Training loss: 0.159 Validation loss : 0.160\n",
      "Epoch 191 Training loss: 0.158 Validation loss : 0.161\n",
      "Epoch 201 Training loss: 0.157 Validation loss : 0.161\n",
      "Epoch 211 Training loss: 0.156 Validation loss : 0.161\n",
      "Epoch 221 Training loss: 0.155 Validation loss : 0.161\n",
      "Epoch 231 Training loss: 0.154 Validation loss : 0.162\n",
      "Epoch 241 Training loss: 0.154 Validation loss : 0.162\n",
      "Epoch 251 Training loss: 0.153 Validation loss : 0.162\n",
      "Epoch 261 Training loss: 0.152 Validation loss : 0.162\n",
      "Epoch 271 Training loss: 0.152 Validation loss : 0.162\n",
      "Epoch 281 Training loss: 0.152 Validation loss : 0.162\n",
      "Epoch 291 Training loss: 0.151 Validation loss : 0.162\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891}\n",
      "Epoch 1 Training loss: 0.499 Validation loss : 0.171\n",
      "Epoch 11 Training loss: 0.139 Validation loss : 0.088\n",
      "Epoch 21 Training loss: 0.141 Validation loss : 0.099\n",
      "Epoch 31 Training loss: 0.131 Validation loss : 0.129\n",
      "Epoch 41 Training loss: 0.117 Validation loss : 0.070\n",
      "Epoch 51 Training loss: 0.111 Validation loss : 0.070\n",
      "Epoch 61 Training loss: 0.100 Validation loss : 0.104\n",
      "Epoch 71 Training loss: 0.102 Validation loss : 0.066\n",
      "Epoch 81 Training loss: 0.093 Validation loss : 0.067\n",
      "Epoch 91 Training loss: 0.092 Validation loss : 0.082\n",
      "Epoch 101 Training loss: 0.091 Validation loss : 0.073\n",
      "Epoch 111 Training loss: 0.090 Validation loss : 0.089\n",
      "Epoch 121 Training loss: 0.089 Validation loss : 0.072\n",
      "Epoch 131 Training loss: 0.089 Validation loss : 0.068\n",
      "Epoch 141 Training loss: 0.084 Validation loss : 0.075\n",
      "Epoch 151 Training loss: 0.083 Validation loss : 0.091\n",
      "Epoch 161 Training loss: 0.081 Validation loss : 0.076\n",
      "Epoch 171 Training loss: 0.083 Validation loss : 0.081\n",
      "Epoch 181 Training loss: 0.078 Validation loss : 0.076\n",
      "Epoch 191 Training loss: 0.077 Validation loss : 0.072\n",
      "Epoch 201 Training loss: 0.076 Validation loss : 0.077\n",
      "Epoch 211 Training loss: 0.076 Validation loss : 0.076\n",
      "Epoch 221 Training loss: 0.072 Validation loss : 0.080\n",
      "Epoch 231 Training loss: 0.074 Validation loss : 0.086\n",
      "Epoch 241 Training loss: 0.070 Validation loss : 0.080\n",
      "Epoch 251 Training loss: 0.071 Validation loss : 0.084\n",
      "Epoch 261 Training loss: 0.070 Validation loss : 0.083\n",
      "Epoch 271 Training loss: 0.069 Validation loss : 0.082\n",
      "Epoch 281 Training loss: 0.069 Validation loss : 0.083\n",
      "Epoch 291 Training loss: 0.067 Validation loss : 0.087\n",
      "Epoch 1 Training loss: 1.254 Validation loss : 0.362\n",
      "Epoch 11 Training loss: 0.125 Validation loss : 0.084\n",
      "Epoch 21 Training loss: 0.107 Validation loss : 0.103\n",
      "Epoch 31 Training loss: 0.100 Validation loss : 0.074\n",
      "Epoch 41 Training loss: 0.101 Validation loss : 0.084\n",
      "Epoch 51 Training loss: 0.098 Validation loss : 0.083\n",
      "Epoch 61 Training loss: 0.093 Validation loss : 0.080\n",
      "Epoch 71 Training loss: 0.091 Validation loss : 0.075\n",
      "Epoch 81 Training loss: 0.091 Validation loss : 0.077\n",
      "Epoch 91 Training loss: 0.089 Validation loss : 0.075\n",
      "Epoch 101 Training loss: 0.089 Validation loss : 0.080\n",
      "Epoch 111 Training loss: 0.086 Validation loss : 0.093\n",
      "Epoch 121 Training loss: 0.086 Validation loss : 0.082\n",
      "Epoch 131 Training loss: 0.083 Validation loss : 0.074\n",
      "Epoch 141 Training loss: 0.084 Validation loss : 0.086\n",
      "Epoch 151 Training loss: 0.084 Validation loss : 0.080\n",
      "Epoch 161 Training loss: 0.082 Validation loss : 0.091\n",
      "Epoch 171 Training loss: 0.082 Validation loss : 0.081\n",
      "Epoch 181 Training loss: 0.080 Validation loss : 0.078\n",
      "Epoch 191 Training loss: 0.081 Validation loss : 0.091\n",
      "Epoch 201 Training loss: 0.081 Validation loss : 0.088\n",
      "Epoch 211 Training loss: 0.089 Validation loss : 0.087\n",
      "Epoch 221 Training loss: 0.080 Validation loss : 0.090\n",
      "Epoch 231 Training loss: 0.080 Validation loss : 0.090\n",
      "Epoch 241 Training loss: 0.080 Validation loss : 0.088\n",
      "Epoch 251 Training loss: 0.079 Validation loss : 0.087\n",
      "Epoch 261 Training loss: 0.080 Validation loss : 0.088\n",
      "Epoch 271 Training loss: 0.079 Validation loss : 0.089\n",
      "Epoch 281 Training loss: 0.079 Validation loss : 0.091\n",
      "Epoch 291 Training loss: 0.078 Validation loss : 0.089\n",
      "Epoch 1 Training loss: 1.612 Validation loss : 1.195\n",
      "Epoch 11 Training loss: 0.418 Validation loss : 0.229\n",
      "Epoch 21 Training loss: 0.243 Validation loss : 0.157\n",
      "Epoch 31 Training loss: 0.178 Validation loss : 0.114\n",
      "Epoch 41 Training loss: 0.149 Validation loss : 0.100\n",
      "Epoch 51 Training loss: 0.137 Validation loss : 0.095\n",
      "Epoch 61 Training loss: 0.131 Validation loss : 0.093\n",
      "Epoch 71 Training loss: 0.126 Validation loss : 0.092\n",
      "Epoch 81 Training loss: 0.125 Validation loss : 0.091\n",
      "Epoch 91 Training loss: 0.123 Validation loss : 0.091\n",
      "Epoch 101 Training loss: 0.123 Validation loss : 0.090\n",
      "Epoch 111 Training loss: 0.121 Validation loss : 0.090\n",
      "Epoch 121 Training loss: 0.121 Validation loss : 0.089\n",
      "Epoch 131 Training loss: 0.120 Validation loss : 0.089\n",
      "Epoch 141 Training loss: 0.119 Validation loss : 0.088\n",
      "Epoch 151 Training loss: 0.119 Validation loss : 0.088\n",
      "Epoch 161 Training loss: 0.119 Validation loss : 0.088\n",
      "Epoch 171 Training loss: 0.119 Validation loss : 0.088\n",
      "Epoch 181 Training loss: 0.118 Validation loss : 0.088\n",
      "Epoch 191 Training loss: 0.118 Validation loss : 0.087\n",
      "Epoch 201 Training loss: 0.118 Validation loss : 0.087\n",
      "Epoch 211 Training loss: 0.117 Validation loss : 0.087\n",
      "Epoch 221 Training loss: 0.118 Validation loss : 0.087\n",
      "Epoch 231 Training loss: 0.117 Validation loss : 0.087\n",
      "Epoch 241 Training loss: 0.117 Validation loss : 0.087\n",
      "Epoch 251 Training loss: 0.117 Validation loss : 0.087\n",
      "Epoch 261 Training loss: 0.116 Validation loss : 0.087\n",
      "Epoch 271 Training loss: 0.116 Validation loss : 0.087\n",
      "Epoch 281 Training loss: 0.116 Validation loss : 0.086\n",
      "Epoch 291 Training loss: 0.116 Validation loss : 0.086\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891, (16, 0.01): 20.938849716186525, (16, 0.001): 15.04667465209961, (16, 0.0001): 38.37846633911133}\n",
      "Epoch 1 Training loss: 0.333 Validation loss : 0.384\n",
      "Epoch 11 Training loss: 0.157 Validation loss : 0.072\n",
      "Epoch 21 Training loss: 0.125 Validation loss : 0.114\n",
      "Epoch 31 Training loss: 0.128 Validation loss : 0.123\n",
      "Epoch 41 Training loss: 0.150 Validation loss : 0.090\n",
      "Epoch 51 Training loss: 0.123 Validation loss : 0.081\n",
      "Epoch 61 Training loss: 0.102 Validation loss : 0.146\n",
      "Epoch 71 Training loss: 0.105 Validation loss : 0.096\n",
      "Epoch 81 Training loss: 0.096 Validation loss : 0.070\n",
      "Epoch 91 Training loss: 0.088 Validation loss : 0.083\n",
      "Epoch 101 Training loss: 0.085 Validation loss : 0.196\n",
      "Epoch 111 Training loss: 0.074 Validation loss : 0.191\n",
      "Epoch 121 Training loss: 0.066 Validation loss : 0.118\n",
      "Epoch 131 Training loss: 0.059 Validation loss : 0.119\n",
      "Epoch 141 Training loss: 0.059 Validation loss : 0.136\n",
      "Epoch 151 Training loss: 0.059 Validation loss : 0.113\n",
      "Epoch 161 Training loss: 0.055 Validation loss : 0.107\n",
      "Epoch 171 Training loss: 0.053 Validation loss : 0.128\n",
      "Epoch 181 Training loss: 0.052 Validation loss : 0.151\n",
      "Epoch 191 Training loss: 0.052 Validation loss : 0.146\n",
      "Epoch 201 Training loss: 0.049 Validation loss : 0.146\n",
      "Epoch 211 Training loss: 0.049 Validation loss : 0.135\n",
      "Epoch 221 Training loss: 0.047 Validation loss : 0.178\n",
      "Epoch 231 Training loss: 0.048 Validation loss : 0.180\n",
      "Epoch 241 Training loss: 0.047 Validation loss : 0.146\n",
      "Epoch 251 Training loss: 0.046 Validation loss : 0.177\n",
      "Epoch 261 Training loss: 0.045 Validation loss : 0.159\n",
      "Epoch 271 Training loss: 0.045 Validation loss : 0.181\n",
      "Epoch 281 Training loss: 0.044 Validation loss : 0.169\n",
      "Epoch 291 Training loss: 0.043 Validation loss : 0.187\n",
      "Epoch 1 Training loss: 0.565 Validation loss : 0.202\n",
      "Epoch 11 Training loss: 0.119 Validation loss : 0.110\n",
      "Epoch 21 Training loss: 0.113 Validation loss : 0.069\n",
      "Epoch 31 Training loss: 0.100 Validation loss : 0.091\n",
      "Epoch 41 Training loss: 0.101 Validation loss : 0.068\n",
      "Epoch 51 Training loss: 0.098 Validation loss : 0.081\n",
      "Epoch 61 Training loss: 0.091 Validation loss : 0.076\n",
      "Epoch 71 Training loss: 0.091 Validation loss : 0.108\n",
      "Epoch 81 Training loss: 0.096 Validation loss : 0.083\n",
      "Epoch 91 Training loss: 0.087 Validation loss : 0.098\n",
      "Epoch 101 Training loss: 0.084 Validation loss : 0.080\n",
      "Epoch 111 Training loss: 0.085 Validation loss : 0.090\n",
      "Epoch 121 Training loss: 0.081 Validation loss : 0.110\n",
      "Epoch 131 Training loss: 0.078 Validation loss : 0.078\n",
      "Epoch 141 Training loss: 0.078 Validation loss : 0.101\n",
      "Epoch 151 Training loss: 0.077 Validation loss : 0.087\n",
      "Epoch 161 Training loss: 0.075 Validation loss : 0.099\n",
      "Epoch 171 Training loss: 0.074 Validation loss : 0.099\n",
      "Epoch 181 Training loss: 0.075 Validation loss : 0.110\n",
      "Epoch 191 Training loss: 0.073 Validation loss : 0.105\n",
      "Epoch 201 Training loss: 0.072 Validation loss : 0.086\n",
      "Epoch 211 Training loss: 0.071 Validation loss : 0.089\n",
      "Epoch 221 Training loss: 0.070 Validation loss : 0.097\n",
      "Epoch 231 Training loss: 0.070 Validation loss : 0.093\n",
      "Epoch 241 Training loss: 0.069 Validation loss : 0.100\n",
      "Epoch 251 Training loss: 0.069 Validation loss : 0.099\n",
      "Epoch 261 Training loss: 0.069 Validation loss : 0.096\n",
      "Epoch 271 Training loss: 0.068 Validation loss : 0.106\n",
      "Epoch 281 Training loss: 0.068 Validation loss : 0.102\n",
      "Epoch 291 Training loss: 0.067 Validation loss : 0.098\n",
      "Epoch 1 Training loss: 1.295 Validation loss : 0.747\n",
      "Epoch 11 Training loss: 0.339 Validation loss : 0.223\n",
      "Epoch 21 Training loss: 0.203 Validation loss : 0.129\n",
      "Epoch 31 Training loss: 0.157 Validation loss : 0.105\n",
      "Epoch 41 Training loss: 0.132 Validation loss : 0.087\n",
      "Epoch 51 Training loss: 0.120 Validation loss : 0.081\n",
      "Epoch 61 Training loss: 0.113 Validation loss : 0.075\n",
      "Epoch 71 Training loss: 0.111 Validation loss : 0.075\n",
      "Epoch 81 Training loss: 0.109 Validation loss : 0.074\n",
      "Epoch 91 Training loss: 0.107 Validation loss : 0.074\n",
      "Epoch 101 Training loss: 0.106 Validation loss : 0.074\n",
      "Epoch 111 Training loss: 0.105 Validation loss : 0.073\n",
      "Epoch 121 Training loss: 0.103 Validation loss : 0.076\n",
      "Epoch 131 Training loss: 0.102 Validation loss : 0.076\n",
      "Epoch 141 Training loss: 0.102 Validation loss : 0.074\n",
      "Epoch 151 Training loss: 0.101 Validation loss : 0.074\n",
      "Epoch 161 Training loss: 0.101 Validation loss : 0.075\n",
      "Epoch 171 Training loss: 0.101 Validation loss : 0.075\n",
      "Epoch 181 Training loss: 0.101 Validation loss : 0.075\n",
      "Epoch 191 Training loss: 0.100 Validation loss : 0.075\n",
      "Epoch 201 Training loss: 0.100 Validation loss : 0.075\n",
      "Epoch 211 Training loss: 0.100 Validation loss : 0.075\n",
      "Epoch 221 Training loss: 0.099 Validation loss : 0.074\n",
      "Epoch 231 Training loss: 0.099 Validation loss : 0.075\n",
      "Epoch 241 Training loss: 0.099 Validation loss : 0.075\n",
      "Epoch 251 Training loss: 0.099 Validation loss : 0.074\n",
      "Epoch 261 Training loss: 0.099 Validation loss : 0.074\n",
      "Epoch 271 Training loss: 0.099 Validation loss : 0.075\n",
      "Epoch 281 Training loss: 0.099 Validation loss : 0.075\n",
      "Epoch 291 Training loss: 0.099 Validation loss : 0.075\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891, (16, 0.01): 20.938849716186525, (16, 0.001): 15.04667465209961, (16, 0.0001): 38.37846633911133, (32, 0.01): 15.871765670776368, (32, 0.001): 15.222069473266602, (32, 0.0001): 20.545441513061522}\n",
      "Epoch 1 Training loss: 0.553 Validation loss : 0.231\n",
      "Epoch 11 Training loss: 0.165 Validation loss : 0.086\n",
      "Epoch 21 Training loss: 0.178 Validation loss : 0.100\n",
      "Epoch 31 Training loss: 0.134 Validation loss : 0.068\n",
      "Epoch 41 Training loss: 0.150 Validation loss : 0.215\n",
      "Epoch 51 Training loss: 0.098 Validation loss : 0.081\n",
      "Epoch 61 Training loss: 0.137 Validation loss : 0.088\n",
      "Epoch 71 Training loss: 0.103 Validation loss : 0.080\n",
      "Epoch 81 Training loss: 0.081 Validation loss : 0.082\n",
      "Epoch 91 Training loss: 0.082 Validation loss : 0.091\n",
      "Epoch 101 Training loss: 0.070 Validation loss : 0.144\n",
      "Epoch 111 Training loss: 0.071 Validation loss : 0.107\n",
      "Epoch 121 Training loss: 0.067 Validation loss : 0.093\n",
      "Epoch 131 Training loss: 0.068 Validation loss : 0.151\n",
      "Epoch 141 Training loss: 0.065 Validation loss : 0.134\n",
      "Epoch 151 Training loss: 0.060 Validation loss : 0.170\n",
      "Epoch 161 Training loss: 0.056 Validation loss : 0.160\n",
      "Epoch 171 Training loss: 0.056 Validation loss : 0.155\n",
      "Epoch 181 Training loss: 0.055 Validation loss : 0.179\n",
      "Epoch 191 Training loss: 0.053 Validation loss : 0.119\n",
      "Epoch 201 Training loss: 0.051 Validation loss : 0.154\n",
      "Epoch 211 Training loss: 0.054 Validation loss : 0.125\n",
      "Epoch 221 Training loss: 0.050 Validation loss : 0.174\n",
      "Epoch 231 Training loss: 0.049 Validation loss : 0.143\n",
      "Epoch 241 Training loss: 0.048 Validation loss : 0.144\n",
      "Epoch 251 Training loss: 0.049 Validation loss : 0.149\n",
      "Epoch 261 Training loss: 0.048 Validation loss : 0.152\n",
      "Epoch 271 Training loss: 0.046 Validation loss : 0.161\n",
      "Epoch 281 Training loss: 0.045 Validation loss : 0.178\n",
      "Epoch 291 Training loss: 0.045 Validation loss : 0.164\n",
      "Epoch 1 Training loss: 0.668 Validation loss : 0.122\n",
      "Epoch 11 Training loss: 0.117 Validation loss : 0.094\n",
      "Epoch 21 Training loss: 0.102 Validation loss : 0.074\n",
      "Epoch 31 Training loss: 0.095 Validation loss : 0.155\n",
      "Epoch 41 Training loss: 0.092 Validation loss : 0.095\n",
      "Epoch 51 Training loss: 0.099 Validation loss : 0.077\n",
      "Epoch 61 Training loss: 0.089 Validation loss : 0.076\n",
      "Epoch 71 Training loss: 0.086 Validation loss : 0.070\n",
      "Epoch 81 Training loss: 0.083 Validation loss : 0.075\n",
      "Epoch 91 Training loss: 0.082 Validation loss : 0.084\n",
      "Epoch 101 Training loss: 0.078 Validation loss : 0.116\n",
      "Epoch 111 Training loss: 0.078 Validation loss : 0.091\n",
      "Epoch 121 Training loss: 0.074 Validation loss : 0.101\n",
      "Epoch 131 Training loss: 0.073 Validation loss : 0.102\n",
      "Epoch 141 Training loss: 0.070 Validation loss : 0.074\n",
      "Epoch 151 Training loss: 0.070 Validation loss : 0.122\n",
      "Epoch 161 Training loss: 0.066 Validation loss : 0.092\n",
      "Epoch 171 Training loss: 0.067 Validation loss : 0.091\n",
      "Epoch 181 Training loss: 0.064 Validation loss : 0.103\n",
      "Epoch 191 Training loss: 0.062 Validation loss : 0.097\n",
      "Epoch 201 Training loss: 0.067 Validation loss : 0.112\n",
      "Epoch 211 Training loss: 0.061 Validation loss : 0.110\n",
      "Epoch 221 Training loss: 0.060 Validation loss : 0.106\n",
      "Epoch 231 Training loss: 0.059 Validation loss : 0.121\n",
      "Epoch 241 Training loss: 0.058 Validation loss : 0.117\n",
      "Epoch 251 Training loss: 0.057 Validation loss : 0.110\n",
      "Epoch 261 Training loss: 0.057 Validation loss : 0.121\n",
      "Epoch 271 Training loss: 0.056 Validation loss : 0.121\n",
      "Epoch 281 Training loss: 0.055 Validation loss : 0.113\n",
      "Epoch 291 Training loss: 0.055 Validation loss : 0.118\n",
      "Epoch 1 Training loss: 1.409 Validation loss : 0.809\n",
      "Epoch 11 Training loss: 0.171 Validation loss : 0.106\n",
      "Epoch 21 Training loss: 0.114 Validation loss : 0.082\n",
      "Epoch 31 Training loss: 0.105 Validation loss : 0.073\n",
      "Epoch 41 Training loss: 0.100 Validation loss : 0.073\n",
      "Epoch 51 Training loss: 0.097 Validation loss : 0.079\n",
      "Epoch 61 Training loss: 0.094 Validation loss : 0.078\n",
      "Epoch 71 Training loss: 0.092 Validation loss : 0.068\n",
      "Epoch 81 Training loss: 0.091 Validation loss : 0.069\n",
      "Epoch 91 Training loss: 0.091 Validation loss : 0.074\n",
      "Epoch 101 Training loss: 0.090 Validation loss : 0.074\n",
      "Epoch 111 Training loss: 0.089 Validation loss : 0.073\n",
      "Epoch 121 Training loss: 0.088 Validation loss : 0.075\n",
      "Epoch 131 Training loss: 0.087 Validation loss : 0.074\n",
      "Epoch 141 Training loss: 0.087 Validation loss : 0.076\n",
      "Epoch 151 Training loss: 0.087 Validation loss : 0.077\n",
      "Epoch 161 Training loss: 0.086 Validation loss : 0.079\n",
      "Epoch 171 Training loss: 0.086 Validation loss : 0.076\n",
      "Epoch 181 Training loss: 0.086 Validation loss : 0.074\n",
      "Epoch 191 Training loss: 0.086 Validation loss : 0.077\n",
      "Epoch 201 Training loss: 0.085 Validation loss : 0.074\n",
      "Epoch 211 Training loss: 0.087 Validation loss : 0.075\n",
      "Epoch 221 Training loss: 0.085 Validation loss : 0.076\n",
      "Epoch 231 Training loss: 0.085 Validation loss : 0.076\n",
      "Epoch 241 Training loss: 0.086 Validation loss : 0.076\n",
      "Epoch 251 Training loss: 0.085 Validation loss : 0.076\n",
      "Epoch 261 Training loss: 0.084 Validation loss : 0.076\n",
      "Epoch 271 Training loss: 0.085 Validation loss : 0.075\n",
      "Epoch 281 Training loss: 0.085 Validation loss : 0.076\n",
      "Epoch 291 Training loss: 0.084 Validation loss : 0.076\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891, (16, 0.01): 20.938849716186525, (16, 0.001): 15.04667465209961, (16, 0.0001): 38.37846633911133, (32, 0.01): 15.871765670776368, (32, 0.001): 15.222069473266602, (32, 0.0001): 20.545441513061522, (64, 0.01): 14.731124267578124, (64, 0.001): 18.19775146484375, (64, 0.0001): 16.865403289794923}\n",
      "Epoch 1 Training loss: 0.359 Validation loss : 0.234\n",
      "Epoch 11 Training loss: 0.147 Validation loss : 0.101\n",
      "Epoch 21 Training loss: 0.229 Validation loss : 0.100\n",
      "Epoch 31 Training loss: 0.140 Validation loss : 0.081\n",
      "Epoch 41 Training loss: 0.114 Validation loss : 0.076\n",
      "Epoch 51 Training loss: 0.133 Validation loss : 0.086\n",
      "Epoch 61 Training loss: 0.104 Validation loss : 0.074\n",
      "Epoch 71 Training loss: 0.091 Validation loss : 0.104\n",
      "Epoch 81 Training loss: 0.090 Validation loss : 0.072\n",
      "Epoch 91 Training loss: 0.080 Validation loss : 0.116\n",
      "Epoch 101 Training loss: 0.082 Validation loss : 0.129\n",
      "Epoch 111 Training loss: 0.076 Validation loss : 0.144\n",
      "Epoch 121 Training loss: 0.072 Validation loss : 0.081\n",
      "Epoch 131 Training loss: 0.069 Validation loss : 0.106\n",
      "Epoch 141 Training loss: 0.067 Validation loss : 0.127\n",
      "Epoch 151 Training loss: 0.067 Validation loss : 0.092\n",
      "Epoch 161 Training loss: 0.059 Validation loss : 0.139\n",
      "Epoch 171 Training loss: 0.071 Validation loss : 0.116\n",
      "Epoch 181 Training loss: 0.059 Validation loss : 0.148\n",
      "Epoch 191 Training loss: 0.064 Validation loss : 0.162\n",
      "Epoch 201 Training loss: 0.055 Validation loss : 0.114\n",
      "Epoch 211 Training loss: 0.054 Validation loss : 0.124\n",
      "Epoch 221 Training loss: 0.055 Validation loss : 0.148\n",
      "Epoch 231 Training loss: 0.052 Validation loss : 0.126\n",
      "Epoch 241 Training loss: 0.050 Validation loss : 0.120\n",
      "Epoch 251 Training loss: 0.051 Validation loss : 0.119\n",
      "Epoch 261 Training loss: 0.051 Validation loss : 0.145\n",
      "Epoch 271 Training loss: 0.049 Validation loss : 0.128\n",
      "Epoch 281 Training loss: 0.049 Validation loss : 0.138\n",
      "Epoch 291 Training loss: 0.047 Validation loss : 0.163\n",
      "Epoch 1 Training loss: 0.322 Validation loss : 0.318\n",
      "Epoch 11 Training loss: 0.125 Validation loss : 0.064\n",
      "Epoch 21 Training loss: 0.097 Validation loss : 0.070\n",
      "Epoch 31 Training loss: 0.114 Validation loss : 0.086\n",
      "Epoch 41 Training loss: 0.086 Validation loss : 0.087\n",
      "Epoch 51 Training loss: 0.092 Validation loss : 0.072\n",
      "Epoch 61 Training loss: 0.078 Validation loss : 0.071\n",
      "Epoch 71 Training loss: 0.075 Validation loss : 0.116\n",
      "Epoch 81 Training loss: 0.071 Validation loss : 0.117\n",
      "Epoch 91 Training loss: 0.064 Validation loss : 0.076\n",
      "Epoch 101 Training loss: 0.051 Validation loss : 0.255\n",
      "Epoch 111 Training loss: 0.055 Validation loss : 0.142\n",
      "Epoch 121 Training loss: 0.051 Validation loss : 0.136\n",
      "Epoch 131 Training loss: 0.048 Validation loss : 0.149\n",
      "Epoch 141 Training loss: 0.043 Validation loss : 0.136\n",
      "Epoch 151 Training loss: 0.042 Validation loss : 0.129\n",
      "Epoch 161 Training loss: 0.038 Validation loss : 0.152\n",
      "Epoch 171 Training loss: 0.036 Validation loss : 0.124\n",
      "Epoch 181 Training loss: 0.035 Validation loss : 0.106\n",
      "Epoch 191 Training loss: 0.032 Validation loss : 0.174\n",
      "Epoch 201 Training loss: 0.031 Validation loss : 0.142\n",
      "Epoch 211 Training loss: 0.029 Validation loss : 0.125\n",
      "Epoch 221 Training loss: 0.028 Validation loss : 0.146\n",
      "Epoch 231 Training loss: 0.027 Validation loss : 0.134\n",
      "Epoch 241 Training loss: 0.027 Validation loss : 0.140\n",
      "Epoch 251 Training loss: 0.026 Validation loss : 0.121\n",
      "Epoch 261 Training loss: 0.026 Validation loss : 0.129\n",
      "Epoch 271 Training loss: 0.026 Validation loss : 0.132\n",
      "Epoch 281 Training loss: 0.024 Validation loss : 0.147\n",
      "Epoch 291 Training loss: 0.024 Validation loss : 0.146\n",
      "Epoch 1 Training loss: 1.114 Validation loss : 0.417\n",
      "Epoch 11 Training loss: 0.128 Validation loss : 0.083\n",
      "Epoch 21 Training loss: 0.107 Validation loss : 0.070\n",
      "Epoch 31 Training loss: 0.100 Validation loss : 0.069\n",
      "Epoch 41 Training loss: 0.095 Validation loss : 0.071\n",
      "Epoch 51 Training loss: 0.092 Validation loss : 0.081\n",
      "Epoch 61 Training loss: 0.091 Validation loss : 0.067\n",
      "Epoch 71 Training loss: 0.087 Validation loss : 0.077\n",
      "Epoch 81 Training loss: 0.086 Validation loss : 0.072\n",
      "Epoch 91 Training loss: 0.085 Validation loss : 0.075\n",
      "Epoch 101 Training loss: 0.083 Validation loss : 0.077\n",
      "Epoch 111 Training loss: 0.083 Validation loss : 0.077\n",
      "Epoch 121 Training loss: 0.082 Validation loss : 0.073\n",
      "Epoch 131 Training loss: 0.082 Validation loss : 0.073\n",
      "Epoch 141 Training loss: 0.081 Validation loss : 0.071\n",
      "Epoch 151 Training loss: 0.081 Validation loss : 0.073\n",
      "Epoch 161 Training loss: 0.079 Validation loss : 0.074\n",
      "Epoch 171 Training loss: 0.079 Validation loss : 0.077\n",
      "Epoch 181 Training loss: 0.079 Validation loss : 0.075\n",
      "Epoch 191 Training loss: 0.078 Validation loss : 0.075\n",
      "Epoch 201 Training loss: 0.078 Validation loss : 0.079\n",
      "Epoch 211 Training loss: 0.078 Validation loss : 0.078\n",
      "Epoch 221 Training loss: 0.077 Validation loss : 0.076\n",
      "Epoch 231 Training loss: 0.077 Validation loss : 0.076\n",
      "Epoch 241 Training loss: 0.077 Validation loss : 0.077\n",
      "Epoch 251 Training loss: 0.076 Validation loss : 0.077\n",
      "Epoch 261 Training loss: 0.076 Validation loss : 0.076\n",
      "Epoch 271 Training loss: 0.076 Validation loss : 0.077\n",
      "Epoch 281 Training loss: 0.076 Validation loss : 0.077\n",
      "Epoch 291 Training loss: 0.076 Validation loss : 0.077\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891, (16, 0.01): 20.938849716186525, (16, 0.001): 15.04667465209961, (16, 0.0001): 38.37846633911133, (32, 0.01): 15.871765670776368, (32, 0.001): 15.222069473266602, (32, 0.0001): 20.545441513061522, (64, 0.01): 14.731124267578124, (64, 0.001): 18.19775146484375, (64, 0.0001): 16.865403289794923, (128, 0.01): 15.488911972045898, (128, 0.001): 15.631195831298829, (128, 0.0001): 10.902928085327149}\n",
      "Epoch 1 Training loss: 1.033 Validation loss : 0.369\n",
      "Epoch 11 Training loss: 0.539 Validation loss : 0.421\n",
      "Epoch 21 Training loss: 0.189 Validation loss : 0.270\n",
      "Epoch 31 Training loss: 0.129 Validation loss : 0.090\n",
      "Epoch 41 Training loss: 0.221 Validation loss : 0.112\n",
      "Epoch 51 Training loss: 0.097 Validation loss : 0.211\n",
      "Epoch 61 Training loss: 0.135 Validation loss : 0.103\n",
      "Epoch 71 Training loss: 0.121 Validation loss : 0.265\n",
      "Epoch 81 Training loss: 0.077 Validation loss : 0.168\n",
      "Epoch 91 Training loss: 0.083 Validation loss : 0.153\n",
      "Epoch 101 Training loss: 0.072 Validation loss : 0.165\n",
      "Epoch 111 Training loss: 0.067 Validation loss : 0.082\n",
      "Epoch 121 Training loss: 0.079 Validation loss : 0.095\n",
      "Epoch 131 Training loss: 0.058 Validation loss : 0.129\n",
      "Epoch 141 Training loss: 0.066 Validation loss : 0.135\n",
      "Epoch 151 Training loss: 0.061 Validation loss : 0.232\n",
      "Epoch 161 Training loss: 0.055 Validation loss : 0.124\n",
      "Epoch 171 Training loss: 0.055 Validation loss : 0.149\n",
      "Epoch 181 Training loss: 0.052 Validation loss : 0.139\n",
      "Epoch 191 Training loss: 0.051 Validation loss : 0.127\n",
      "Epoch 201 Training loss: 0.055 Validation loss : 0.157\n",
      "Epoch 211 Training loss: 0.049 Validation loss : 0.204\n",
      "Epoch 221 Training loss: 0.049 Validation loss : 0.148\n",
      "Epoch 231 Training loss: 0.050 Validation loss : 0.147\n",
      "Epoch 241 Training loss: 0.048 Validation loss : 0.160\n",
      "Epoch 251 Training loss: 0.046 Validation loss : 0.160\n",
      "Epoch 261 Training loss: 0.049 Validation loss : 0.149\n",
      "Epoch 271 Training loss: 0.047 Validation loss : 0.146\n",
      "Epoch 281 Training loss: 0.046 Validation loss : 0.149\n",
      "Epoch 291 Training loss: 0.045 Validation loss : 0.147\n",
      "Epoch 1 Training loss: 0.332 Validation loss : 0.086\n",
      "Epoch 11 Training loss: 0.138 Validation loss : 0.070\n",
      "Epoch 21 Training loss: 0.111 Validation loss : 0.093\n",
      "Epoch 31 Training loss: 0.094 Validation loss : 0.078\n",
      "Epoch 41 Training loss: 0.098 Validation loss : 0.070\n",
      "Epoch 51 Training loss: 0.111 Validation loss : 0.129\n",
      "Epoch 61 Training loss: 0.080 Validation loss : 0.084\n",
      "Epoch 71 Training loss: 0.075 Validation loss : 0.085\n",
      "Epoch 81 Training loss: 0.073 Validation loss : 0.120\n",
      "Epoch 91 Training loss: 0.060 Validation loss : 0.110\n",
      "Epoch 101 Training loss: 0.053 Validation loss : 0.102\n",
      "Epoch 111 Training loss: 0.045 Validation loss : 0.092\n",
      "Epoch 121 Training loss: 0.041 Validation loss : 0.117\n",
      "Epoch 131 Training loss: 0.036 Validation loss : 0.127\n",
      "Epoch 141 Training loss: 0.034 Validation loss : 0.132\n",
      "Epoch 151 Training loss: 0.035 Validation loss : 0.122\n",
      "Epoch 161 Training loss: 0.029 Validation loss : 0.120\n",
      "Epoch 171 Training loss: 0.026 Validation loss : 0.150\n",
      "Epoch 181 Training loss: 0.024 Validation loss : 0.145\n",
      "Epoch 191 Training loss: 0.022 Validation loss : 0.147\n",
      "Epoch 201 Training loss: 0.022 Validation loss : 0.157\n",
      "Epoch 211 Training loss: 0.021 Validation loss : 0.158\n",
      "Epoch 221 Training loss: 0.020 Validation loss : 0.155\n",
      "Epoch 231 Training loss: 0.018 Validation loss : 0.166\n",
      "Epoch 241 Training loss: 0.018 Validation loss : 0.166\n",
      "Epoch 251 Training loss: 0.017 Validation loss : 0.156\n",
      "Epoch 261 Training loss: 0.016 Validation loss : 0.166\n",
      "Epoch 271 Training loss: 0.016 Validation loss : 0.169\n",
      "Epoch 281 Training loss: 0.016 Validation loss : 0.169\n",
      "Epoch 291 Training loss: 0.016 Validation loss : 0.174\n",
      "Epoch 1 Training loss: 0.521 Validation loss : 0.154\n",
      "Epoch 11 Training loss: 0.109 Validation loss : 0.074\n",
      "Epoch 21 Training loss: 0.106 Validation loss : 0.106\n",
      "Epoch 31 Training loss: 0.094 Validation loss : 0.080\n",
      "Epoch 41 Training loss: 0.095 Validation loss : 0.071\n",
      "Epoch 51 Training loss: 0.086 Validation loss : 0.078\n",
      "Epoch 61 Training loss: 0.087 Validation loss : 0.075\n",
      "Epoch 71 Training loss: 0.085 Validation loss : 0.083\n",
      "Epoch 81 Training loss: 0.078 Validation loss : 0.077\n",
      "Epoch 91 Training loss: 0.081 Validation loss : 0.089\n",
      "Epoch 101 Training loss: 0.080 Validation loss : 0.085\n",
      "Epoch 111 Training loss: 0.081 Validation loss : 0.082\n",
      "Epoch 121 Training loss: 0.077 Validation loss : 0.081\n",
      "Epoch 131 Training loss: 0.076 Validation loss : 0.090\n",
      "Epoch 141 Training loss: 0.076 Validation loss : 0.088\n",
      "Epoch 151 Training loss: 0.075 Validation loss : 0.099\n",
      "Epoch 161 Training loss: 0.073 Validation loss : 0.094\n",
      "Epoch 171 Training loss: 0.072 Validation loss : 0.097\n",
      "Epoch 181 Training loss: 0.072 Validation loss : 0.084\n",
      "Epoch 191 Training loss: 0.072 Validation loss : 0.088\n",
      "Epoch 201 Training loss: 0.071 Validation loss : 0.090\n",
      "Epoch 211 Training loss: 0.070 Validation loss : 0.092\n",
      "Epoch 221 Training loss: 0.070 Validation loss : 0.096\n",
      "Epoch 231 Training loss: 0.069 Validation loss : 0.090\n",
      "Epoch 241 Training loss: 0.069 Validation loss : 0.093\n",
      "Epoch 251 Training loss: 0.068 Validation loss : 0.090\n",
      "Epoch 261 Training loss: 0.069 Validation loss : 0.092\n",
      "Epoch 271 Training loss: 0.069 Validation loss : 0.092\n",
      "Epoch 281 Training loss: 0.068 Validation loss : 0.095\n",
      "Epoch 291 Training loss: 0.067 Validation loss : 0.092\n",
      "{(8, 0.01): 22.205411987304686, (8, 0.001): 27.826680297851563, (8, 0.0001): 42.29873199462891, (16, 0.01): 20.938849716186525, (16, 0.001): 15.04667465209961, (16, 0.0001): 38.37846633911133, (32, 0.01): 15.871765670776368, (32, 0.001): 15.222069473266602, (32, 0.0001): 20.545441513061522, (64, 0.01): 14.731124267578124, (64, 0.001): 18.19775146484375, (64, 0.0001): 16.865403289794923, (128, 0.01): 15.488911972045898, (128, 0.001): 15.631195831298829, (128, 0.0001): 10.902928085327149, (256, 0.01): 22.282574234008788, (256, 0.001): 12.839627990722656, (256, 0.0001): 18.399097290039062}\n"
     ]
    }
   ],
   "source": [
    "week_1_pred_2_04 = week1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 0.01): 22.205411987304686,\n",
       " (8, 0.001): 27.826680297851563,\n",
       " (8, 0.0001): 42.29873199462891,\n",
       " (16, 0.01): 20.938849716186525,\n",
       " (16, 0.001): 15.04667465209961,\n",
       " (16, 0.0001): 38.37846633911133,\n",
       " (32, 0.01): 15.871765670776368,\n",
       " (32, 0.001): 15.222069473266602,\n",
       " (32, 0.0001): 20.545441513061522,\n",
       " (64, 0.01): 14.731124267578124,\n",
       " (64, 0.001): 18.19775146484375,\n",
       " (64, 0.0001): 16.865403289794923,\n",
       " (128, 0.01): 15.488911972045898,\n",
       " (128, 0.001): 15.631195831298829,\n",
       " (128, 0.0001): 10.902928085327149,\n",
       " (256, 0.01): 22.282574234008788,\n",
       " (256, 0.001): 12.839627990722656,\n",
       " (256, 0.0001): 18.399097290039062}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_1_pred_2_04[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_mae = week_1_pred_2_04[0][(128, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_mlp_pred = week_1_pred_2_04[1][(128, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 0.569 Validation loss : 0.480\n",
      "Epoch 11 Training loss: 0.345 Validation loss : 0.422\n",
      "Epoch 21 Training loss: 0.316 Validation loss : 0.348\n",
      "Epoch 31 Training loss: 0.320 Validation loss : 0.417\n",
      "Epoch 41 Training loss: 0.318 Validation loss : 0.370\n",
      "Epoch 51 Training loss: 0.301 Validation loss : 0.326\n",
      "Epoch 61 Training loss: 0.298 Validation loss : 0.375\n",
      "Epoch 71 Training loss: 0.293 Validation loss : 0.338\n",
      "Epoch 81 Training loss: 0.287 Validation loss : 0.339\n",
      "Epoch 91 Training loss: 0.281 Validation loss : 0.339\n",
      "Epoch 101 Training loss: 0.278 Validation loss : 0.339\n",
      "Epoch 111 Training loss: 0.286 Validation loss : 0.342\n",
      "Epoch 121 Training loss: 0.275 Validation loss : 0.363\n",
      "Epoch 131 Training loss: 0.273 Validation loss : 0.345\n",
      "Epoch 141 Training loss: 0.276 Validation loss : 0.332\n",
      "Epoch 151 Training loss: 0.265 Validation loss : 0.348\n",
      "Epoch 161 Training loss: 0.263 Validation loss : 0.347\n",
      "Epoch 171 Training loss: 0.260 Validation loss : 0.352\n",
      "Epoch 181 Training loss: 0.258 Validation loss : 0.346\n",
      "Epoch 191 Training loss: 0.258 Validation loss : 0.344\n",
      "Epoch 201 Training loss: 0.258 Validation loss : 0.353\n",
      "Epoch 211 Training loss: 0.255 Validation loss : 0.352\n",
      "Epoch 221 Training loss: 0.257 Validation loss : 0.353\n",
      "Epoch 231 Training loss: 0.253 Validation loss : 0.350\n",
      "Epoch 241 Training loss: 0.252 Validation loss : 0.357\n",
      "Epoch 251 Training loss: 0.253 Validation loss : 0.349\n",
      "Epoch 261 Training loss: 0.251 Validation loss : 0.351\n",
      "Epoch 271 Training loss: 0.253 Validation loss : 0.347\n",
      "Epoch 281 Training loss: 0.252 Validation loss : 0.354\n",
      "Epoch 291 Training loss: 0.251 Validation loss : 0.355\n",
      "Epoch 1 Training loss: 1.340 Validation loss : 0.942\n",
      "Epoch 11 Training loss: 0.394 Validation loss : 0.467\n",
      "Epoch 21 Training loss: 0.340 Validation loss : 0.409\n",
      "Epoch 31 Training loss: 0.326 Validation loss : 0.388\n",
      "Epoch 41 Training loss: 0.318 Validation loss : 0.377\n",
      "Epoch 51 Training loss: 0.308 Validation loss : 0.373\n",
      "Epoch 61 Training loss: 0.305 Validation loss : 0.363\n",
      "Epoch 71 Training loss: 0.299 Validation loss : 0.358\n",
      "Epoch 81 Training loss: 0.301 Validation loss : 0.355\n",
      "Epoch 91 Training loss: 0.296 Validation loss : 0.350\n",
      "Epoch 101 Training loss: 0.294 Validation loss : 0.347\n",
      "Epoch 111 Training loss: 0.288 Validation loss : 0.341\n",
      "Epoch 121 Training loss: 0.287 Validation loss : 0.344\n",
      "Epoch 131 Training loss: 0.284 Validation loss : 0.336\n",
      "Epoch 141 Training loss: 0.282 Validation loss : 0.336\n",
      "Epoch 151 Training loss: 0.281 Validation loss : 0.332\n",
      "Epoch 161 Training loss: 0.279 Validation loss : 0.335\n",
      "Epoch 171 Training loss: 0.279 Validation loss : 0.332\n",
      "Epoch 181 Training loss: 0.278 Validation loss : 0.331\n",
      "Epoch 191 Training loss: 0.277 Validation loss : 0.328\n",
      "Epoch 201 Training loss: 0.277 Validation loss : 0.328\n",
      "Epoch 211 Training loss: 0.276 Validation loss : 0.328\n",
      "Epoch 221 Training loss: 0.276 Validation loss : 0.327\n",
      "Epoch 231 Training loss: 0.276 Validation loss : 0.326\n",
      "Epoch 241 Training loss: 0.275 Validation loss : 0.326\n",
      "Epoch 251 Training loss: 0.275 Validation loss : 0.325\n",
      "Epoch 261 Training loss: 0.275 Validation loss : 0.324\n",
      "Epoch 271 Training loss: 0.275 Validation loss : 0.324\n",
      "Epoch 281 Training loss: 0.274 Validation loss : 0.324\n",
      "Epoch 291 Training loss: 0.274 Validation loss : 0.323\n",
      "Epoch 1 Training loss: 1.748 Validation loss : 1.549\n",
      "Epoch 11 Training loss: 0.987 Validation loss : 0.865\n",
      "Epoch 21 Training loss: 0.635 Validation loss : 0.669\n",
      "Epoch 31 Training loss: 0.521 Validation loss : 0.578\n",
      "Epoch 41 Training loss: 0.481 Validation loss : 0.532\n",
      "Epoch 51 Training loss: 0.460 Validation loss : 0.507\n",
      "Epoch 61 Training loss: 0.444 Validation loss : 0.492\n",
      "Epoch 71 Training loss: 0.433 Validation loss : 0.480\n",
      "Epoch 81 Training loss: 0.425 Validation loss : 0.471\n",
      "Epoch 91 Training loss: 0.418 Validation loss : 0.464\n",
      "Epoch 101 Training loss: 0.411 Validation loss : 0.459\n",
      "Epoch 111 Training loss: 0.405 Validation loss : 0.454\n",
      "Epoch 121 Training loss: 0.400 Validation loss : 0.450\n",
      "Epoch 131 Training loss: 0.395 Validation loss : 0.446\n",
      "Epoch 141 Training loss: 0.391 Validation loss : 0.442\n",
      "Epoch 151 Training loss: 0.394 Validation loss : 0.439\n",
      "Epoch 161 Training loss: 0.389 Validation loss : 0.436\n",
      "Epoch 171 Training loss: 0.383 Validation loss : 0.433\n",
      "Epoch 181 Training loss: 0.380 Validation loss : 0.431\n",
      "Epoch 191 Training loss: 0.378 Validation loss : 0.429\n",
      "Epoch 201 Training loss: 0.389 Validation loss : 0.426\n",
      "Epoch 211 Training loss: 0.374 Validation loss : 0.425\n",
      "Epoch 221 Training loss: 0.372 Validation loss : 0.423\n",
      "Epoch 231 Training loss: 0.370 Validation loss : 0.422\n",
      "Epoch 241 Training loss: 0.369 Validation loss : 0.420\n",
      "Epoch 251 Training loss: 0.372 Validation loss : 0.419\n",
      "Epoch 261 Training loss: 0.368 Validation loss : 0.418\n",
      "Epoch 271 Training loss: 0.366 Validation loss : 0.418\n",
      "Epoch 281 Training loss: 0.365 Validation loss : 0.417\n",
      "Epoch 291 Training loss: 0.365 Validation loss : 0.416\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129}\n",
      "Epoch 1 Training loss: 0.688 Validation loss : 0.429\n",
      "Epoch 11 Training loss: 0.420 Validation loss : 0.355\n",
      "Epoch 21 Training loss: 0.319 Validation loss : 0.383\n",
      "Epoch 31 Training loss: 0.306 Validation loss : 0.359\n",
      "Epoch 41 Training loss: 0.314 Validation loss : 0.358\n",
      "Epoch 51 Training loss: 0.306 Validation loss : 0.354\n",
      "Epoch 61 Training loss: 0.298 Validation loss : 0.361\n",
      "Epoch 71 Training loss: 0.303 Validation loss : 0.362\n",
      "Epoch 81 Training loss: 0.280 Validation loss : 0.355\n",
      "Epoch 91 Training loss: 0.270 Validation loss : 0.371\n",
      "Epoch 101 Training loss: 0.259 Validation loss : 0.376\n",
      "Epoch 111 Training loss: 0.267 Validation loss : 0.386\n",
      "Epoch 121 Training loss: 0.250 Validation loss : 0.369\n",
      "Epoch 131 Training loss: 0.260 Validation loss : 0.388\n",
      "Epoch 141 Training loss: 0.247 Validation loss : 0.412\n",
      "Epoch 151 Training loss: 0.238 Validation loss : 0.392\n",
      "Epoch 161 Training loss: 0.238 Validation loss : 0.409\n",
      "Epoch 171 Training loss: 0.236 Validation loss : 0.433\n",
      "Epoch 181 Training loss: 0.237 Validation loss : 0.430\n",
      "Epoch 191 Training loss: 0.227 Validation loss : 0.437\n",
      "Epoch 201 Training loss: 0.224 Validation loss : 0.423\n",
      "Epoch 211 Training loss: 0.224 Validation loss : 0.444\n",
      "Epoch 221 Training loss: 0.222 Validation loss : 0.438\n",
      "Epoch 231 Training loss: 0.219 Validation loss : 0.441\n",
      "Epoch 241 Training loss: 0.218 Validation loss : 0.431\n",
      "Epoch 251 Training loss: 0.222 Validation loss : 0.434\n",
      "Epoch 261 Training loss: 0.215 Validation loss : 0.432\n",
      "Epoch 271 Training loss: 0.214 Validation loss : 0.446\n",
      "Epoch 281 Training loss: 0.212 Validation loss : 0.431\n",
      "Epoch 291 Training loss: 0.212 Validation loss : 0.435\n",
      "Epoch 1 Training loss: 1.205 Validation loss : 0.762\n",
      "Epoch 11 Training loss: 0.317 Validation loss : 0.343\n",
      "Epoch 21 Training loss: 0.305 Validation loss : 0.303\n",
      "Epoch 31 Training loss: 0.291 Validation loss : 0.296\n",
      "Epoch 41 Training loss: 0.284 Validation loss : 0.300\n",
      "Epoch 51 Training loss: 0.282 Validation loss : 0.293\n",
      "Epoch 61 Training loss: 0.276 Validation loss : 0.303\n",
      "Epoch 71 Training loss: 0.282 Validation loss : 0.290\n",
      "Epoch 81 Training loss: 0.273 Validation loss : 0.297\n",
      "Epoch 91 Training loss: 0.274 Validation loss : 0.295\n",
      "Epoch 101 Training loss: 0.271 Validation loss : 0.300\n",
      "Epoch 111 Training loss: 0.269 Validation loss : 0.308\n",
      "Epoch 121 Training loss: 0.266 Validation loss : 0.300\n",
      "Epoch 131 Training loss: 0.264 Validation loss : 0.302\n",
      "Epoch 141 Training loss: 0.263 Validation loss : 0.308\n",
      "Epoch 151 Training loss: 0.262 Validation loss : 0.299\n",
      "Epoch 161 Training loss: 0.260 Validation loss : 0.299\n",
      "Epoch 171 Training loss: 0.260 Validation loss : 0.302\n",
      "Epoch 181 Training loss: 0.259 Validation loss : 0.306\n",
      "Epoch 191 Training loss: 0.256 Validation loss : 0.298\n",
      "Epoch 201 Training loss: 0.257 Validation loss : 0.304\n",
      "Epoch 211 Training loss: 0.255 Validation loss : 0.302\n",
      "Epoch 221 Training loss: 0.256 Validation loss : 0.305\n",
      "Epoch 231 Training loss: 0.254 Validation loss : 0.306\n",
      "Epoch 241 Training loss: 0.254 Validation loss : 0.305\n",
      "Epoch 251 Training loss: 0.255 Validation loss : 0.305\n",
      "Epoch 261 Training loss: 0.267 Validation loss : 0.303\n",
      "Epoch 271 Training loss: 0.252 Validation loss : 0.306\n",
      "Epoch 281 Training loss: 0.252 Validation loss : 0.306\n",
      "Epoch 291 Training loss: 0.251 Validation loss : 0.305\n",
      "Epoch 1 Training loss: 1.902 Validation loss : 1.651\n",
      "Epoch 11 Training loss: 0.937 Validation loss : 0.806\n",
      "Epoch 21 Training loss: 0.534 Validation loss : 0.573\n",
      "Epoch 31 Training loss: 0.420 Validation loss : 0.483\n",
      "Epoch 41 Training loss: 0.381 Validation loss : 0.449\n",
      "Epoch 51 Training loss: 0.371 Validation loss : 0.435\n",
      "Epoch 61 Training loss: 0.355 Validation loss : 0.423\n",
      "Epoch 71 Training loss: 0.348 Validation loss : 0.415\n",
      "Epoch 81 Training loss: 0.346 Validation loss : 0.408\n",
      "Epoch 91 Training loss: 0.340 Validation loss : 0.403\n",
      "Epoch 101 Training loss: 0.336 Validation loss : 0.399\n",
      "Epoch 111 Training loss: 0.337 Validation loss : 0.396\n",
      "Epoch 121 Training loss: 0.330 Validation loss : 0.393\n",
      "Epoch 131 Training loss: 0.328 Validation loss : 0.390\n",
      "Epoch 141 Training loss: 0.326 Validation loss : 0.388\n",
      "Epoch 151 Training loss: 0.329 Validation loss : 0.386\n",
      "Epoch 161 Training loss: 0.346 Validation loss : 0.384\n",
      "Epoch 171 Training loss: 0.324 Validation loss : 0.383\n",
      "Epoch 181 Training loss: 0.322 Validation loss : 0.381\n",
      "Epoch 191 Training loss: 0.320 Validation loss : 0.380\n",
      "Epoch 201 Training loss: 0.319 Validation loss : 0.379\n",
      "Epoch 211 Training loss: 0.318 Validation loss : 0.378\n",
      "Epoch 221 Training loss: 0.319 Validation loss : 0.377\n",
      "Epoch 231 Training loss: 0.317 Validation loss : 0.376\n",
      "Epoch 241 Training loss: 0.317 Validation loss : 0.376\n",
      "Epoch 251 Training loss: 0.317 Validation loss : 0.375\n",
      "Epoch 261 Training loss: 0.317 Validation loss : 0.375\n",
      "Epoch 271 Training loss: 0.316 Validation loss : 0.374\n",
      "Epoch 281 Training loss: 0.316 Validation loss : 0.374\n",
      "Epoch 291 Training loss: 0.315 Validation loss : 0.374\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129, (16, 0.01): 23.84177734375, (16, 0.001): 32.19263053894043, (16, 0.0001): 34.18379806518555}\n",
      "Epoch 1 Training loss: 0.784 Validation loss : 0.508\n",
      "Epoch 11 Training loss: 0.358 Validation loss : 0.395\n",
      "Epoch 21 Training loss: 0.338 Validation loss : 0.322\n",
      "Epoch 31 Training loss: 0.326 Validation loss : 0.409\n",
      "Epoch 41 Training loss: 0.332 Validation loss : 0.312\n",
      "Epoch 51 Training loss: 0.303 Validation loss : 0.335\n",
      "Epoch 61 Training loss: 0.279 Validation loss : 0.372\n",
      "Epoch 71 Training loss: 0.296 Validation loss : 0.325\n",
      "Epoch 81 Training loss: 0.277 Validation loss : 0.333\n",
      "Epoch 91 Training loss: 0.280 Validation loss : 0.307\n",
      "Epoch 101 Training loss: 0.269 Validation loss : 0.441\n",
      "Epoch 111 Training loss: 0.256 Validation loss : 0.368\n",
      "Epoch 121 Training loss: 0.243 Validation loss : 0.381\n",
      "Epoch 131 Training loss: 0.238 Validation loss : 0.371\n",
      "Epoch 141 Training loss: 0.236 Validation loss : 0.369\n",
      "Epoch 151 Training loss: 0.226 Validation loss : 0.384\n",
      "Epoch 161 Training loss: 0.216 Validation loss : 0.339\n",
      "Epoch 171 Training loss: 0.215 Validation loss : 0.406\n",
      "Epoch 181 Training loss: 0.208 Validation loss : 0.360\n",
      "Epoch 191 Training loss: 0.200 Validation loss : 0.400\n",
      "Epoch 201 Training loss: 0.199 Validation loss : 0.406\n",
      "Epoch 211 Training loss: 0.192 Validation loss : 0.387\n",
      "Epoch 221 Training loss: 0.193 Validation loss : 0.384\n",
      "Epoch 231 Training loss: 0.191 Validation loss : 0.416\n",
      "Epoch 241 Training loss: 0.184 Validation loss : 0.411\n",
      "Epoch 251 Training loss: 0.181 Validation loss : 0.417\n",
      "Epoch 261 Training loss: 0.180 Validation loss : 0.417\n",
      "Epoch 271 Training loss: 0.177 Validation loss : 0.401\n",
      "Epoch 281 Training loss: 0.175 Validation loss : 0.408\n",
      "Epoch 291 Training loss: 0.173 Validation loss : 0.416\n",
      "Epoch 1 Training loss: 0.941 Validation loss : 0.497\n",
      "Epoch 11 Training loss: 0.325 Validation loss : 0.343\n",
      "Epoch 21 Training loss: 0.301 Validation loss : 0.323\n",
      "Epoch 31 Training loss: 0.301 Validation loss : 0.325\n",
      "Epoch 41 Training loss: 0.291 Validation loss : 0.341\n",
      "Epoch 51 Training loss: 0.285 Validation loss : 0.314\n",
      "Epoch 61 Training loss: 0.272 Validation loss : 0.327\n",
      "Epoch 71 Training loss: 0.266 Validation loss : 0.329\n",
      "Epoch 81 Training loss: 0.271 Validation loss : 0.323\n",
      "Epoch 91 Training loss: 0.258 Validation loss : 0.362\n",
      "Epoch 101 Training loss: 0.255 Validation loss : 0.326\n",
      "Epoch 111 Training loss: 0.250 Validation loss : 0.343\n",
      "Epoch 121 Training loss: 0.247 Validation loss : 0.333\n",
      "Epoch 131 Training loss: 0.243 Validation loss : 0.342\n",
      "Epoch 141 Training loss: 0.241 Validation loss : 0.350\n",
      "Epoch 151 Training loss: 0.236 Validation loss : 0.351\n",
      "Epoch 161 Training loss: 0.232 Validation loss : 0.354\n",
      "Epoch 171 Training loss: 0.234 Validation loss : 0.354\n",
      "Epoch 181 Training loss: 0.227 Validation loss : 0.357\n",
      "Epoch 191 Training loss: 0.226 Validation loss : 0.370\n",
      "Epoch 201 Training loss: 0.222 Validation loss : 0.366\n",
      "Epoch 211 Training loss: 0.222 Validation loss : 0.360\n",
      "Epoch 221 Training loss: 0.243 Validation loss : 0.364\n",
      "Epoch 231 Training loss: 0.217 Validation loss : 0.369\n",
      "Epoch 241 Training loss: 0.217 Validation loss : 0.352\n",
      "Epoch 251 Training loss: 0.214 Validation loss : 0.371\n",
      "Epoch 261 Training loss: 0.213 Validation loss : 0.362\n",
      "Epoch 271 Training loss: 0.212 Validation loss : 0.359\n",
      "Epoch 281 Training loss: 0.211 Validation loss : 0.361\n",
      "Epoch 291 Training loss: 0.210 Validation loss : 0.363\n",
      "Epoch 1 Training loss: 1.546 Validation loss : 1.312\n",
      "Epoch 11 Training loss: 0.474 Validation loss : 0.503\n",
      "Epoch 21 Training loss: 0.379 Validation loss : 0.431\n",
      "Epoch 31 Training loss: 0.344 Validation loss : 0.391\n",
      "Epoch 41 Training loss: 0.325 Validation loss : 0.365\n",
      "Epoch 51 Training loss: 0.315 Validation loss : 0.352\n",
      "Epoch 61 Training loss: 0.304 Validation loss : 0.342\n",
      "Epoch 71 Training loss: 0.300 Validation loss : 0.334\n",
      "Epoch 81 Training loss: 0.297 Validation loss : 0.329\n",
      "Epoch 91 Training loss: 0.292 Validation loss : 0.325\n",
      "Epoch 101 Training loss: 0.290 Validation loss : 0.322\n",
      "Epoch 111 Training loss: 0.287 Validation loss : 0.320\n",
      "Epoch 121 Training loss: 0.286 Validation loss : 0.318\n",
      "Epoch 131 Training loss: 0.285 Validation loss : 0.317\n",
      "Epoch 141 Training loss: 0.284 Validation loss : 0.316\n",
      "Epoch 151 Training loss: 0.282 Validation loss : 0.315\n",
      "Epoch 161 Training loss: 0.281 Validation loss : 0.315\n",
      "Epoch 171 Training loss: 0.281 Validation loss : 0.314\n",
      "Epoch 181 Training loss: 0.302 Validation loss : 0.314\n",
      "Epoch 191 Training loss: 0.282 Validation loss : 0.314\n",
      "Epoch 201 Training loss: 0.279 Validation loss : 0.314\n",
      "Epoch 211 Training loss: 0.279 Validation loss : 0.313\n",
      "Epoch 221 Training loss: 0.278 Validation loss : 0.313\n",
      "Epoch 231 Training loss: 0.278 Validation loss : 0.313\n",
      "Epoch 241 Training loss: 0.278 Validation loss : 0.313\n",
      "Epoch 251 Training loss: 0.278 Validation loss : 0.313\n",
      "Epoch 261 Training loss: 0.278 Validation loss : 0.313\n",
      "Epoch 271 Training loss: 0.277 Validation loss : 0.313\n",
      "Epoch 281 Training loss: 0.277 Validation loss : 0.312\n",
      "Epoch 291 Training loss: 0.277 Validation loss : 0.312\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129, (16, 0.01): 23.84177734375, (16, 0.001): 32.19263053894043, (16, 0.0001): 34.18379806518555, (32, 0.01): 18.150708923339845, (32, 0.001): 17.45832489013672, (32, 0.0001): 30.624948654174805}\n",
      "Epoch 1 Training loss: 0.594 Validation loss : 0.396\n",
      "Epoch 11 Training loss: 0.367 Validation loss : 0.410\n",
      "Epoch 21 Training loss: 0.383 Validation loss : 0.439\n",
      "Epoch 31 Training loss: 0.335 Validation loss : 0.325\n",
      "Epoch 41 Training loss: 0.309 Validation loss : 0.336\n",
      "Epoch 51 Training loss: 0.282 Validation loss : 0.384\n",
      "Epoch 61 Training loss: 0.276 Validation loss : 0.349\n",
      "Epoch 71 Training loss: 0.263 Validation loss : 0.357\n",
      "Epoch 81 Training loss: 0.240 Validation loss : 0.415\n",
      "Epoch 91 Training loss: 0.234 Validation loss : 0.301\n",
      "Epoch 101 Training loss: 0.197 Validation loss : 0.327\n",
      "Epoch 111 Training loss: 0.184 Validation loss : 0.273\n",
      "Epoch 121 Training loss: 0.168 Validation loss : 0.295\n",
      "Epoch 131 Training loss: 0.168 Validation loss : 0.387\n",
      "Epoch 141 Training loss: 0.155 Validation loss : 0.461\n",
      "Epoch 151 Training loss: 0.155 Validation loss : 0.413\n",
      "Epoch 161 Training loss: 0.145 Validation loss : 0.413\n",
      "Epoch 171 Training loss: 0.151 Validation loss : 0.483\n",
      "Epoch 181 Training loss: 0.136 Validation loss : 0.462\n",
      "Epoch 191 Training loss: 0.134 Validation loss : 0.462\n",
      "Epoch 201 Training loss: 0.130 Validation loss : 0.417\n",
      "Epoch 211 Training loss: 0.125 Validation loss : 0.519\n",
      "Epoch 221 Training loss: 0.124 Validation loss : 0.430\n",
      "Epoch 231 Training loss: 0.122 Validation loss : 0.468\n",
      "Epoch 241 Training loss: 0.122 Validation loss : 0.475\n",
      "Epoch 251 Training loss: 0.121 Validation loss : 0.438\n",
      "Epoch 261 Training loss: 0.120 Validation loss : 0.426\n",
      "Epoch 271 Training loss: 0.117 Validation loss : 0.510\n",
      "Epoch 281 Training loss: 0.117 Validation loss : 0.468\n",
      "Epoch 291 Training loss: 0.116 Validation loss : 0.453\n",
      "Epoch 1 Training loss: 0.582 Validation loss : 0.467\n",
      "Epoch 11 Training loss: 0.307 Validation loss : 0.318\n",
      "Epoch 21 Training loss: 0.315 Validation loss : 0.323\n",
      "Epoch 31 Training loss: 0.275 Validation loss : 0.298\n",
      "Epoch 41 Training loss: 0.269 Validation loss : 0.330\n",
      "Epoch 51 Training loss: 0.276 Validation loss : 0.357\n",
      "Epoch 61 Training loss: 0.249 Validation loss : 0.308\n",
      "Epoch 71 Training loss: 0.245 Validation loss : 0.331\n",
      "Epoch 81 Training loss: 0.233 Validation loss : 0.307\n",
      "Epoch 91 Training loss: 0.229 Validation loss : 0.328\n",
      "Epoch 101 Training loss: 0.210 Validation loss : 0.388\n",
      "Epoch 111 Training loss: 0.207 Validation loss : 0.356\n",
      "Epoch 121 Training loss: 0.194 Validation loss : 0.388\n",
      "Epoch 131 Training loss: 0.188 Validation loss : 0.380\n",
      "Epoch 141 Training loss: 0.178 Validation loss : 0.417\n",
      "Epoch 151 Training loss: 0.171 Validation loss : 0.389\n",
      "Epoch 161 Training loss: 0.169 Validation loss : 0.354\n",
      "Epoch 171 Training loss: 0.162 Validation loss : 0.314\n",
      "Epoch 181 Training loss: 0.155 Validation loss : 0.342\n",
      "Epoch 191 Training loss: 0.154 Validation loss : 0.366\n",
      "Epoch 201 Training loss: 0.151 Validation loss : 0.362\n",
      "Epoch 211 Training loss: 0.148 Validation loss : 0.343\n",
      "Epoch 221 Training loss: 0.145 Validation loss : 0.349\n",
      "Epoch 231 Training loss: 0.143 Validation loss : 0.366\n",
      "Epoch 241 Training loss: 0.143 Validation loss : 0.366\n",
      "Epoch 251 Training loss: 0.139 Validation loss : 0.379\n",
      "Epoch 261 Training loss: 0.138 Validation loss : 0.370\n",
      "Epoch 271 Training loss: 0.136 Validation loss : 0.351\n",
      "Epoch 281 Training loss: 0.133 Validation loss : 0.350\n",
      "Epoch 291 Training loss: 0.134 Validation loss : 0.364\n",
      "Epoch 1 Training loss: 1.573 Validation loss : 1.162\n",
      "Epoch 11 Training loss: 0.389 Validation loss : 0.429\n",
      "Epoch 21 Training loss: 0.335 Validation loss : 0.378\n",
      "Epoch 31 Training loss: 0.313 Validation loss : 0.354\n",
      "Epoch 41 Training loss: 0.303 Validation loss : 0.341\n",
      "Epoch 51 Training loss: 0.303 Validation loss : 0.334\n",
      "Epoch 61 Training loss: 0.291 Validation loss : 0.329\n",
      "Epoch 71 Training loss: 0.289 Validation loss : 0.325\n",
      "Epoch 81 Training loss: 0.290 Validation loss : 0.322\n",
      "Epoch 91 Training loss: 0.284 Validation loss : 0.321\n",
      "Epoch 101 Training loss: 0.283 Validation loss : 0.317\n",
      "Epoch 111 Training loss: 0.281 Validation loss : 0.317\n",
      "Epoch 121 Training loss: 0.280 Validation loss : 0.316\n",
      "Epoch 131 Training loss: 0.278 Validation loss : 0.314\n",
      "Epoch 141 Training loss: 0.278 Validation loss : 0.316\n",
      "Epoch 151 Training loss: 0.277 Validation loss : 0.315\n",
      "Epoch 161 Training loss: 0.276 Validation loss : 0.313\n",
      "Epoch 171 Training loss: 0.275 Validation loss : 0.314\n",
      "Epoch 181 Training loss: 0.275 Validation loss : 0.315\n",
      "Epoch 191 Training loss: 0.274 Validation loss : 0.312\n",
      "Epoch 201 Training loss: 0.273 Validation loss : 0.313\n",
      "Epoch 211 Training loss: 0.273 Validation loss : 0.313\n",
      "Epoch 221 Training loss: 0.273 Validation loss : 0.313\n",
      "Epoch 231 Training loss: 0.273 Validation loss : 0.312\n",
      "Epoch 241 Training loss: 0.273 Validation loss : 0.313\n",
      "Epoch 251 Training loss: 0.272 Validation loss : 0.313\n",
      "Epoch 261 Training loss: 0.272 Validation loss : 0.313\n",
      "Epoch 271 Training loss: 0.271 Validation loss : 0.313\n",
      "Epoch 281 Training loss: 0.271 Validation loss : 0.312\n",
      "Epoch 291 Training loss: 0.272 Validation loss : 0.312\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129, (16, 0.01): 23.84177734375, (16, 0.001): 32.19263053894043, (16, 0.0001): 34.18379806518555, (32, 0.01): 18.150708923339845, (32, 0.001): 17.45832489013672, (32, 0.0001): 30.624948654174805, (64, 0.01): 33.844782485961915, (64, 0.001): 20.575035095214844, (64, 0.0001): 28.587895431518554}\n",
      "Epoch 1 Training loss: 0.681 Validation loss : 0.704\n",
      "Epoch 11 Training loss: 0.345 Validation loss : 0.444\n",
      "Epoch 21 Training loss: 0.359 Validation loss : 0.368\n",
      "Epoch 31 Training loss: 0.328 Validation loss : 0.299\n",
      "Epoch 41 Training loss: 0.285 Validation loss : 0.343\n",
      "Epoch 51 Training loss: 0.297 Validation loss : 0.353\n",
      "Epoch 61 Training loss: 0.279 Validation loss : 0.310\n",
      "Epoch 71 Training loss: 0.290 Validation loss : 0.312\n",
      "Epoch 81 Training loss: 0.267 Validation loss : 0.469\n",
      "Epoch 91 Training loss: 0.259 Validation loss : 0.447\n",
      "Epoch 101 Training loss: 0.238 Validation loss : 0.312\n",
      "Epoch 111 Training loss: 0.233 Validation loss : 0.471\n",
      "Epoch 121 Training loss: 0.218 Validation loss : 0.309\n",
      "Epoch 131 Training loss: 0.213 Validation loss : 0.626\n",
      "Epoch 141 Training loss: 0.197 Validation loss : 0.353\n",
      "Epoch 151 Training loss: 0.185 Validation loss : 0.373\n",
      "Epoch 161 Training loss: 0.176 Validation loss : 0.350\n",
      "Epoch 171 Training loss: 0.176 Validation loss : 0.376\n",
      "Epoch 181 Training loss: 0.167 Validation loss : 0.403\n",
      "Epoch 191 Training loss: 0.154 Validation loss : 0.429\n",
      "Epoch 201 Training loss: 0.149 Validation loss : 0.430\n",
      "Epoch 211 Training loss: 0.144 Validation loss : 0.442\n",
      "Epoch 221 Training loss: 0.138 Validation loss : 0.436\n",
      "Epoch 231 Training loss: 0.134 Validation loss : 0.447\n",
      "Epoch 241 Training loss: 0.129 Validation loss : 0.434\n",
      "Epoch 251 Training loss: 0.125 Validation loss : 0.468\n",
      "Epoch 261 Training loss: 0.122 Validation loss : 0.462\n",
      "Epoch 271 Training loss: 0.118 Validation loss : 0.464\n",
      "Epoch 281 Training loss: 0.114 Validation loss : 0.471\n",
      "Epoch 291 Training loss: 0.112 Validation loss : 0.478\n",
      "Epoch 1 Training loss: 0.596 Validation loss : 0.421\n",
      "Epoch 11 Training loss: 0.331 Validation loss : 0.326\n",
      "Epoch 21 Training loss: 0.319 Validation loss : 0.312\n",
      "Epoch 31 Training loss: 0.290 Validation loss : 0.343\n",
      "Epoch 41 Training loss: 0.266 Validation loss : 0.337\n",
      "Epoch 51 Training loss: 0.255 Validation loss : 0.321\n",
      "Epoch 61 Training loss: 0.249 Validation loss : 0.331\n",
      "Epoch 71 Training loss: 0.228 Validation loss : 0.337\n",
      "Epoch 81 Training loss: 0.205 Validation loss : 0.312\n",
      "Epoch 91 Training loss: 0.189 Validation loss : 0.338\n",
      "Epoch 101 Training loss: 0.169 Validation loss : 0.370\n",
      "Epoch 111 Training loss: 0.154 Validation loss : 0.359\n",
      "Epoch 121 Training loss: 0.141 Validation loss : 0.350\n",
      "Epoch 131 Training loss: 0.139 Validation loss : 0.372\n",
      "Epoch 141 Training loss: 0.119 Validation loss : 0.370\n",
      "Epoch 151 Training loss: 0.116 Validation loss : 0.402\n",
      "Epoch 161 Training loss: 0.106 Validation loss : 0.378\n",
      "Epoch 171 Training loss: 0.103 Validation loss : 0.382\n",
      "Epoch 181 Training loss: 0.097 Validation loss : 0.379\n",
      "Epoch 191 Training loss: 0.093 Validation loss : 0.396\n",
      "Epoch 201 Training loss: 0.089 Validation loss : 0.404\n",
      "Epoch 211 Training loss: 0.088 Validation loss : 0.418\n",
      "Epoch 221 Training loss: 0.082 Validation loss : 0.414\n",
      "Epoch 231 Training loss: 0.080 Validation loss : 0.398\n",
      "Epoch 241 Training loss: 0.077 Validation loss : 0.414\n",
      "Epoch 251 Training loss: 0.075 Validation loss : 0.411\n",
      "Epoch 261 Training loss: 0.075 Validation loss : 0.425\n",
      "Epoch 271 Training loss: 0.072 Validation loss : 0.427\n",
      "Epoch 281 Training loss: 0.073 Validation loss : 0.407\n",
      "Epoch 291 Training loss: 0.070 Validation loss : 0.403\n",
      "Epoch 1 Training loss: 1.012 Validation loss : 0.676\n",
      "Epoch 11 Training loss: 0.332 Validation loss : 0.348\n",
      "Epoch 21 Training loss: 0.300 Validation loss : 0.321\n",
      "Epoch 31 Training loss: 0.309 Validation loss : 0.324\n",
      "Epoch 41 Training loss: 0.278 Validation loss : 0.351\n",
      "Epoch 51 Training loss: 0.280 Validation loss : 0.305\n",
      "Epoch 61 Training loss: 0.274 Validation loss : 0.309\n",
      "Epoch 71 Training loss: 0.272 Validation loss : 0.309\n",
      "Epoch 81 Training loss: 0.267 Validation loss : 0.310\n",
      "Epoch 91 Training loss: 0.263 Validation loss : 0.302\n",
      "Epoch 101 Training loss: 0.264 Validation loss : 0.307\n",
      "Epoch 111 Training loss: 0.259 Validation loss : 0.318\n",
      "Epoch 121 Training loss: 0.258 Validation loss : 0.317\n",
      "Epoch 131 Training loss: 0.268 Validation loss : 0.323\n",
      "Epoch 141 Training loss: 0.253 Validation loss : 0.318\n",
      "Epoch 151 Training loss: 0.252 Validation loss : 0.312\n",
      "Epoch 161 Training loss: 0.251 Validation loss : 0.315\n",
      "Epoch 171 Training loss: 0.250 Validation loss : 0.312\n",
      "Epoch 181 Training loss: 0.249 Validation loss : 0.313\n",
      "Epoch 191 Training loss: 0.248 Validation loss : 0.317\n",
      "Epoch 201 Training loss: 0.247 Validation loss : 0.319\n",
      "Epoch 211 Training loss: 0.246 Validation loss : 0.316\n",
      "Epoch 221 Training loss: 0.246 Validation loss : 0.318\n",
      "Epoch 231 Training loss: 0.247 Validation loss : 0.319\n",
      "Epoch 241 Training loss: 0.244 Validation loss : 0.316\n",
      "Epoch 251 Training loss: 0.244 Validation loss : 0.317\n",
      "Epoch 261 Training loss: 0.243 Validation loss : 0.319\n",
      "Epoch 271 Training loss: 0.243 Validation loss : 0.318\n",
      "Epoch 281 Training loss: 0.242 Validation loss : 0.321\n",
      "Epoch 291 Training loss: 0.242 Validation loss : 0.319\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129, (16, 0.01): 23.84177734375, (16, 0.001): 32.19263053894043, (16, 0.0001): 34.18379806518555, (32, 0.01): 18.150708923339845, (32, 0.001): 17.45832489013672, (32, 0.0001): 30.624948654174805, (64, 0.01): 33.844782485961915, (64, 0.001): 20.575035095214844, (64, 0.0001): 28.587895431518554, (128, 0.01): 61.27295204162598, (128, 0.001): 26.917884826660156, (128, 0.0001): 18.51411590576172}\n",
      "Epoch 1 Training loss: 0.673 Validation loss : 0.348\n",
      "Epoch 11 Training loss: 0.415 Validation loss : 0.533\n",
      "Epoch 21 Training loss: 0.376 Validation loss : 0.345\n",
      "Epoch 31 Training loss: 0.354 Validation loss : 0.366\n",
      "Epoch 41 Training loss: 0.306 Validation loss : 0.375\n",
      "Epoch 51 Training loss: 0.346 Validation loss : 0.340\n",
      "Epoch 61 Training loss: 0.282 Validation loss : 0.344\n",
      "Epoch 71 Training loss: 0.268 Validation loss : 0.364\n",
      "Epoch 81 Training loss: 0.241 Validation loss : 0.322\n",
      "Epoch 91 Training loss: 0.240 Validation loss : 0.349\n",
      "Epoch 101 Training loss: 0.223 Validation loss : 0.311\n",
      "Epoch 111 Training loss: 0.210 Validation loss : 0.325\n",
      "Epoch 121 Training loss: 0.209 Validation loss : 0.320\n",
      "Epoch 131 Training loss: 0.185 Validation loss : 0.353\n",
      "Epoch 141 Training loss: 0.162 Validation loss : 0.349\n",
      "Epoch 151 Training loss: 0.158 Validation loss : 0.372\n",
      "Epoch 161 Training loss: 0.158 Validation loss : 0.398\n",
      "Epoch 171 Training loss: 0.142 Validation loss : 0.382\n",
      "Epoch 181 Training loss: 0.138 Validation loss : 0.436\n",
      "Epoch 191 Training loss: 0.126 Validation loss : 0.425\n",
      "Epoch 201 Training loss: 0.124 Validation loss : 0.447\n",
      "Epoch 211 Training loss: 0.119 Validation loss : 0.432\n",
      "Epoch 221 Training loss: 0.119 Validation loss : 0.422\n",
      "Epoch 231 Training loss: 0.117 Validation loss : 0.436\n",
      "Epoch 241 Training loss: 0.114 Validation loss : 0.454\n",
      "Epoch 251 Training loss: 0.109 Validation loss : 0.453\n",
      "Epoch 261 Training loss: 0.110 Validation loss : 0.457\n",
      "Epoch 271 Training loss: 0.107 Validation loss : 0.471\n",
      "Epoch 281 Training loss: 0.104 Validation loss : 0.477\n",
      "Epoch 291 Training loss: 0.102 Validation loss : 0.487\n",
      "Epoch 1 Training loss: 0.464 Validation loss : 0.445\n",
      "Epoch 11 Training loss: 0.312 Validation loss : 0.355\n",
      "Epoch 21 Training loss: 0.300 Validation loss : 0.310\n",
      "Epoch 31 Training loss: 0.269 Validation loss : 0.306\n",
      "Epoch 41 Training loss: 0.268 Validation loss : 0.325\n",
      "Epoch 51 Training loss: 0.248 Validation loss : 0.343\n",
      "Epoch 61 Training loss: 0.210 Validation loss : 0.305\n",
      "Epoch 71 Training loss: 0.188 Validation loss : 0.281\n",
      "Epoch 81 Training loss: 0.144 Validation loss : 0.328\n",
      "Epoch 91 Training loss: 0.120 Validation loss : 0.354\n",
      "Epoch 101 Training loss: 0.113 Validation loss : 0.333\n",
      "Epoch 111 Training loss: 0.135 Validation loss : 0.315\n",
      "Epoch 121 Training loss: 0.091 Validation loss : 0.382\n",
      "Epoch 131 Training loss: 0.082 Validation loss : 0.491\n",
      "Epoch 141 Training loss: 0.077 Validation loss : 0.403\n",
      "Epoch 151 Training loss: 0.078 Validation loss : 0.358\n",
      "Epoch 161 Training loss: 0.068 Validation loss : 0.414\n",
      "Epoch 171 Training loss: 0.063 Validation loss : 0.382\n",
      "Epoch 181 Training loss: 0.058 Validation loss : 0.411\n",
      "Epoch 191 Training loss: 0.056 Validation loss : 0.408\n",
      "Epoch 201 Training loss: 0.052 Validation loss : 0.397\n",
      "Epoch 211 Training loss: 0.052 Validation loss : 0.394\n",
      "Epoch 221 Training loss: 0.050 Validation loss : 0.397\n",
      "Epoch 231 Training loss: 0.048 Validation loss : 0.414\n",
      "Epoch 241 Training loss: 0.046 Validation loss : 0.414\n",
      "Epoch 251 Training loss: 0.044 Validation loss : 0.418\n",
      "Epoch 261 Training loss: 0.043 Validation loss : 0.423\n",
      "Epoch 271 Training loss: 0.042 Validation loss : 0.414\n",
      "Epoch 281 Training loss: 0.041 Validation loss : 0.426\n",
      "Epoch 291 Training loss: 0.040 Validation loss : 0.436\n",
      "Epoch 1 Training loss: 0.673 Validation loss : 0.487\n",
      "Epoch 11 Training loss: 0.327 Validation loss : 0.325\n",
      "Epoch 21 Training loss: 0.293 Validation loss : 0.306\n",
      "Epoch 31 Training loss: 0.279 Validation loss : 0.305\n",
      "Epoch 41 Training loss: 0.265 Validation loss : 0.312\n",
      "Epoch 51 Training loss: 0.266 Validation loss : 0.353\n",
      "Epoch 61 Training loss: 0.257 Validation loss : 0.309\n",
      "Epoch 71 Training loss: 0.264 Validation loss : 0.311\n",
      "Epoch 81 Training loss: 0.249 Validation loss : 0.325\n",
      "Epoch 91 Training loss: 0.246 Validation loss : 0.332\n",
      "Epoch 101 Training loss: 0.247 Validation loss : 0.315\n",
      "Epoch 111 Training loss: 0.238 Validation loss : 0.357\n",
      "Epoch 121 Training loss: 0.236 Validation loss : 0.320\n",
      "Epoch 131 Training loss: 0.234 Validation loss : 0.321\n",
      "Epoch 141 Training loss: 0.233 Validation loss : 0.322\n",
      "Epoch 151 Training loss: 0.231 Validation loss : 0.337\n",
      "Epoch 161 Training loss: 0.227 Validation loss : 0.336\n",
      "Epoch 171 Training loss: 0.226 Validation loss : 0.337\n",
      "Epoch 181 Training loss: 0.223 Validation loss : 0.329\n",
      "Epoch 191 Training loss: 0.222 Validation loss : 0.352\n",
      "Epoch 201 Training loss: 0.220 Validation loss : 0.342\n",
      "Epoch 211 Training loss: 0.219 Validation loss : 0.341\n",
      "Epoch 221 Training loss: 0.219 Validation loss : 0.349\n",
      "Epoch 231 Training loss: 0.218 Validation loss : 0.339\n",
      "Epoch 241 Training loss: 0.217 Validation loss : 0.342\n",
      "Epoch 251 Training loss: 0.216 Validation loss : 0.343\n",
      "Epoch 261 Training loss: 0.215 Validation loss : 0.347\n",
      "Epoch 271 Training loss: 0.215 Validation loss : 0.350\n",
      "Epoch 281 Training loss: 0.214 Validation loss : 0.344\n",
      "Epoch 291 Training loss: 0.214 Validation loss : 0.349\n",
      "{(8, 0.01): 23.416033782958984, (8, 0.001): 39.088656158447264, (8, 0.0001): 27.09070411682129, (16, 0.01): 23.84177734375, (16, 0.001): 32.19263053894043, (16, 0.0001): 34.18379806518555, (32, 0.01): 18.150708923339845, (32, 0.001): 17.45832489013672, (32, 0.0001): 30.624948654174805, (64, 0.01): 33.844782485961915, (64, 0.001): 20.575035095214844, (64, 0.0001): 28.587895431518554, (128, 0.01): 61.27295204162598, (128, 0.001): 26.917884826660156, (128, 0.0001): 18.51411590576172, (256, 0.01): 27.098297729492188, (256, 0.001): 23.791760559082032, (256, 0.0001): 17.37345802307129}\n"
     ]
    }
   ],
   "source": [
    "week_2_pred_2_11 = week2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "week2_mae = week_2_pred_2_11[0][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "week2_mlp_pred = week_1_pred_2_04[1][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 0.801 Validation loss : 0.735\n",
      "Epoch 11 Training loss: 0.544 Validation loss : 0.614\n",
      "Epoch 21 Training loss: 0.512 Validation loss : 0.601\n",
      "Epoch 31 Training loss: 0.474 Validation loss : 0.618\n",
      "Epoch 41 Training loss: 0.481 Validation loss : 0.573\n",
      "Epoch 51 Training loss: 0.484 Validation loss : 0.603\n",
      "Epoch 61 Training loss: 0.482 Validation loss : 0.579\n",
      "Epoch 71 Training loss: 0.470 Validation loss : 0.554\n",
      "Epoch 81 Training loss: 0.477 Validation loss : 0.585\n",
      "Epoch 91 Training loss: 0.466 Validation loss : 0.596\n",
      "Epoch 101 Training loss: 0.463 Validation loss : 0.556\n",
      "Epoch 111 Training loss: 0.453 Validation loss : 0.551\n",
      "Epoch 121 Training loss: 0.453 Validation loss : 0.569\n",
      "Epoch 131 Training loss: 0.454 Validation loss : 0.584\n",
      "Epoch 141 Training loss: 0.455 Validation loss : 0.578\n",
      "Epoch 151 Training loss: 0.452 Validation loss : 0.561\n",
      "Epoch 161 Training loss: 0.452 Validation loss : 0.559\n",
      "Epoch 171 Training loss: 0.448 Validation loss : 0.567\n",
      "Epoch 181 Training loss: 0.447 Validation loss : 0.563\n",
      "Epoch 191 Training loss: 0.446 Validation loss : 0.569\n",
      "Epoch 201 Training loss: 0.443 Validation loss : 0.560\n",
      "Epoch 211 Training loss: 0.445 Validation loss : 0.564\n",
      "Epoch 221 Training loss: 0.443 Validation loss : 0.558\n",
      "Epoch 231 Training loss: 0.445 Validation loss : 0.565\n",
      "Epoch 241 Training loss: 0.443 Validation loss : 0.560\n",
      "Epoch 251 Training loss: 0.441 Validation loss : 0.565\n",
      "Epoch 261 Training loss: 0.441 Validation loss : 0.565\n",
      "Epoch 271 Training loss: 0.440 Validation loss : 0.565\n",
      "Epoch 281 Training loss: 0.440 Validation loss : 0.567\n",
      "Epoch 291 Training loss: 0.439 Validation loss : 0.565\n",
      "Epoch 1 Training loss: 1.414 Validation loss : 1.196\n",
      "Epoch 11 Training loss: 0.546 Validation loss : 0.650\n",
      "Epoch 21 Training loss: 0.509 Validation loss : 0.608\n",
      "Epoch 31 Training loss: 0.481 Validation loss : 0.602\n",
      "Epoch 41 Training loss: 0.479 Validation loss : 0.575\n",
      "Epoch 51 Training loss: 0.470 Validation loss : 0.570\n",
      "Epoch 61 Training loss: 0.465 Validation loss : 0.574\n",
      "Epoch 71 Training loss: 0.464 Validation loss : 0.556\n",
      "Epoch 81 Training loss: 0.462 Validation loss : 0.558\n",
      "Epoch 91 Training loss: 0.457 Validation loss : 0.550\n",
      "Epoch 101 Training loss: 0.457 Validation loss : 0.547\n",
      "Epoch 111 Training loss: 0.456 Validation loss : 0.549\n",
      "Epoch 121 Training loss: 0.454 Validation loss : 0.553\n",
      "Epoch 131 Training loss: 0.455 Validation loss : 0.551\n",
      "Epoch 141 Training loss: 0.452 Validation loss : 0.550\n",
      "Epoch 151 Training loss: 0.451 Validation loss : 0.558\n",
      "Epoch 161 Training loss: 0.449 Validation loss : 0.550\n",
      "Epoch 171 Training loss: 0.450 Validation loss : 0.551\n",
      "Epoch 181 Training loss: 0.448 Validation loss : 0.547\n",
      "Epoch 191 Training loss: 0.447 Validation loss : 0.549\n",
      "Epoch 201 Training loss: 0.448 Validation loss : 0.549\n",
      "Epoch 211 Training loss: 0.446 Validation loss : 0.547\n",
      "Epoch 221 Training loss: 0.446 Validation loss : 0.550\n",
      "Epoch 231 Training loss: 0.447 Validation loss : 0.551\n",
      "Epoch 241 Training loss: 0.446 Validation loss : 0.550\n",
      "Epoch 251 Training loss: 0.445 Validation loss : 0.549\n",
      "Epoch 261 Training loss: 0.445 Validation loss : 0.549\n",
      "Epoch 271 Training loss: 0.447 Validation loss : 0.550\n",
      "Epoch 281 Training loss: 0.445 Validation loss : 0.550\n",
      "Epoch 291 Training loss: 0.444 Validation loss : 0.551\n",
      "Epoch 1 Training loss: 1.576 Validation loss : 1.513\n",
      "Epoch 11 Training loss: 1.251 Validation loss : 1.221\n",
      "Epoch 21 Training loss: 0.894 Validation loss : 0.956\n",
      "Epoch 31 Training loss: 0.747 Validation loss : 0.845\n",
      "Epoch 41 Training loss: 0.665 Validation loss : 0.769\n",
      "Epoch 51 Training loss: 0.630 Validation loss : 0.737\n",
      "Epoch 61 Training loss: 0.614 Validation loss : 0.721\n",
      "Epoch 71 Training loss: 0.603 Validation loss : 0.710\n",
      "Epoch 81 Training loss: 0.594 Validation loss : 0.704\n",
      "Epoch 91 Training loss: 0.588 Validation loss : 0.699\n",
      "Epoch 101 Training loss: 0.583 Validation loss : 0.696\n",
      "Epoch 111 Training loss: 0.580 Validation loss : 0.694\n",
      "Epoch 121 Training loss: 0.576 Validation loss : 0.693\n",
      "Epoch 131 Training loss: 0.573 Validation loss : 0.692\n",
      "Epoch 141 Training loss: 0.572 Validation loss : 0.691\n",
      "Epoch 151 Training loss: 0.569 Validation loss : 0.691\n",
      "Epoch 161 Training loss: 0.568 Validation loss : 0.690\n",
      "Epoch 171 Training loss: 0.567 Validation loss : 0.689\n",
      "Epoch 181 Training loss: 0.565 Validation loss : 0.688\n",
      "Epoch 191 Training loss: 0.564 Validation loss : 0.687\n",
      "Epoch 201 Training loss: 0.563 Validation loss : 0.686\n",
      "Epoch 211 Training loss: 0.562 Validation loss : 0.686\n",
      "Epoch 221 Training loss: 0.561 Validation loss : 0.685\n",
      "Epoch 231 Training loss: 0.561 Validation loss : 0.685\n",
      "Epoch 241 Training loss: 0.561 Validation loss : 0.684\n",
      "Epoch 251 Training loss: 0.561 Validation loss : 0.684\n",
      "Epoch 261 Training loss: 0.559 Validation loss : 0.683\n",
      "Epoch 271 Training loss: 0.559 Validation loss : 0.683\n",
      "Epoch 281 Training loss: 0.560 Validation loss : 0.683\n",
      "Epoch 291 Training loss: 0.558 Validation loss : 0.683\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133}\n",
      "Epoch 1 Training loss: 0.848 Validation loss : 0.802\n",
      "Epoch 11 Training loss: 0.529 Validation loss : 0.739\n",
      "Epoch 21 Training loss: 0.508 Validation loss : 0.572\n",
      "Epoch 31 Training loss: 0.514 Validation loss : 0.767\n",
      "Epoch 41 Training loss: 0.486 Validation loss : 0.607\n",
      "Epoch 51 Training loss: 0.484 Validation loss : 0.570\n",
      "Epoch 61 Training loss: 0.460 Validation loss : 0.579\n",
      "Epoch 71 Training loss: 0.471 Validation loss : 0.578\n",
      "Epoch 81 Training loss: 0.457 Validation loss : 0.600\n",
      "Epoch 91 Training loss: 0.456 Validation loss : 0.575\n",
      "Epoch 101 Training loss: 0.451 Validation loss : 0.567\n",
      "Epoch 111 Training loss: 0.460 Validation loss : 0.557\n",
      "Epoch 121 Training loss: 0.416 Validation loss : 0.533\n",
      "Epoch 131 Training loss: 0.403 Validation loss : 0.535\n",
      "Epoch 141 Training loss: 0.385 Validation loss : 0.504\n",
      "Epoch 151 Training loss: 0.380 Validation loss : 0.532\n",
      "Epoch 161 Training loss: 0.368 Validation loss : 0.498\n",
      "Epoch 171 Training loss: 0.360 Validation loss : 0.500\n",
      "Epoch 181 Training loss: 0.355 Validation loss : 0.506\n",
      "Epoch 191 Training loss: 0.351 Validation loss : 0.511\n",
      "Epoch 201 Training loss: 0.348 Validation loss : 0.527\n",
      "Epoch 211 Training loss: 0.339 Validation loss : 0.535\n",
      "Epoch 221 Training loss: 0.335 Validation loss : 0.520\n",
      "Epoch 231 Training loss: 0.332 Validation loss : 0.541\n",
      "Epoch 241 Training loss: 0.330 Validation loss : 0.546\n",
      "Epoch 251 Training loss: 0.326 Validation loss : 0.547\n",
      "Epoch 261 Training loss: 0.324 Validation loss : 0.553\n",
      "Epoch 271 Training loss: 0.321 Validation loss : 0.547\n",
      "Epoch 281 Training loss: 0.318 Validation loss : 0.552\n",
      "Epoch 291 Training loss: 0.318 Validation loss : 0.553\n",
      "Epoch 1 Training loss: 1.135 Validation loss : 1.048\n",
      "Epoch 11 Training loss: 0.507 Validation loss : 0.599\n",
      "Epoch 21 Training loss: 0.497 Validation loss : 0.585\n",
      "Epoch 31 Training loss: 0.489 Validation loss : 0.574\n",
      "Epoch 41 Training loss: 0.485 Validation loss : 0.583\n",
      "Epoch 51 Training loss: 0.469 Validation loss : 0.560\n",
      "Epoch 61 Training loss: 0.471 Validation loss : 0.567\n",
      "Epoch 71 Training loss: 0.463 Validation loss : 0.553\n",
      "Epoch 81 Training loss: 0.461 Validation loss : 0.550\n",
      "Epoch 91 Training loss: 0.457 Validation loss : 0.555\n",
      "Epoch 101 Training loss: 0.452 Validation loss : 0.539\n",
      "Epoch 111 Training loss: 0.451 Validation loss : 0.537\n",
      "Epoch 121 Training loss: 0.447 Validation loss : 0.545\n",
      "Epoch 131 Training loss: 0.446 Validation loss : 0.541\n",
      "Epoch 141 Training loss: 0.443 Validation loss : 0.539\n",
      "Epoch 151 Training loss: 0.442 Validation loss : 0.541\n",
      "Epoch 161 Training loss: 0.440 Validation loss : 0.543\n",
      "Epoch 171 Training loss: 0.439 Validation loss : 0.541\n",
      "Epoch 181 Training loss: 0.438 Validation loss : 0.543\n",
      "Epoch 191 Training loss: 0.438 Validation loss : 0.539\n",
      "Epoch 201 Training loss: 0.436 Validation loss : 0.536\n",
      "Epoch 211 Training loss: 0.435 Validation loss : 0.540\n",
      "Epoch 221 Training loss: 0.435 Validation loss : 0.539\n",
      "Epoch 231 Training loss: 0.433 Validation loss : 0.541\n",
      "Epoch 241 Training loss: 0.434 Validation loss : 0.540\n",
      "Epoch 251 Training loss: 0.432 Validation loss : 0.541\n",
      "Epoch 261 Training loss: 0.432 Validation loss : 0.540\n",
      "Epoch 271 Training loss: 0.432 Validation loss : 0.539\n",
      "Epoch 281 Training loss: 0.431 Validation loss : 0.541\n",
      "Epoch 291 Training loss: 0.432 Validation loss : 0.540\n",
      "Epoch 1 Training loss: 1.286 Validation loss : 1.257\n",
      "Epoch 11 Training loss: 0.763 Validation loss : 0.879\n",
      "Epoch 21 Training loss: 0.621 Validation loss : 0.729\n",
      "Epoch 31 Training loss: 0.590 Validation loss : 0.698\n",
      "Epoch 41 Training loss: 0.575 Validation loss : 0.681\n",
      "Epoch 51 Training loss: 0.564 Validation loss : 0.667\n",
      "Epoch 61 Training loss: 0.556 Validation loss : 0.657\n",
      "Epoch 71 Training loss: 0.548 Validation loss : 0.650\n",
      "Epoch 81 Training loss: 0.544 Validation loss : 0.644\n",
      "Epoch 91 Training loss: 0.538 Validation loss : 0.639\n",
      "Epoch 101 Training loss: 0.534 Validation loss : 0.635\n",
      "Epoch 111 Training loss: 0.530 Validation loss : 0.632\n",
      "Epoch 121 Training loss: 0.528 Validation loss : 0.629\n",
      "Epoch 131 Training loss: 0.526 Validation loss : 0.626\n",
      "Epoch 141 Training loss: 0.523 Validation loss : 0.625\n",
      "Epoch 151 Training loss: 0.524 Validation loss : 0.623\n",
      "Epoch 161 Training loss: 0.519 Validation loss : 0.622\n",
      "Epoch 171 Training loss: 0.518 Validation loss : 0.621\n",
      "Epoch 181 Training loss: 0.517 Validation loss : 0.620\n",
      "Epoch 191 Training loss: 0.520 Validation loss : 0.619\n",
      "Epoch 201 Training loss: 0.515 Validation loss : 0.618\n",
      "Epoch 211 Training loss: 0.514 Validation loss : 0.618\n",
      "Epoch 221 Training loss: 0.512 Validation loss : 0.617\n",
      "Epoch 231 Training loss: 0.512 Validation loss : 0.617\n",
      "Epoch 241 Training loss: 0.510 Validation loss : 0.617\n",
      "Epoch 251 Training loss: 0.511 Validation loss : 0.616\n",
      "Epoch 261 Training loss: 0.509 Validation loss : 0.616\n",
      "Epoch 271 Training loss: 0.509 Validation loss : 0.616\n",
      "Epoch 281 Training loss: 0.508 Validation loss : 0.615\n",
      "Epoch 291 Training loss: 0.508 Validation loss : 0.615\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133, (16, 0.01): 27.955728759765623, (16, 0.001): 27.303933029174804, (16, 0.0001): 32.252861404418944}\n",
      "Epoch 1 Training loss: 0.808 Validation loss : 0.713\n",
      "Epoch 11 Training loss: 0.520 Validation loss : 0.725\n",
      "Epoch 21 Training loss: 0.515 Validation loss : 0.601\n",
      "Epoch 31 Training loss: 0.502 Validation loss : 0.627\n",
      "Epoch 41 Training loss: 0.478 Validation loss : 0.633\n",
      "Epoch 51 Training loss: 0.463 Validation loss : 0.659\n",
      "Epoch 61 Training loss: 0.455 Validation loss : 0.557\n",
      "Epoch 71 Training loss: 0.423 Validation loss : 0.529\n",
      "Epoch 81 Training loss: 0.425 Validation loss : 0.569\n",
      "Epoch 91 Training loss: 0.408 Validation loss : 0.522\n",
      "Epoch 101 Training loss: 0.391 Validation loss : 0.528\n",
      "Epoch 111 Training loss: 0.387 Validation loss : 0.527\n",
      "Epoch 121 Training loss: 0.383 Validation loss : 0.534\n",
      "Epoch 131 Training loss: 0.359 Validation loss : 0.531\n",
      "Epoch 141 Training loss: 0.354 Validation loss : 0.562\n",
      "Epoch 151 Training loss: 0.349 Validation loss : 0.534\n",
      "Epoch 161 Training loss: 0.346 Validation loss : 0.526\n",
      "Epoch 171 Training loss: 0.343 Validation loss : 0.527\n",
      "Epoch 181 Training loss: 0.338 Validation loss : 0.541\n",
      "Epoch 191 Training loss: 0.334 Validation loss : 0.541\n",
      "Epoch 201 Training loss: 0.330 Validation loss : 0.529\n",
      "Epoch 211 Training loss: 0.325 Validation loss : 0.529\n",
      "Epoch 221 Training loss: 0.327 Validation loss : 0.525\n",
      "Epoch 231 Training loss: 0.322 Validation loss : 0.537\n",
      "Epoch 241 Training loss: 0.321 Validation loss : 0.535\n",
      "Epoch 251 Training loss: 0.327 Validation loss : 0.533\n",
      "Epoch 261 Training loss: 0.318 Validation loss : 0.535\n",
      "Epoch 271 Training loss: 0.316 Validation loss : 0.533\n",
      "Epoch 281 Training loss: 0.315 Validation loss : 0.533\n",
      "Epoch 291 Training loss: 0.319 Validation loss : 0.532\n",
      "Epoch 1 Training loss: 1.010 Validation loss : 0.784\n",
      "Epoch 11 Training loss: 0.498 Validation loss : 0.594\n",
      "Epoch 21 Training loss: 0.500 Validation loss : 0.564\n",
      "Epoch 31 Training loss: 0.471 Validation loss : 0.544\n",
      "Epoch 41 Training loss: 0.460 Validation loss : 0.556\n",
      "Epoch 51 Training loss: 0.446 Validation loss : 0.553\n",
      "Epoch 61 Training loss: 0.444 Validation loss : 0.556\n",
      "Epoch 71 Training loss: 0.438 Validation loss : 0.536\n",
      "Epoch 81 Training loss: 0.422 Validation loss : 0.537\n",
      "Epoch 91 Training loss: 0.416 Validation loss : 0.520\n",
      "Epoch 101 Training loss: 0.410 Validation loss : 0.524\n",
      "Epoch 111 Training loss: 0.403 Validation loss : 0.511\n",
      "Epoch 121 Training loss: 0.396 Validation loss : 0.512\n",
      "Epoch 131 Training loss: 0.391 Validation loss : 0.509\n",
      "Epoch 141 Training loss: 0.386 Validation loss : 0.506\n",
      "Epoch 151 Training loss: 0.381 Validation loss : 0.518\n",
      "Epoch 161 Training loss: 0.376 Validation loss : 0.514\n",
      "Epoch 171 Training loss: 0.376 Validation loss : 0.509\n",
      "Epoch 181 Training loss: 0.371 Validation loss : 0.511\n",
      "Epoch 191 Training loss: 0.368 Validation loss : 0.508\n",
      "Epoch 201 Training loss: 0.367 Validation loss : 0.513\n",
      "Epoch 211 Training loss: 0.362 Validation loss : 0.507\n",
      "Epoch 221 Training loss: 0.363 Validation loss : 0.507\n",
      "Epoch 231 Training loss: 0.359 Validation loss : 0.511\n",
      "Epoch 241 Training loss: 0.357 Validation loss : 0.513\n",
      "Epoch 251 Training loss: 0.355 Validation loss : 0.509\n",
      "Epoch 261 Training loss: 0.354 Validation loss : 0.512\n",
      "Epoch 271 Training loss: 0.353 Validation loss : 0.512\n",
      "Epoch 281 Training loss: 0.353 Validation loss : 0.511\n",
      "Epoch 291 Training loss: 0.350 Validation loss : 0.512\n",
      "Epoch 1 Training loss: 1.687 Validation loss : 1.538\n",
      "Epoch 11 Training loss: 0.680 Validation loss : 0.802\n",
      "Epoch 21 Training loss: 0.590 Validation loss : 0.695\n",
      "Epoch 31 Training loss: 0.563 Validation loss : 0.661\n",
      "Epoch 41 Training loss: 0.545 Validation loss : 0.642\n",
      "Epoch 51 Training loss: 0.535 Validation loss : 0.628\n",
      "Epoch 61 Training loss: 0.526 Validation loss : 0.618\n",
      "Epoch 71 Training loss: 0.517 Validation loss : 0.610\n",
      "Epoch 81 Training loss: 0.515 Validation loss : 0.604\n",
      "Epoch 91 Training loss: 0.505 Validation loss : 0.599\n",
      "Epoch 101 Training loss: 0.501 Validation loss : 0.595\n",
      "Epoch 111 Training loss: 0.498 Validation loss : 0.592\n",
      "Epoch 121 Training loss: 0.496 Validation loss : 0.589\n",
      "Epoch 131 Training loss: 0.493 Validation loss : 0.586\n",
      "Epoch 141 Training loss: 0.491 Validation loss : 0.585\n",
      "Epoch 151 Training loss: 0.497 Validation loss : 0.584\n",
      "Epoch 161 Training loss: 0.488 Validation loss : 0.583\n",
      "Epoch 171 Training loss: 0.486 Validation loss : 0.582\n",
      "Epoch 181 Training loss: 0.485 Validation loss : 0.581\n",
      "Epoch 191 Training loss: 0.484 Validation loss : 0.581\n",
      "Epoch 201 Training loss: 0.483 Validation loss : 0.580\n",
      "Epoch 211 Training loss: 0.482 Validation loss : 0.580\n",
      "Epoch 221 Training loss: 0.482 Validation loss : 0.580\n",
      "Epoch 231 Training loss: 0.481 Validation loss : 0.580\n",
      "Epoch 241 Training loss: 0.480 Validation loss : 0.579\n",
      "Epoch 251 Training loss: 0.480 Validation loss : 0.579\n",
      "Epoch 261 Training loss: 0.479 Validation loss : 0.579\n",
      "Epoch 271 Training loss: 0.478 Validation loss : 0.578\n",
      "Epoch 281 Training loss: 0.478 Validation loss : 0.578\n",
      "Epoch 291 Training loss: 0.478 Validation loss : 0.578\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133, (16, 0.01): 27.955728759765623, (16, 0.001): 27.303933029174804, (16, 0.0001): 32.252861404418944, (32, 0.01): 23.296868057250975, (32, 0.001): 21.532077102661134, (32, 0.0001): 31.554113388061523}\n",
      "Epoch 1 Training loss: 0.748 Validation loss : 0.602\n",
      "Epoch 11 Training loss: 0.523 Validation loss : 0.616\n",
      "Epoch 21 Training loss: 0.506 Validation loss : 0.666\n",
      "Epoch 31 Training loss: 0.480 Validation loss : 0.600\n",
      "Epoch 41 Training loss: 0.468 Validation loss : 0.639\n",
      "Epoch 51 Training loss: 0.417 Validation loss : 0.585\n",
      "Epoch 61 Training loss: 0.395 Validation loss : 0.615\n",
      "Epoch 71 Training loss: 0.345 Validation loss : 0.622\n",
      "Epoch 81 Training loss: 0.341 Validation loss : 0.740\n",
      "Epoch 91 Training loss: 0.319 Validation loss : 0.631\n",
      "Epoch 101 Training loss: 0.299 Validation loss : 0.627\n",
      "Epoch 111 Training loss: 0.296 Validation loss : 0.644\n",
      "Epoch 121 Training loss: 0.265 Validation loss : 0.638\n",
      "Epoch 131 Training loss: 0.267 Validation loss : 0.634\n",
      "Epoch 141 Training loss: 0.253 Validation loss : 0.614\n",
      "Epoch 151 Training loss: 0.230 Validation loss : 0.648\n",
      "Epoch 161 Training loss: 0.225 Validation loss : 0.622\n",
      "Epoch 171 Training loss: 0.207 Validation loss : 0.717\n",
      "Epoch 181 Training loss: 0.201 Validation loss : 0.657\n",
      "Epoch 191 Training loss: 0.196 Validation loss : 0.769\n",
      "Epoch 201 Training loss: 0.191 Validation loss : 0.790\n",
      "Epoch 211 Training loss: 0.196 Validation loss : 0.825\n",
      "Epoch 221 Training loss: 0.178 Validation loss : 0.882\n",
      "Epoch 231 Training loss: 0.176 Validation loss : 0.934\n",
      "Epoch 241 Training loss: 0.174 Validation loss : 0.912\n",
      "Epoch 251 Training loss: 0.170 Validation loss : 0.923\n",
      "Epoch 261 Training loss: 0.168 Validation loss : 1.014\n",
      "Epoch 271 Training loss: 0.167 Validation loss : 1.061\n",
      "Epoch 281 Training loss: 0.166 Validation loss : 1.029\n",
      "Epoch 291 Training loss: 0.163 Validation loss : 1.049\n",
      "Epoch 1 Training loss: 0.879 Validation loss : 0.751\n",
      "Epoch 11 Training loss: 0.522 Validation loss : 0.584\n",
      "Epoch 21 Training loss: 0.488 Validation loss : 0.614\n",
      "Epoch 31 Training loss: 0.478 Validation loss : 0.565\n",
      "Epoch 41 Training loss: 0.464 Validation loss : 0.583\n",
      "Epoch 51 Training loss: 0.452 Validation loss : 0.587\n",
      "Epoch 61 Training loss: 0.454 Validation loss : 0.555\n",
      "Epoch 71 Training loss: 0.443 Validation loss : 0.566\n",
      "Epoch 81 Training loss: 0.424 Validation loss : 0.554\n",
      "Epoch 91 Training loss: 0.448 Validation loss : 0.545\n",
      "Epoch 101 Training loss: 0.410 Validation loss : 0.546\n",
      "Epoch 111 Training loss: 0.404 Validation loss : 0.548\n",
      "Epoch 121 Training loss: 0.394 Validation loss : 0.530\n",
      "Epoch 131 Training loss: 0.385 Validation loss : 0.536\n",
      "Epoch 141 Training loss: 0.382 Validation loss : 0.543\n",
      "Epoch 151 Training loss: 0.371 Validation loss : 0.529\n",
      "Epoch 161 Training loss: 0.367 Validation loss : 0.538\n",
      "Epoch 171 Training loss: 0.360 Validation loss : 0.540\n",
      "Epoch 181 Training loss: 0.355 Validation loss : 0.537\n",
      "Epoch 191 Training loss: 0.354 Validation loss : 0.527\n",
      "Epoch 201 Training loss: 0.346 Validation loss : 0.532\n",
      "Epoch 211 Training loss: 0.343 Validation loss : 0.535\n",
      "Epoch 221 Training loss: 0.339 Validation loss : 0.525\n",
      "Epoch 231 Training loss: 0.336 Validation loss : 0.536\n",
      "Epoch 241 Training loss: 0.333 Validation loss : 0.532\n",
      "Epoch 251 Training loss: 0.330 Validation loss : 0.541\n",
      "Epoch 261 Training loss: 0.328 Validation loss : 0.538\n",
      "Epoch 271 Training loss: 0.326 Validation loss : 0.542\n",
      "Epoch 281 Training loss: 0.324 Validation loss : 0.535\n",
      "Epoch 291 Training loss: 0.322 Validation loss : 0.536\n",
      "Epoch 1 Training loss: 1.483 Validation loss : 1.252\n",
      "Epoch 11 Training loss: 0.608 Validation loss : 0.710\n",
      "Epoch 21 Training loss: 0.562 Validation loss : 0.649\n",
      "Epoch 31 Training loss: 0.522 Validation loss : 0.621\n",
      "Epoch 41 Training loss: 0.506 Validation loss : 0.603\n",
      "Epoch 51 Training loss: 0.494 Validation loss : 0.592\n",
      "Epoch 61 Training loss: 0.499 Validation loss : 0.587\n",
      "Epoch 71 Training loss: 0.481 Validation loss : 0.584\n",
      "Epoch 81 Training loss: 0.476 Validation loss : 0.578\n",
      "Epoch 91 Training loss: 0.472 Validation loss : 0.576\n",
      "Epoch 101 Training loss: 0.471 Validation loss : 0.578\n",
      "Epoch 111 Training loss: 0.468 Validation loss : 0.573\n",
      "Epoch 121 Training loss: 0.466 Validation loss : 0.571\n",
      "Epoch 131 Training loss: 0.464 Validation loss : 0.570\n",
      "Epoch 141 Training loss: 0.462 Validation loss : 0.571\n",
      "Epoch 151 Training loss: 0.460 Validation loss : 0.569\n",
      "Epoch 161 Training loss: 0.459 Validation loss : 0.567\n",
      "Epoch 171 Training loss: 0.459 Validation loss : 0.567\n",
      "Epoch 181 Training loss: 0.458 Validation loss : 0.566\n",
      "Epoch 191 Training loss: 0.457 Validation loss : 0.564\n",
      "Epoch 201 Training loss: 0.456 Validation loss : 0.564\n",
      "Epoch 211 Training loss: 0.455 Validation loss : 0.563\n",
      "Epoch 221 Training loss: 0.455 Validation loss : 0.563\n",
      "Epoch 231 Training loss: 0.454 Validation loss : 0.563\n",
      "Epoch 241 Training loss: 0.529 Validation loss : 0.563\n",
      "Epoch 251 Training loss: 0.453 Validation loss : 0.563\n",
      "Epoch 261 Training loss: 0.453 Validation loss : 0.563\n",
      "Epoch 271 Training loss: 0.454 Validation loss : 0.562\n",
      "Epoch 281 Training loss: 0.452 Validation loss : 0.562\n",
      "Epoch 291 Training loss: 0.452 Validation loss : 0.562\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133, (16, 0.01): 27.955728759765623, (16, 0.001): 27.303933029174804, (16, 0.0001): 32.252861404418944, (32, 0.01): 23.296868057250975, (32, 0.001): 21.532077102661134, (32, 0.0001): 31.554113388061523, (64, 0.01): 23.399887084960938, (64, 0.001): 15.678517532348632, (64, 0.0001): 24.91776496887207}\n",
      "Epoch 1 Training loss: 1.121 Validation loss : 1.148\n",
      "Epoch 11 Training loss: 0.570 Validation loss : 0.711\n",
      "Epoch 21 Training loss: 0.526 Validation loss : 0.643\n",
      "Epoch 31 Training loss: 0.476 Validation loss : 0.730\n",
      "Epoch 41 Training loss: 0.442 Validation loss : 0.567\n",
      "Epoch 51 Training loss: 0.439 Validation loss : 0.555\n",
      "Epoch 61 Training loss: 0.428 Validation loss : 0.624\n",
      "Epoch 71 Training loss: 0.375 Validation loss : 0.545\n",
      "Epoch 81 Training loss: 0.354 Validation loss : 0.589\n",
      "Epoch 91 Training loss: 0.343 Validation loss : 0.605\n",
      "Epoch 101 Training loss: 0.324 Validation loss : 0.541\n",
      "Epoch 111 Training loss: 0.316 Validation loss : 0.533\n",
      "Epoch 121 Training loss: 0.306 Validation loss : 0.550\n",
      "Epoch 131 Training loss: 0.285 Validation loss : 0.596\n",
      "Epoch 141 Training loss: 0.278 Validation loss : 0.545\n",
      "Epoch 151 Training loss: 0.261 Validation loss : 0.542\n",
      "Epoch 161 Training loss: 0.251 Validation loss : 0.580\n",
      "Epoch 171 Training loss: 0.253 Validation loss : 0.586\n",
      "Epoch 181 Training loss: 0.241 Validation loss : 0.620\n",
      "Epoch 191 Training loss: 0.238 Validation loss : 0.613\n",
      "Epoch 201 Training loss: 0.234 Validation loss : 0.625\n",
      "Epoch 211 Training loss: 0.230 Validation loss : 0.641\n",
      "Epoch 221 Training loss: 0.228 Validation loss : 0.613\n",
      "Epoch 231 Training loss: 0.223 Validation loss : 0.664\n",
      "Epoch 241 Training loss: 0.221 Validation loss : 0.661\n",
      "Epoch 251 Training loss: 0.218 Validation loss : 0.684\n",
      "Epoch 261 Training loss: 0.215 Validation loss : 0.672\n",
      "Epoch 271 Training loss: 0.213 Validation loss : 0.680\n",
      "Epoch 281 Training loss: 0.211 Validation loss : 0.705\n",
      "Epoch 291 Training loss: 0.210 Validation loss : 0.684\n",
      "Epoch 1 Training loss: 0.798 Validation loss : 0.719\n",
      "Epoch 11 Training loss: 0.500 Validation loss : 0.604\n",
      "Epoch 21 Training loss: 0.469 Validation loss : 0.541\n",
      "Epoch 31 Training loss: 0.446 Validation loss : 0.518\n",
      "Epoch 41 Training loss: 0.438 Validation loss : 0.514\n",
      "Epoch 51 Training loss: 0.397 Validation loss : 0.545\n",
      "Epoch 61 Training loss: 0.372 Validation loss : 0.565\n",
      "Epoch 71 Training loss: 0.346 Validation loss : 0.537\n",
      "Epoch 81 Training loss: 0.327 Validation loss : 0.536\n",
      "Epoch 91 Training loss: 0.307 Validation loss : 0.533\n",
      "Epoch 101 Training loss: 0.289 Validation loss : 0.548\n",
      "Epoch 111 Training loss: 0.266 Validation loss : 0.575\n",
      "Epoch 121 Training loss: 0.250 Validation loss : 0.603\n",
      "Epoch 131 Training loss: 0.232 Validation loss : 0.602\n",
      "Epoch 141 Training loss: 0.216 Validation loss : 0.622\n",
      "Epoch 151 Training loss: 0.201 Validation loss : 0.617\n",
      "Epoch 161 Training loss: 0.189 Validation loss : 0.618\n",
      "Epoch 171 Training loss: 0.180 Validation loss : 0.614\n",
      "Epoch 181 Training loss: 0.171 Validation loss : 0.604\n",
      "Epoch 191 Training loss: 0.164 Validation loss : 0.610\n",
      "Epoch 201 Training loss: 0.161 Validation loss : 0.633\n",
      "Epoch 211 Training loss: 0.149 Validation loss : 0.614\n",
      "Epoch 221 Training loss: 0.146 Validation loss : 0.632\n",
      "Epoch 231 Training loss: 0.141 Validation loss : 0.638\n",
      "Epoch 241 Training loss: 0.138 Validation loss : 0.640\n",
      "Epoch 251 Training loss: 0.134 Validation loss : 0.645\n",
      "Epoch 261 Training loss: 0.132 Validation loss : 0.651\n",
      "Epoch 271 Training loss: 0.128 Validation loss : 0.651\n",
      "Epoch 281 Training loss: 0.127 Validation loss : 0.651\n",
      "Epoch 291 Training loss: 0.124 Validation loss : 0.649\n",
      "Epoch 1 Training loss: 1.146 Validation loss : 0.938\n",
      "Epoch 11 Training loss: 0.537 Validation loss : 0.623\n",
      "Epoch 21 Training loss: 0.502 Validation loss : 0.588\n",
      "Epoch 31 Training loss: 0.484 Validation loss : 0.573\n",
      "Epoch 41 Training loss: 0.474 Validation loss : 0.567\n",
      "Epoch 51 Training loss: 0.466 Validation loss : 0.566\n",
      "Epoch 61 Training loss: 0.459 Validation loss : 0.568\n",
      "Epoch 71 Training loss: 0.457 Validation loss : 0.579\n",
      "Epoch 81 Training loss: 0.453 Validation loss : 0.562\n",
      "Epoch 91 Training loss: 0.453 Validation loss : 0.562\n",
      "Epoch 101 Training loss: 0.447 Validation loss : 0.555\n",
      "Epoch 111 Training loss: 0.449 Validation loss : 0.562\n",
      "Epoch 121 Training loss: 0.446 Validation loss : 0.559\n",
      "Epoch 131 Training loss: 0.444 Validation loss : 0.553\n",
      "Epoch 141 Training loss: 0.444 Validation loss : 0.557\n",
      "Epoch 151 Training loss: 0.442 Validation loss : 0.556\n",
      "Epoch 161 Training loss: 0.441 Validation loss : 0.560\n",
      "Epoch 171 Training loss: 0.441 Validation loss : 0.558\n",
      "Epoch 181 Training loss: 0.439 Validation loss : 0.561\n",
      "Epoch 191 Training loss: 0.439 Validation loss : 0.557\n",
      "Epoch 201 Training loss: 0.438 Validation loss : 0.557\n",
      "Epoch 211 Training loss: 0.439 Validation loss : 0.556\n",
      "Epoch 221 Training loss: 0.436 Validation loss : 0.555\n",
      "Epoch 231 Training loss: 0.436 Validation loss : 0.556\n",
      "Epoch 241 Training loss: 0.435 Validation loss : 0.556\n",
      "Epoch 251 Training loss: 0.437 Validation loss : 0.555\n",
      "Epoch 261 Training loss: 0.434 Validation loss : 0.556\n",
      "Epoch 271 Training loss: 0.438 Validation loss : 0.556\n",
      "Epoch 281 Training loss: 0.437 Validation loss : 0.556\n",
      "Epoch 291 Training loss: 0.434 Validation loss : 0.556\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133, (16, 0.01): 27.955728759765623, (16, 0.001): 27.303933029174804, (16, 0.0001): 32.252861404418944, (32, 0.01): 23.296868057250975, (32, 0.001): 21.532077102661134, (32, 0.0001): 31.554113388061523, (64, 0.01): 23.399887084960938, (64, 0.001): 15.678517532348632, (64, 0.0001): 24.91776496887207, (128, 0.01): 26.84285675048828, (128, 0.001): 19.919128494262694, (128, 0.0001): 24.513163681030274}\n",
      "Epoch 1 Training loss: 1.420 Validation loss : 0.646\n",
      "Epoch 11 Training loss: 0.561 Validation loss : 0.658\n",
      "Epoch 21 Training loss: 0.589 Validation loss : 0.606\n",
      "Epoch 31 Training loss: 0.496 Validation loss : 0.669\n",
      "Epoch 41 Training loss: 0.478 Validation loss : 0.460\n",
      "Epoch 51 Training loss: 0.415 Validation loss : 0.497\n",
      "Epoch 61 Training loss: 0.405 Validation loss : 0.539\n",
      "Epoch 71 Training loss: 0.369 Validation loss : 0.535\n",
      "Epoch 81 Training loss: 0.368 Validation loss : 0.587\n",
      "Epoch 91 Training loss: 0.339 Validation loss : 0.566\n",
      "Epoch 101 Training loss: 0.318 Validation loss : 0.586\n",
      "Epoch 111 Training loss: 0.310 Validation loss : 0.593\n",
      "Epoch 121 Training loss: 0.299 Validation loss : 0.582\n",
      "Epoch 131 Training loss: 0.285 Validation loss : 0.581\n",
      "Epoch 141 Training loss: 0.266 Validation loss : 0.633\n",
      "Epoch 151 Training loss: 0.248 Validation loss : 0.668\n",
      "Epoch 161 Training loss: 0.238 Validation loss : 0.674\n",
      "Epoch 171 Training loss: 0.227 Validation loss : 0.645\n",
      "Epoch 181 Training loss: 0.219 Validation loss : 0.714\n",
      "Epoch 191 Training loss: 0.215 Validation loss : 0.712\n",
      "Epoch 201 Training loss: 0.212 Validation loss : 0.736\n",
      "Epoch 211 Training loss: 0.205 Validation loss : 0.734\n",
      "Epoch 221 Training loss: 0.196 Validation loss : 0.741\n",
      "Epoch 231 Training loss: 0.192 Validation loss : 0.716\n",
      "Epoch 241 Training loss: 0.191 Validation loss : 0.722\n",
      "Epoch 251 Training loss: 0.182 Validation loss : 0.744\n",
      "Epoch 261 Training loss: 0.181 Validation loss : 0.745\n",
      "Epoch 271 Training loss: 0.177 Validation loss : 0.751\n",
      "Epoch 281 Training loss: 0.175 Validation loss : 0.723\n",
      "Epoch 291 Training loss: 0.173 Validation loss : 0.723\n",
      "Epoch 1 Training loss: 0.712 Validation loss : 0.696\n",
      "Epoch 11 Training loss: 0.492 Validation loss : 0.582\n",
      "Epoch 21 Training loss: 0.484 Validation loss : 0.632\n",
      "Epoch 31 Training loss: 0.436 Validation loss : 0.558\n",
      "Epoch 41 Training loss: 0.393 Validation loss : 0.521\n",
      "Epoch 51 Training loss: 0.350 Validation loss : 0.558\n",
      "Epoch 61 Training loss: 0.335 Validation loss : 0.574\n",
      "Epoch 71 Training loss: 0.285 Validation loss : 0.551\n",
      "Epoch 81 Training loss: 0.258 Validation loss : 0.563\n",
      "Epoch 91 Training loss: 0.212 Validation loss : 0.563\n",
      "Epoch 101 Training loss: 0.195 Validation loss : 0.584\n",
      "Epoch 111 Training loss: 0.169 Validation loss : 0.602\n",
      "Epoch 121 Training loss: 0.161 Validation loss : 0.582\n",
      "Epoch 131 Training loss: 0.147 Validation loss : 0.611\n",
      "Epoch 141 Training loss: 0.142 Validation loss : 0.627\n",
      "Epoch 151 Training loss: 0.135 Validation loss : 0.636\n",
      "Epoch 161 Training loss: 0.123 Validation loss : 0.626\n",
      "Epoch 171 Training loss: 0.119 Validation loss : 0.670\n",
      "Epoch 181 Training loss: 0.116 Validation loss : 0.673\n",
      "Epoch 191 Training loss: 0.110 Validation loss : 0.686\n",
      "Epoch 201 Training loss: 0.109 Validation loss : 0.672\n",
      "Epoch 211 Training loss: 0.107 Validation loss : 0.694\n",
      "Epoch 221 Training loss: 0.106 Validation loss : 0.685\n",
      "Epoch 231 Training loss: 0.102 Validation loss : 0.706\n",
      "Epoch 241 Training loss: 0.100 Validation loss : 0.704\n",
      "Epoch 251 Training loss: 0.099 Validation loss : 0.707\n",
      "Epoch 261 Training loss: 0.099 Validation loss : 0.723\n",
      "Epoch 271 Training loss: 0.096 Validation loss : 0.725\n",
      "Epoch 281 Training loss: 0.096 Validation loss : 0.729\n",
      "Epoch 291 Training loss: 0.094 Validation loss : 0.743\n",
      "Epoch 1 Training loss: 0.908 Validation loss : 0.778\n",
      "Epoch 11 Training loss: 0.503 Validation loss : 0.618\n",
      "Epoch 21 Training loss: 0.467 Validation loss : 0.582\n",
      "Epoch 31 Training loss: 0.454 Validation loss : 0.559\n",
      "Epoch 41 Training loss: 0.455 Validation loss : 0.553\n",
      "Epoch 51 Training loss: 0.437 Validation loss : 0.562\n",
      "Epoch 61 Training loss: 0.444 Validation loss : 0.549\n",
      "Epoch 71 Training loss: 0.441 Validation loss : 0.562\n",
      "Epoch 81 Training loss: 0.432 Validation loss : 0.545\n",
      "Epoch 91 Training loss: 0.434 Validation loss : 0.544\n",
      "Epoch 101 Training loss: 0.431 Validation loss : 0.544\n",
      "Epoch 111 Training loss: 0.421 Validation loss : 0.539\n",
      "Epoch 121 Training loss: 0.414 Validation loss : 0.587\n",
      "Epoch 131 Training loss: 0.415 Validation loss : 0.539\n",
      "Epoch 141 Training loss: 0.413 Validation loss : 0.544\n",
      "Epoch 151 Training loss: 0.415 Validation loss : 0.541\n",
      "Epoch 161 Training loss: 0.407 Validation loss : 0.536\n",
      "Epoch 171 Training loss: 0.406 Validation loss : 0.540\n",
      "Epoch 181 Training loss: 0.412 Validation loss : 0.537\n",
      "Epoch 191 Training loss: 0.401 Validation loss : 0.541\n",
      "Epoch 201 Training loss: 0.398 Validation loss : 0.549\n",
      "Epoch 211 Training loss: 0.398 Validation loss : 0.535\n",
      "Epoch 221 Training loss: 0.397 Validation loss : 0.539\n",
      "Epoch 231 Training loss: 0.395 Validation loss : 0.538\n",
      "Epoch 241 Training loss: 0.394 Validation loss : 0.541\n",
      "Epoch 251 Training loss: 0.393 Validation loss : 0.536\n",
      "Epoch 261 Training loss: 0.392 Validation loss : 0.540\n",
      "Epoch 271 Training loss: 0.391 Validation loss : 0.539\n",
      "Epoch 281 Training loss: 0.391 Validation loss : 0.538\n",
      "Epoch 291 Training loss: 0.390 Validation loss : 0.537\n",
      "{(8, 0.01): 29.468596038818358, (8, 0.001): 30.0545938873291, (8, 0.0001): 103.73010696411133, (16, 0.01): 27.955728759765623, (16, 0.001): 27.303933029174804, (16, 0.0001): 32.252861404418944, (32, 0.01): 23.296868057250975, (32, 0.001): 21.532077102661134, (32, 0.0001): 31.554113388061523, (64, 0.01): 23.399887084960938, (64, 0.001): 15.678517532348632, (64, 0.0001): 24.91776496887207, (128, 0.01): 26.84285675048828, (128, 0.001): 19.919128494262694, (128, 0.0001): 24.513163681030274, (256, 0.01): 19.14558464050293, (256, 0.001): 20.44915901184082, (256, 0.0001): 15.713558578491211}\n"
     ]
    }
   ],
   "source": [
    "wee3_pred_2_18 = week3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "week3_mae = wee3_pred_2_18[0][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "week3_mlp_pred = wee3_pred_2_18[1][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 1.148 Validation loss : 0.691\n",
      "Epoch 11 Training loss: 0.801 Validation loss : 0.608\n",
      "Epoch 21 Training loss: 0.771 Validation loss : 0.589\n",
      "Epoch 31 Training loss: 0.755 Validation loss : 0.612\n",
      "Epoch 41 Training loss: 0.748 Validation loss : 0.590\n",
      "Epoch 51 Training loss: 0.701 Validation loss : 0.589\n",
      "Epoch 61 Training loss: 0.693 Validation loss : 0.612\n",
      "Epoch 71 Training loss: 0.690 Validation loss : 0.606\n",
      "Epoch 81 Training loss: 0.620 Validation loss : 0.641\n",
      "Epoch 91 Training loss: 0.606 Validation loss : 0.636\n",
      "Epoch 101 Training loss: 0.584 Validation loss : 0.757\n",
      "Epoch 111 Training loss: 0.557 Validation loss : 0.852\n",
      "Epoch 121 Training loss: 0.551 Validation loss : 0.694\n",
      "Epoch 131 Training loss: 0.548 Validation loss : 0.592\n",
      "Epoch 141 Training loss: 0.536 Validation loss : 0.626\n",
      "Epoch 151 Training loss: 0.506 Validation loss : 0.598\n",
      "Epoch 161 Training loss: 0.509 Validation loss : 0.600\n",
      "Epoch 171 Training loss: 0.497 Validation loss : 0.615\n",
      "Epoch 181 Training loss: 0.489 Validation loss : 0.624\n",
      "Epoch 191 Training loss: 0.484 Validation loss : 0.610\n",
      "Epoch 201 Training loss: 0.478 Validation loss : 0.634\n",
      "Epoch 211 Training loss: 0.473 Validation loss : 0.621\n",
      "Epoch 221 Training loss: 0.459 Validation loss : 0.636\n",
      "Epoch 231 Training loss: 0.454 Validation loss : 0.621\n",
      "Epoch 241 Training loss: 0.455 Validation loss : 0.644\n",
      "Epoch 251 Training loss: 0.451 Validation loss : 0.657\n",
      "Epoch 261 Training loss: 0.450 Validation loss : 0.659\n",
      "Epoch 271 Training loss: 0.446 Validation loss : 0.663\n",
      "Epoch 281 Training loss: 0.442 Validation loss : 0.663\n",
      "Epoch 291 Training loss: 0.440 Validation loss : 0.661\n",
      "Epoch 1 Training loss: 1.772 Validation loss : 1.183\n",
      "Epoch 11 Training loss: 0.836 Validation loss : 0.628\n",
      "Epoch 21 Training loss: 0.775 Validation loss : 0.605\n",
      "Epoch 31 Training loss: 0.761 Validation loss : 0.600\n",
      "Epoch 41 Training loss: 0.745 Validation loss : 0.595\n",
      "Epoch 51 Training loss: 0.738 Validation loss : 0.585\n",
      "Epoch 61 Training loss: 0.733 Validation loss : 0.580\n",
      "Epoch 71 Training loss: 0.727 Validation loss : 0.579\n",
      "Epoch 81 Training loss: 0.719 Validation loss : 0.582\n",
      "Epoch 91 Training loss: 0.724 Validation loss : 0.571\n",
      "Epoch 101 Training loss: 0.719 Validation loss : 0.568\n",
      "Epoch 111 Training loss: 0.716 Validation loss : 0.566\n",
      "Epoch 121 Training loss: 0.714 Validation loss : 0.564\n",
      "Epoch 131 Training loss: 0.712 Validation loss : 0.566\n",
      "Epoch 141 Training loss: 0.711 Validation loss : 0.563\n",
      "Epoch 151 Training loss: 0.708 Validation loss : 0.566\n",
      "Epoch 161 Training loss: 0.712 Validation loss : 0.562\n",
      "Epoch 171 Training loss: 0.707 Validation loss : 0.560\n",
      "Epoch 181 Training loss: 0.725 Validation loss : 0.561\n",
      "Epoch 191 Training loss: 0.706 Validation loss : 0.559\n",
      "Epoch 201 Training loss: 0.705 Validation loss : 0.559\n",
      "Epoch 211 Training loss: 0.704 Validation loss : 0.559\n",
      "Epoch 221 Training loss: 0.704 Validation loss : 0.559\n",
      "Epoch 231 Training loss: 0.703 Validation loss : 0.559\n",
      "Epoch 241 Training loss: 0.710 Validation loss : 0.558\n",
      "Epoch 251 Training loss: 0.702 Validation loss : 0.558\n",
      "Epoch 261 Training loss: 0.702 Validation loss : 0.559\n",
      "Epoch 271 Training loss: 0.701 Validation loss : 0.558\n",
      "Epoch 281 Training loss: 0.702 Validation loss : 0.558\n",
      "Epoch 291 Training loss: 0.701 Validation loss : 0.558\n",
      "Epoch 1 Training loss: 1.677 Validation loss : 1.226\n",
      "Epoch 11 Training loss: 1.259 Validation loss : 0.938\n",
      "Epoch 21 Training loss: 1.077 Validation loss : 0.830\n",
      "Epoch 31 Training loss: 1.029 Validation loss : 0.783\n",
      "Epoch 41 Training loss: 0.999 Validation loss : 0.753\n",
      "Epoch 51 Training loss: 0.978 Validation loss : 0.732\n",
      "Epoch 61 Training loss: 0.960 Validation loss : 0.717\n",
      "Epoch 71 Training loss: 0.944 Validation loss : 0.706\n",
      "Epoch 81 Training loss: 0.932 Validation loss : 0.696\n",
      "Epoch 91 Training loss: 0.921 Validation loss : 0.689\n",
      "Epoch 101 Training loss: 0.913 Validation loss : 0.682\n",
      "Epoch 111 Training loss: 0.905 Validation loss : 0.677\n",
      "Epoch 121 Training loss: 0.898 Validation loss : 0.672\n",
      "Epoch 131 Training loss: 0.892 Validation loss : 0.667\n",
      "Epoch 141 Training loss: 0.887 Validation loss : 0.664\n",
      "Epoch 151 Training loss: 0.912 Validation loss : 0.660\n",
      "Epoch 161 Training loss: 0.878 Validation loss : 0.658\n",
      "Epoch 171 Training loss: 0.875 Validation loss : 0.655\n",
      "Epoch 181 Training loss: 0.872 Validation loss : 0.653\n",
      "Epoch 191 Training loss: 0.869 Validation loss : 0.651\n",
      "Epoch 201 Training loss: 0.868 Validation loss : 0.649\n",
      "Epoch 211 Training loss: 0.864 Validation loss : 0.648\n",
      "Epoch 221 Training loss: 0.863 Validation loss : 0.647\n",
      "Epoch 231 Training loss: 0.862 Validation loss : 0.645\n",
      "Epoch 241 Training loss: 0.859 Validation loss : 0.644\n",
      "Epoch 251 Training loss: 0.858 Validation loss : 0.643\n",
      "Epoch 261 Training loss: 0.858 Validation loss : 0.643\n",
      "Epoch 271 Training loss: 0.862 Validation loss : 0.642\n",
      "Epoch 281 Training loss: 0.854 Validation loss : 0.641\n",
      "Epoch 291 Training loss: 0.853 Validation loss : 0.641\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287}\n",
      "Epoch 1 Training loss: 1.046 Validation loss : 0.640\n",
      "Epoch 11 Training loss: 0.819 Validation loss : 0.623\n",
      "Epoch 21 Training loss: 0.761 Validation loss : 0.590\n",
      "Epoch 31 Training loss: 0.746 Validation loss : 0.610\n",
      "Epoch 41 Training loss: 0.736 Validation loss : 0.637\n",
      "Epoch 51 Training loss: 0.686 Validation loss : 0.539\n",
      "Epoch 61 Training loss: 0.656 Validation loss : 0.617\n",
      "Epoch 71 Training loss: 0.605 Validation loss : 0.630\n",
      "Epoch 81 Training loss: 0.576 Validation loss : 0.581\n",
      "Epoch 91 Training loss: 0.569 Validation loss : 0.617\n",
      "Epoch 101 Training loss: 0.523 Validation loss : 0.616\n",
      "Epoch 111 Training loss: 0.498 Validation loss : 0.629\n",
      "Epoch 121 Training loss: 0.465 Validation loss : 0.693\n",
      "Epoch 131 Training loss: 0.465 Validation loss : 0.703\n",
      "Epoch 141 Training loss: 0.448 Validation loss : 0.693\n",
      "Epoch 151 Training loss: 0.439 Validation loss : 0.775\n",
      "Epoch 161 Training loss: 0.428 Validation loss : 0.755\n",
      "Epoch 171 Training loss: 0.431 Validation loss : 0.762\n",
      "Epoch 181 Training loss: 0.426 Validation loss : 0.788\n",
      "Epoch 191 Training loss: 0.417 Validation loss : 0.741\n",
      "Epoch 201 Training loss: 0.420 Validation loss : 0.758\n",
      "Epoch 211 Training loss: 0.414 Validation loss : 0.748\n",
      "Epoch 221 Training loss: 0.421 Validation loss : 0.765\n",
      "Epoch 231 Training loss: 0.408 Validation loss : 0.802\n",
      "Epoch 241 Training loss: 0.407 Validation loss : 0.795\n",
      "Epoch 251 Training loss: 0.406 Validation loss : 0.811\n",
      "Epoch 261 Training loss: 0.405 Validation loss : 0.811\n",
      "Epoch 271 Training loss: 0.403 Validation loss : 0.809\n",
      "Epoch 281 Training loss: 0.403 Validation loss : 0.809\n",
      "Epoch 291 Training loss: 0.401 Validation loss : 0.815\n",
      "Epoch 1 Training loss: 1.381 Validation loss : 0.892\n",
      "Epoch 11 Training loss: 0.803 Validation loss : 0.595\n",
      "Epoch 21 Training loss: 0.779 Validation loss : 0.572\n",
      "Epoch 31 Training loss: 0.753 Validation loss : 0.606\n",
      "Epoch 41 Training loss: 0.738 Validation loss : 0.602\n",
      "Epoch 51 Training loss: 0.732 Validation loss : 0.601\n",
      "Epoch 61 Training loss: 0.723 Validation loss : 0.571\n",
      "Epoch 71 Training loss: 0.720 Validation loss : 0.563\n",
      "Epoch 81 Training loss: 0.717 Validation loss : 0.559\n",
      "Epoch 91 Training loss: 0.710 Validation loss : 0.582\n",
      "Epoch 101 Training loss: 0.706 Validation loss : 0.560\n",
      "Epoch 111 Training loss: 0.701 Validation loss : 0.574\n",
      "Epoch 121 Training loss: 0.696 Validation loss : 0.554\n",
      "Epoch 131 Training loss: 0.698 Validation loss : 0.562\n",
      "Epoch 141 Training loss: 0.694 Validation loss : 0.557\n",
      "Epoch 151 Training loss: 0.693 Validation loss : 0.559\n",
      "Epoch 161 Training loss: 0.691 Validation loss : 0.556\n",
      "Epoch 171 Training loss: 0.689 Validation loss : 0.553\n",
      "Epoch 181 Training loss: 0.693 Validation loss : 0.556\n",
      "Epoch 191 Training loss: 0.695 Validation loss : 0.553\n",
      "Epoch 201 Training loss: 0.684 Validation loss : 0.555\n",
      "Epoch 211 Training loss: 0.683 Validation loss : 0.553\n",
      "Epoch 221 Training loss: 0.682 Validation loss : 0.551\n",
      "Epoch 231 Training loss: 0.681 Validation loss : 0.553\n",
      "Epoch 241 Training loss: 0.681 Validation loss : 0.552\n",
      "Epoch 251 Training loss: 0.680 Validation loss : 0.554\n",
      "Epoch 261 Training loss: 0.679 Validation loss : 0.552\n",
      "Epoch 271 Training loss: 0.679 Validation loss : 0.552\n",
      "Epoch 281 Training loss: 0.680 Validation loss : 0.552\n",
      "Epoch 291 Training loss: 0.678 Validation loss : 0.552\n",
      "Epoch 1 Training loss: 1.967 Validation loss : 1.424\n",
      "Epoch 11 Training loss: 1.327 Validation loss : 1.015\n",
      "Epoch 21 Training loss: 1.107 Validation loss : 0.832\n",
      "Epoch 31 Training loss: 1.018 Validation loss : 0.758\n",
      "Epoch 41 Training loss: 0.963 Validation loss : 0.714\n",
      "Epoch 51 Training loss: 0.928 Validation loss : 0.687\n",
      "Epoch 61 Training loss: 0.907 Validation loss : 0.670\n",
      "Epoch 71 Training loss: 0.890 Validation loss : 0.657\n",
      "Epoch 81 Training loss: 0.877 Validation loss : 0.648\n",
      "Epoch 91 Training loss: 0.876 Validation loss : 0.641\n",
      "Epoch 101 Training loss: 0.857 Validation loss : 0.635\n",
      "Epoch 111 Training loss: 0.850 Validation loss : 0.631\n",
      "Epoch 121 Training loss: 0.850 Validation loss : 0.626\n",
      "Epoch 131 Training loss: 0.837 Validation loss : 0.623\n",
      "Epoch 141 Training loss: 0.832 Validation loss : 0.620\n",
      "Epoch 151 Training loss: 0.827 Validation loss : 0.618\n",
      "Epoch 161 Training loss: 0.825 Validation loss : 0.615\n",
      "Epoch 171 Training loss: 0.820 Validation loss : 0.613\n",
      "Epoch 181 Training loss: 0.816 Validation loss : 0.612\n",
      "Epoch 191 Training loss: 0.819 Validation loss : 0.610\n",
      "Epoch 201 Training loss: 0.810 Validation loss : 0.609\n",
      "Epoch 211 Training loss: 0.808 Validation loss : 0.609\n",
      "Epoch 221 Training loss: 0.807 Validation loss : 0.608\n",
      "Epoch 231 Training loss: 0.805 Validation loss : 0.607\n",
      "Epoch 241 Training loss: 0.804 Validation loss : 0.607\n",
      "Epoch 251 Training loss: 0.802 Validation loss : 0.606\n",
      "Epoch 261 Training loss: 0.801 Validation loss : 0.606\n",
      "Epoch 271 Training loss: 0.800 Validation loss : 0.605\n",
      "Epoch 281 Training loss: 0.801 Validation loss : 0.605\n",
      "Epoch 291 Training loss: 0.798 Validation loss : 0.604\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287, (16, 0.01): 34.13525451660156, (16, 0.001): 22.973655395507812, (16, 0.0001): 19.510242080688478}\n",
      "Epoch 1 Training loss: 1.094 Validation loss : 0.636\n",
      "Epoch 11 Training loss: 0.813 Validation loss : 0.695\n",
      "Epoch 21 Training loss: 0.761 Validation loss : 0.639\n",
      "Epoch 31 Training loss: 0.674 Validation loss : 0.560\n",
      "Epoch 41 Training loss: 0.625 Validation loss : 0.619\n",
      "Epoch 51 Training loss: 0.584 Validation loss : 0.657\n",
      "Epoch 61 Training loss: 0.531 Validation loss : 0.666\n",
      "Epoch 71 Training loss: 0.460 Validation loss : 0.731\n",
      "Epoch 81 Training loss: 0.456 Validation loss : 0.661\n",
      "Epoch 91 Training loss: 0.409 Validation loss : 0.683\n",
      "Epoch 101 Training loss: 0.394 Validation loss : 0.665\n",
      "Epoch 111 Training loss: 0.379 Validation loss : 0.655\n",
      "Epoch 121 Training loss: 0.380 Validation loss : 0.749\n",
      "Epoch 131 Training loss: 0.364 Validation loss : 0.786\n",
      "Epoch 141 Training loss: 0.352 Validation loss : 0.739\n",
      "Epoch 151 Training loss: 0.344 Validation loss : 0.749\n",
      "Epoch 161 Training loss: 0.334 Validation loss : 0.757\n",
      "Epoch 171 Training loss: 0.330 Validation loss : 0.767\n",
      "Epoch 181 Training loss: 0.316 Validation loss : 0.768\n",
      "Epoch 191 Training loss: 0.311 Validation loss : 0.784\n",
      "Epoch 201 Training loss: 0.312 Validation loss : 0.845\n",
      "Epoch 211 Training loss: 0.302 Validation loss : 0.842\n",
      "Epoch 221 Training loss: 0.300 Validation loss : 0.809\n",
      "Epoch 231 Training loss: 0.295 Validation loss : 0.848\n",
      "Epoch 241 Training loss: 0.294 Validation loss : 0.822\n",
      "Epoch 251 Training loss: 0.293 Validation loss : 0.867\n",
      "Epoch 261 Training loss: 0.286 Validation loss : 0.875\n",
      "Epoch 271 Training loss: 0.285 Validation loss : 0.901\n",
      "Epoch 281 Training loss: 0.281 Validation loss : 0.900\n",
      "Epoch 291 Training loss: 0.280 Validation loss : 0.905\n",
      "Epoch 1 Training loss: 1.256 Validation loss : 0.739\n",
      "Epoch 11 Training loss: 0.762 Validation loss : 0.587\n",
      "Epoch 21 Training loss: 0.731 Validation loss : 0.556\n",
      "Epoch 31 Training loss: 0.720 Validation loss : 0.570\n",
      "Epoch 41 Training loss: 0.706 Validation loss : 0.563\n",
      "Epoch 51 Training loss: 0.701 Validation loss : 0.536\n",
      "Epoch 61 Training loss: 0.689 Validation loss : 0.534\n",
      "Epoch 71 Training loss: 0.673 Validation loss : 0.540\n",
      "Epoch 81 Training loss: 0.665 Validation loss : 0.537\n",
      "Epoch 91 Training loss: 0.651 Validation loss : 0.529\n",
      "Epoch 101 Training loss: 0.640 Validation loss : 0.536\n",
      "Epoch 111 Training loss: 0.639 Validation loss : 0.544\n",
      "Epoch 121 Training loss: 0.630 Validation loss : 0.533\n",
      "Epoch 131 Training loss: 0.624 Validation loss : 0.533\n",
      "Epoch 141 Training loss: 0.621 Validation loss : 0.532\n",
      "Epoch 151 Training loss: 0.613 Validation loss : 0.540\n",
      "Epoch 161 Training loss: 0.609 Validation loss : 0.529\n",
      "Epoch 171 Training loss: 0.605 Validation loss : 0.529\n",
      "Epoch 181 Training loss: 0.602 Validation loss : 0.537\n",
      "Epoch 191 Training loss: 0.597 Validation loss : 0.526\n",
      "Epoch 201 Training loss: 0.594 Validation loss : 0.541\n",
      "Epoch 211 Training loss: 0.592 Validation loss : 0.527\n",
      "Epoch 221 Training loss: 0.589 Validation loss : 0.528\n",
      "Epoch 231 Training loss: 0.586 Validation loss : 0.531\n",
      "Epoch 241 Training loss: 0.591 Validation loss : 0.528\n",
      "Epoch 251 Training loss: 0.584 Validation loss : 0.528\n",
      "Epoch 261 Training loss: 0.582 Validation loss : 0.529\n",
      "Epoch 271 Training loss: 0.580 Validation loss : 0.529\n",
      "Epoch 281 Training loss: 0.578 Validation loss : 0.528\n",
      "Epoch 291 Training loss: 0.578 Validation loss : 0.528\n",
      "Epoch 1 Training loss: 1.603 Validation loss : 1.148\n",
      "Epoch 11 Training loss: 0.942 Validation loss : 0.696\n",
      "Epoch 21 Training loss: 0.849 Validation loss : 0.620\n",
      "Epoch 31 Training loss: 0.819 Validation loss : 0.602\n",
      "Epoch 41 Training loss: 0.807 Validation loss : 0.594\n",
      "Epoch 51 Training loss: 0.791 Validation loss : 0.591\n",
      "Epoch 61 Training loss: 0.781 Validation loss : 0.589\n",
      "Epoch 71 Training loss: 0.774 Validation loss : 0.588\n",
      "Epoch 81 Training loss: 0.769 Validation loss : 0.587\n",
      "Epoch 91 Training loss: 0.763 Validation loss : 0.586\n",
      "Epoch 101 Training loss: 0.759 Validation loss : 0.584\n",
      "Epoch 111 Training loss: 0.755 Validation loss : 0.583\n",
      "Epoch 121 Training loss: 0.760 Validation loss : 0.583\n",
      "Epoch 131 Training loss: 0.780 Validation loss : 0.583\n",
      "Epoch 141 Training loss: 0.748 Validation loss : 0.582\n",
      "Epoch 151 Training loss: 0.746 Validation loss : 0.582\n",
      "Epoch 161 Training loss: 0.744 Validation loss : 0.582\n",
      "Epoch 171 Training loss: 0.745 Validation loss : 0.582\n",
      "Epoch 181 Training loss: 0.742 Validation loss : 0.582\n",
      "Epoch 191 Training loss: 0.740 Validation loss : 0.581\n",
      "Epoch 201 Training loss: 0.739 Validation loss : 0.581\n",
      "Epoch 211 Training loss: 0.739 Validation loss : 0.581\n",
      "Epoch 221 Training loss: 0.740 Validation loss : 0.580\n",
      "Epoch 231 Training loss: 0.737 Validation loss : 0.580\n",
      "Epoch 241 Training loss: 0.758 Validation loss : 0.580\n",
      "Epoch 251 Training loss: 0.736 Validation loss : 0.580\n",
      "Epoch 261 Training loss: 0.735 Validation loss : 0.579\n",
      "Epoch 271 Training loss: 0.735 Validation loss : 0.579\n",
      "Epoch 281 Training loss: 0.749 Validation loss : 0.579\n",
      "Epoch 291 Training loss: 0.739 Validation loss : 0.579\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287, (16, 0.01): 34.13525451660156, (16, 0.001): 22.973655395507812, (16, 0.0001): 19.510242080688478, (32, 0.01): 24.570661087036132, (32, 0.001): 28.43379005432129, (32, 0.0001): 28.04708595275879}\n",
      "Epoch 1 Training loss: 1.069 Validation loss : 0.621\n",
      "Epoch 11 Training loss: 0.788 Validation loss : 0.613\n",
      "Epoch 21 Training loss: 0.771 Validation loss : 0.565\n",
      "Epoch 31 Training loss: 0.714 Validation loss : 0.581\n",
      "Epoch 41 Training loss: 0.640 Validation loss : 0.609\n",
      "Epoch 51 Training loss: 0.572 Validation loss : 0.810\n",
      "Epoch 61 Training loss: 0.531 Validation loss : 0.643\n",
      "Epoch 71 Training loss: 0.498 Validation loss : 0.663\n",
      "Epoch 81 Training loss: 0.485 Validation loss : 0.600\n",
      "Epoch 91 Training loss: 0.456 Validation loss : 0.639\n",
      "Epoch 101 Training loss: 0.467 Validation loss : 0.674\n",
      "Epoch 111 Training loss: 0.427 Validation loss : 0.675\n",
      "Epoch 121 Training loss: 0.420 Validation loss : 0.723\n",
      "Epoch 131 Training loss: 0.408 Validation loss : 0.683\n",
      "Epoch 141 Training loss: 0.405 Validation loss : 0.672\n",
      "Epoch 151 Training loss: 0.383 Validation loss : 0.708\n",
      "Epoch 161 Training loss: 0.387 Validation loss : 0.728\n",
      "Epoch 171 Training loss: 0.369 Validation loss : 0.724\n",
      "Epoch 181 Training loss: 0.379 Validation loss : 0.763\n",
      "Epoch 191 Training loss: 0.356 Validation loss : 0.803\n",
      "Epoch 201 Training loss: 0.347 Validation loss : 0.849\n",
      "Epoch 211 Training loss: 0.342 Validation loss : 0.839\n",
      "Epoch 221 Training loss: 0.334 Validation loss : 0.879\n",
      "Epoch 231 Training loss: 0.325 Validation loss : 0.860\n",
      "Epoch 241 Training loss: 0.318 Validation loss : 0.859\n",
      "Epoch 251 Training loss: 0.313 Validation loss : 0.859\n",
      "Epoch 261 Training loss: 0.310 Validation loss : 0.890\n",
      "Epoch 271 Training loss: 0.304 Validation loss : 0.894\n",
      "Epoch 281 Training loss: 0.298 Validation loss : 0.899\n",
      "Epoch 291 Training loss: 0.294 Validation loss : 0.939\n",
      "Epoch 1 Training loss: 1.155 Validation loss : 0.699\n",
      "Epoch 11 Training loss: 0.748 Validation loss : 0.590\n",
      "Epoch 21 Training loss: 0.718 Validation loss : 0.571\n",
      "Epoch 31 Training loss: 0.694 Validation loss : 0.559\n",
      "Epoch 41 Training loss: 0.671 Validation loss : 0.551\n",
      "Epoch 51 Training loss: 0.635 Validation loss : 0.529\n",
      "Epoch 61 Training loss: 0.624 Validation loss : 0.560\n",
      "Epoch 71 Training loss: 0.593 Validation loss : 0.563\n",
      "Epoch 81 Training loss: 0.576 Validation loss : 0.590\n",
      "Epoch 91 Training loss: 0.567 Validation loss : 0.576\n",
      "Epoch 101 Training loss: 0.546 Validation loss : 0.564\n",
      "Epoch 111 Training loss: 0.534 Validation loss : 0.585\n",
      "Epoch 121 Training loss: 0.524 Validation loss : 0.623\n",
      "Epoch 131 Training loss: 0.513 Validation loss : 0.598\n",
      "Epoch 141 Training loss: 0.500 Validation loss : 0.614\n",
      "Epoch 151 Training loss: 0.505 Validation loss : 0.622\n",
      "Epoch 161 Training loss: 0.488 Validation loss : 0.631\n",
      "Epoch 171 Training loss: 0.479 Validation loss : 0.624\n",
      "Epoch 181 Training loss: 0.476 Validation loss : 0.641\n",
      "Epoch 191 Training loss: 0.468 Validation loss : 0.632\n",
      "Epoch 201 Training loss: 0.463 Validation loss : 0.643\n",
      "Epoch 211 Training loss: 0.457 Validation loss : 0.645\n",
      "Epoch 221 Training loss: 0.453 Validation loss : 0.636\n",
      "Epoch 231 Training loss: 0.451 Validation loss : 0.654\n",
      "Epoch 241 Training loss: 0.448 Validation loss : 0.653\n",
      "Epoch 251 Training loss: 0.445 Validation loss : 0.651\n",
      "Epoch 261 Training loss: 0.441 Validation loss : 0.657\n",
      "Epoch 271 Training loss: 0.438 Validation loss : 0.659\n",
      "Epoch 281 Training loss: 0.436 Validation loss : 0.659\n",
      "Epoch 291 Training loss: 0.434 Validation loss : 0.661\n",
      "Epoch 1 Training loss: 1.647 Validation loss : 1.124\n",
      "Epoch 11 Training loss: 0.878 Validation loss : 0.662\n",
      "Epoch 21 Training loss: 0.809 Validation loss : 0.619\n",
      "Epoch 31 Training loss: 0.802 Validation loss : 0.606\n",
      "Epoch 41 Training loss: 0.759 Validation loss : 0.592\n",
      "Epoch 51 Training loss: 0.749 Validation loss : 0.587\n",
      "Epoch 61 Training loss: 0.738 Validation loss : 0.585\n",
      "Epoch 71 Training loss: 0.732 Validation loss : 0.580\n",
      "Epoch 81 Training loss: 0.733 Validation loss : 0.580\n",
      "Epoch 91 Training loss: 0.723 Validation loss : 0.582\n",
      "Epoch 101 Training loss: 0.719 Validation loss : 0.575\n",
      "Epoch 111 Training loss: 0.719 Validation loss : 0.576\n",
      "Epoch 121 Training loss: 0.715 Validation loss : 0.577\n",
      "Epoch 131 Training loss: 0.715 Validation loss : 0.580\n",
      "Epoch 141 Training loss: 0.714 Validation loss : 0.575\n",
      "Epoch 151 Training loss: 0.711 Validation loss : 0.574\n",
      "Epoch 161 Training loss: 0.709 Validation loss : 0.576\n",
      "Epoch 171 Training loss: 0.712 Validation loss : 0.575\n",
      "Epoch 181 Training loss: 0.708 Validation loss : 0.576\n",
      "Epoch 191 Training loss: 0.707 Validation loss : 0.576\n",
      "Epoch 201 Training loss: 0.707 Validation loss : 0.576\n",
      "Epoch 211 Training loss: 0.708 Validation loss : 0.575\n",
      "Epoch 221 Training loss: 0.706 Validation loss : 0.575\n",
      "Epoch 231 Training loss: 0.704 Validation loss : 0.575\n",
      "Epoch 241 Training loss: 0.704 Validation loss : 0.576\n",
      "Epoch 251 Training loss: 0.704 Validation loss : 0.574\n",
      "Epoch 261 Training loss: 0.703 Validation loss : 0.575\n",
      "Epoch 271 Training loss: 0.705 Validation loss : 0.575\n",
      "Epoch 281 Training loss: 0.702 Validation loss : 0.574\n",
      "Epoch 291 Training loss: 0.702 Validation loss : 0.575\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287, (16, 0.01): 34.13525451660156, (16, 0.001): 22.973655395507812, (16, 0.0001): 19.510242080688478, (32, 0.01): 24.570661087036132, (32, 0.001): 28.43379005432129, (32, 0.0001): 28.04708595275879, (64, 0.01): 46.53031196594238, (64, 0.001): 50.14472457885742, (64, 0.0001): 23.57990348815918}\n",
      "Epoch 1 Training loss: 1.203 Validation loss : 0.702\n",
      "Epoch 11 Training loss: 0.878 Validation loss : 0.597\n",
      "Epoch 21 Training loss: 0.772 Validation loss : 0.604\n",
      "Epoch 31 Training loss: 0.704 Validation loss : 0.598\n",
      "Epoch 41 Training loss: 0.653 Validation loss : 0.590\n",
      "Epoch 51 Training loss: 0.633 Validation loss : 0.724\n",
      "Epoch 61 Training loss: 0.571 Validation loss : 0.635\n",
      "Epoch 71 Training loss: 0.557 Validation loss : 0.729\n",
      "Epoch 81 Training loss: 0.474 Validation loss : 0.746\n",
      "Epoch 91 Training loss: 0.404 Validation loss : 0.894\n",
      "Epoch 101 Training loss: 0.376 Validation loss : 0.938\n",
      "Epoch 111 Training loss: 0.357 Validation loss : 0.893\n",
      "Epoch 121 Training loss: 0.318 Validation loss : 0.953\n",
      "Epoch 131 Training loss: 0.312 Validation loss : 0.981\n",
      "Epoch 141 Training loss: 0.293 Validation loss : 0.985\n",
      "Epoch 151 Training loss: 0.281 Validation loss : 0.990\n",
      "Epoch 161 Training loss: 0.271 Validation loss : 1.166\n",
      "Epoch 171 Training loss: 0.261 Validation loss : 1.089\n",
      "Epoch 181 Training loss: 0.255 Validation loss : 1.123\n",
      "Epoch 191 Training loss: 0.253 Validation loss : 1.101\n",
      "Epoch 201 Training loss: 0.245 Validation loss : 1.179\n",
      "Epoch 211 Training loss: 0.235 Validation loss : 1.136\n",
      "Epoch 221 Training loss: 0.227 Validation loss : 1.155\n",
      "Epoch 231 Training loss: 0.221 Validation loss : 1.158\n",
      "Epoch 241 Training loss: 0.219 Validation loss : 1.174\n",
      "Epoch 251 Training loss: 0.219 Validation loss : 1.216\n",
      "Epoch 261 Training loss: 0.212 Validation loss : 1.193\n",
      "Epoch 271 Training loss: 0.207 Validation loss : 1.182\n",
      "Epoch 281 Training loss: 0.205 Validation loss : 1.207\n",
      "Epoch 291 Training loss: 0.197 Validation loss : 1.196\n",
      "Epoch 1 Training loss: 1.068 Validation loss : 0.630\n",
      "Epoch 11 Training loss: 0.756 Validation loss : 0.576\n",
      "Epoch 21 Training loss: 0.700 Validation loss : 0.559\n",
      "Epoch 31 Training loss: 0.661 Validation loss : 0.569\n",
      "Epoch 41 Training loss: 0.636 Validation loss : 0.565\n",
      "Epoch 51 Training loss: 0.612 Validation loss : 0.550\n",
      "Epoch 61 Training loss: 0.577 Validation loss : 0.561\n",
      "Epoch 71 Training loss: 0.537 Validation loss : 0.596\n",
      "Epoch 81 Training loss: 0.512 Validation loss : 0.580\n",
      "Epoch 91 Training loss: 0.480 Validation loss : 0.614\n",
      "Epoch 101 Training loss: 0.449 Validation loss : 0.654\n",
      "Epoch 111 Training loss: 0.434 Validation loss : 0.647\n",
      "Epoch 121 Training loss: 0.416 Validation loss : 0.663\n",
      "Epoch 131 Training loss: 0.392 Validation loss : 0.671\n",
      "Epoch 141 Training loss: 0.367 Validation loss : 0.706\n",
      "Epoch 151 Training loss: 0.349 Validation loss : 0.715\n",
      "Epoch 161 Training loss: 0.331 Validation loss : 0.732\n",
      "Epoch 171 Training loss: 0.328 Validation loss : 0.745\n",
      "Epoch 181 Training loss: 0.314 Validation loss : 0.732\n",
      "Epoch 191 Training loss: 0.300 Validation loss : 0.732\n",
      "Epoch 201 Training loss: 0.295 Validation loss : 0.750\n",
      "Epoch 211 Training loss: 0.287 Validation loss : 0.748\n",
      "Epoch 221 Training loss: 0.281 Validation loss : 0.753\n",
      "Epoch 231 Training loss: 0.274 Validation loss : 0.737\n",
      "Epoch 241 Training loss: 0.269 Validation loss : 0.751\n",
      "Epoch 251 Training loss: 0.266 Validation loss : 0.757\n",
      "Epoch 261 Training loss: 0.261 Validation loss : 0.759\n",
      "Epoch 271 Training loss: 0.259 Validation loss : 0.757\n",
      "Epoch 281 Training loss: 0.255 Validation loss : 0.765\n",
      "Epoch 291 Training loss: 0.251 Validation loss : 0.764\n",
      "Epoch 1 Training loss: 1.353 Validation loss : 0.937\n",
      "Epoch 11 Training loss: 0.817 Validation loss : 0.605\n",
      "Epoch 21 Training loss: 0.770 Validation loss : 0.586\n",
      "Epoch 31 Training loss: 0.745 Validation loss : 0.579\n",
      "Epoch 41 Training loss: 0.735 Validation loss : 0.567\n",
      "Epoch 51 Training loss: 0.755 Validation loss : 0.568\n",
      "Epoch 61 Training loss: 0.717 Validation loss : 0.581\n",
      "Epoch 71 Training loss: 0.716 Validation loss : 0.562\n",
      "Epoch 81 Training loss: 0.707 Validation loss : 0.567\n",
      "Epoch 91 Training loss: 0.705 Validation loss : 0.561\n",
      "Epoch 101 Training loss: 0.700 Validation loss : 0.560\n",
      "Epoch 111 Training loss: 0.699 Validation loss : 0.560\n",
      "Epoch 121 Training loss: 0.697 Validation loss : 0.560\n",
      "Epoch 131 Training loss: 0.695 Validation loss : 0.564\n",
      "Epoch 141 Training loss: 0.691 Validation loss : 0.559\n",
      "Epoch 151 Training loss: 0.688 Validation loss : 0.559\n",
      "Epoch 161 Training loss: 0.688 Validation loss : 0.560\n",
      "Epoch 171 Training loss: 0.686 Validation loss : 0.559\n",
      "Epoch 181 Training loss: 0.685 Validation loss : 0.558\n",
      "Epoch 191 Training loss: 0.686 Validation loss : 0.557\n",
      "Epoch 201 Training loss: 0.683 Validation loss : 0.557\n",
      "Epoch 211 Training loss: 0.681 Validation loss : 0.558\n",
      "Epoch 221 Training loss: 0.701 Validation loss : 0.557\n",
      "Epoch 231 Training loss: 0.679 Validation loss : 0.556\n",
      "Epoch 241 Training loss: 0.679 Validation loss : 0.556\n",
      "Epoch 251 Training loss: 0.678 Validation loss : 0.556\n",
      "Epoch 261 Training loss: 0.679 Validation loss : 0.555\n",
      "Epoch 271 Training loss: 0.679 Validation loss : 0.556\n",
      "Epoch 281 Training loss: 0.678 Validation loss : 0.556\n",
      "Epoch 291 Training loss: 0.677 Validation loss : 0.556\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287, (16, 0.01): 34.13525451660156, (16, 0.001): 22.973655395507812, (16, 0.0001): 19.510242080688478, (32, 0.01): 24.570661087036132, (32, 0.001): 28.43379005432129, (32, 0.0001): 28.04708595275879, (64, 0.01): 46.53031196594238, (64, 0.001): 50.14472457885742, (64, 0.0001): 23.57990348815918, (128, 0.01): 32.10346221923828, (128, 0.001): 26.858419799804686, (128, 0.0001): 26.437442779541016}\n",
      "Epoch 1 Training loss: 1.460 Validation loss : 0.657\n",
      "Epoch 11 Training loss: 0.907 Validation loss : 0.718\n",
      "Epoch 21 Training loss: 0.832 Validation loss : 0.588\n",
      "Epoch 31 Training loss: 0.723 Validation loss : 0.591\n",
      "Epoch 41 Training loss: 0.696 Validation loss : 0.750\n",
      "Epoch 51 Training loss: 0.622 Validation loss : 0.566\n",
      "Epoch 61 Training loss: 0.537 Validation loss : 0.738\n",
      "Epoch 71 Training loss: 0.500 Validation loss : 0.800\n",
      "Epoch 81 Training loss: 0.513 Validation loss : 0.767\n",
      "Epoch 91 Training loss: 0.439 Validation loss : 0.777\n",
      "Epoch 101 Training loss: 0.417 Validation loss : 0.701\n",
      "Epoch 111 Training loss: 0.408 Validation loss : 0.690\n",
      "Epoch 121 Training loss: 0.373 Validation loss : 0.700\n",
      "Epoch 131 Training loss: 0.348 Validation loss : 0.755\n",
      "Epoch 141 Training loss: 0.310 Validation loss : 0.766\n",
      "Epoch 151 Training loss: 0.324 Validation loss : 0.778\n",
      "Epoch 161 Training loss: 0.281 Validation loss : 0.866\n",
      "Epoch 171 Training loss: 0.270 Validation loss : 0.892\n",
      "Epoch 181 Training loss: 0.261 Validation loss : 0.867\n",
      "Epoch 191 Training loss: 0.243 Validation loss : 0.889\n",
      "Epoch 201 Training loss: 0.240 Validation loss : 0.877\n",
      "Epoch 211 Training loss: 0.227 Validation loss : 0.925\n",
      "Epoch 221 Training loss: 0.222 Validation loss : 0.922\n",
      "Epoch 231 Training loss: 0.222 Validation loss : 0.909\n",
      "Epoch 241 Training loss: 0.228 Validation loss : 0.981\n",
      "Epoch 251 Training loss: 0.217 Validation loss : 1.005\n",
      "Epoch 261 Training loss: 0.211 Validation loss : 1.016\n",
      "Epoch 271 Training loss: 0.208 Validation loss : 1.001\n",
      "Epoch 281 Training loss: 0.206 Validation loss : 1.003\n",
      "Epoch 291 Training loss: 0.205 Validation loss : 1.000\n",
      "Epoch 1 Training loss: 0.996 Validation loss : 0.678\n",
      "Epoch 11 Training loss: 0.745 Validation loss : 0.579\n",
      "Epoch 21 Training loss: 0.677 Validation loss : 0.584\n",
      "Epoch 31 Training loss: 0.634 Validation loss : 0.609\n",
      "Epoch 41 Training loss: 0.549 Validation loss : 0.637\n",
      "Epoch 51 Training loss: 0.488 Validation loss : 0.632\n",
      "Epoch 61 Training loss: 0.441 Validation loss : 0.615\n",
      "Epoch 71 Training loss: 0.388 Validation loss : 0.605\n",
      "Epoch 81 Training loss: 0.355 Validation loss : 0.664\n",
      "Epoch 91 Training loss: 0.318 Validation loss : 0.712\n",
      "Epoch 101 Training loss: 0.299 Validation loss : 0.742\n",
      "Epoch 111 Training loss: 0.278 Validation loss : 0.767\n",
      "Epoch 121 Training loss: 0.268 Validation loss : 0.777\n",
      "Epoch 131 Training loss: 0.266 Validation loss : 0.792\n",
      "Epoch 141 Training loss: 0.242 Validation loss : 0.823\n",
      "Epoch 151 Training loss: 0.234 Validation loss : 0.789\n",
      "Epoch 161 Training loss: 0.228 Validation loss : 0.777\n",
      "Epoch 171 Training loss: 0.221 Validation loss : 0.825\n",
      "Epoch 181 Training loss: 0.214 Validation loss : 0.809\n",
      "Epoch 191 Training loss: 0.206 Validation loss : 0.828\n",
      "Epoch 201 Training loss: 0.201 Validation loss : 0.854\n",
      "Epoch 211 Training loss: 0.198 Validation loss : 0.811\n",
      "Epoch 221 Training loss: 0.193 Validation loss : 0.838\n",
      "Epoch 231 Training loss: 0.189 Validation loss : 0.830\n",
      "Epoch 241 Training loss: 0.185 Validation loss : 0.844\n",
      "Epoch 251 Training loss: 0.183 Validation loss : 0.859\n",
      "Epoch 261 Training loss: 0.182 Validation loss : 0.852\n",
      "Epoch 271 Training loss: 0.178 Validation loss : 0.861\n",
      "Epoch 281 Training loss: 0.178 Validation loss : 0.864\n",
      "Epoch 291 Training loss: 0.175 Validation loss : 0.867\n",
      "Epoch 1 Training loss: 1.159 Validation loss : 0.706\n",
      "Epoch 11 Training loss: 0.772 Validation loss : 0.578\n",
      "Epoch 21 Training loss: 0.738 Validation loss : 0.570\n",
      "Epoch 31 Training loss: 0.718 Validation loss : 0.553\n",
      "Epoch 41 Training loss: 0.709 Validation loss : 0.545\n",
      "Epoch 51 Training loss: 0.698 Validation loss : 0.544\n",
      "Epoch 61 Training loss: 0.696 Validation loss : 0.575\n",
      "Epoch 71 Training loss: 0.682 Validation loss : 0.555\n",
      "Epoch 81 Training loss: 0.675 Validation loss : 0.535\n",
      "Epoch 91 Training loss: 0.669 Validation loss : 0.534\n",
      "Epoch 101 Training loss: 0.662 Validation loss : 0.537\n",
      "Epoch 111 Training loss: 0.656 Validation loss : 0.535\n",
      "Epoch 121 Training loss: 0.656 Validation loss : 0.540\n",
      "Epoch 131 Training loss: 0.646 Validation loss : 0.534\n",
      "Epoch 141 Training loss: 0.645 Validation loss : 0.532\n",
      "Epoch 151 Training loss: 0.640 Validation loss : 0.532\n",
      "Epoch 161 Training loss: 0.638 Validation loss : 0.530\n",
      "Epoch 171 Training loss: 0.634 Validation loss : 0.531\n",
      "Epoch 181 Training loss: 0.631 Validation loss : 0.528\n",
      "Epoch 191 Training loss: 0.627 Validation loss : 0.528\n",
      "Epoch 201 Training loss: 0.627 Validation loss : 0.528\n",
      "Epoch 211 Training loss: 0.624 Validation loss : 0.531\n",
      "Epoch 221 Training loss: 0.621 Validation loss : 0.527\n",
      "Epoch 231 Training loss: 0.620 Validation loss : 0.531\n",
      "Epoch 241 Training loss: 0.618 Validation loss : 0.529\n",
      "Epoch 251 Training loss: 0.620 Validation loss : 0.531\n",
      "Epoch 261 Training loss: 0.615 Validation loss : 0.528\n",
      "Epoch 271 Training loss: 0.614 Validation loss : 0.530\n",
      "Epoch 281 Training loss: 0.613 Validation loss : 0.527\n",
      "Epoch 291 Training loss: 0.613 Validation loss : 0.527\n",
      "{(8, 0.01): 42.379359588623046, (8, 0.001): 30.67802909851074, (8, 0.0001): 17.609180679321287, (16, 0.01): 34.13525451660156, (16, 0.001): 22.973655395507812, (16, 0.0001): 19.510242080688478, (32, 0.01): 24.570661087036132, (32, 0.001): 28.43379005432129, (32, 0.0001): 28.04708595275879, (64, 0.01): 46.53031196594238, (64, 0.001): 50.14472457885742, (64, 0.0001): 23.57990348815918, (128, 0.01): 32.10346221923828, (128, 0.001): 26.858419799804686, (128, 0.0001): 26.437442779541016, (256, 0.01): 25.904155349731447, (256, 0.001): 28.14394058227539, (256, 0.0001): 15.227695083618164}\n"
     ]
    }
   ],
   "source": [
    "week4_pred_2_26 = week4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "week4_mae = week4_pred_2_26[0][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "week4_mlp_pred = week4_pred_2_26[1][(256, 0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.902928085327149 17.37345802307129 15.713558578491211 15.227695083618164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.804409942626954"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(week1_mae, week2_mae, week3_mae, week4_mae)\n",
    "mae_all_four_weeks = np.mean([week1_mae, week2_mae, week3_mae, week4_mae])\n",
    "mae_all_four_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_all_four_weeks = np.stack([week1_mlp_pred, week2_mlp_pred, week3_mlp_pred, week4_mlp_pred], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.28645134e+01, 2.09407043e+00, 5.44606361e+01, 2.75767860e+01,\n",
       "        9.86205063e+01, 1.86086960e+01, 1.44608574e+01, 2.04736328e+00,\n",
       "        3.32840454e+02, 6.15409851e+01, 9.87140656e+00, 1.33667717e+01,\n",
       "        5.42458992e+01, 6.34236488e+01, 2.81825638e+01, 1.74767723e+01,\n",
       "        1.47852211e+01, 4.44486694e+01, 1.05986633e+01, 3.14857330e+01,\n",
       "        6.62478104e+01, 8.57927399e+01, 1.51034775e+01, 2.97746010e+01,\n",
       "        6.15858116e+01, 5.77153778e+00, 2.63474922e+01, 6.48354340e+00,\n",
       "        1.67643623e+01, 6.76163559e+01, 1.01438179e+01, 1.27382507e+02,\n",
       "        2.30765533e+01, 1.87202301e+01, 6.66986847e+01, 9.00839462e+01,\n",
       "        1.09734840e+01, 1.83136826e+02, 4.34403992e+00, 3.80143852e+01,\n",
       "        8.54008484e+00, 3.52451782e+01, 2.70767609e+02, 6.35208130e-01,\n",
       "        2.45851898e+00, 4.12973480e+01, 4.47575302e+01, 1.08207664e+01,\n",
       "        5.56143494e+01, 3.27552795e-01],\n",
       "       [3.13779411e+01, 8.66275787e+00, 1.60542908e+01, 2.42709770e+01,\n",
       "        1.22551750e+02, 1.29770203e+01, 2.64638519e+00, 8.87510681e+00,\n",
       "        3.96053101e+02, 5.64583855e+01, 8.66891479e+00, 1.05708313e+01,\n",
       "        2.58663177e+00, 1.42253418e+01, 5.93666077e-01, 9.93259430e+00,\n",
       "        3.37890625e-01, 4.03391991e+01, 4.04067230e+00, 2.10583382e+01,\n",
       "        2.11607361e+01, 6.73262863e+01, 1.21263390e+01, 3.86883812e+01,\n",
       "        9.39779663e+00, 1.15270386e+01, 2.77570076e+01, 7.69813538e+00,\n",
       "        1.56786995e+01, 7.32949829e+00, 2.28451614e+01, 5.35735245e+01,\n",
       "        4.22406006e+00, 2.03470993e+01, 4.57169724e+01, 3.62554588e+01,\n",
       "        3.74986343e+01, 1.40610046e+02, 1.19529610e+01, 3.40933571e+01,\n",
       "        1.22425117e+01, 2.60146370e+01, 2.96950867e+02, 1.21767349e+01,\n",
       "        1.14126091e+01, 2.94275703e+01, 1.00258255e+01, 9.83860016e+00,\n",
       "        5.91849518e+00, 8.50907898e+00],\n",
       "       [1.78594246e+01, 5.81211853e+00, 4.61575356e+01, 9.90948486e+00,\n",
       "        1.17421509e+02, 2.34754982e+01, 2.14296875e+01, 2.93006134e+00,\n",
       "        2.08249542e+02, 3.01407242e+01, 1.44049721e+01, 2.56155968e+01,\n",
       "        4.79957809e+01, 5.36015778e+01, 2.75400963e+01, 2.07354469e+01,\n",
       "        2.67143326e+01, 2.22180405e+01, 1.48541603e+01, 1.54240456e+01,\n",
       "        7.08461151e+01, 7.34098129e+01, 1.66063957e+01, 2.21691284e+01,\n",
       "        4.00517349e+01, 7.12140656e+00, 1.05020103e+01, 1.88500099e+01,\n",
       "        7.82787323e+00, 4.91875763e+01, 2.23387375e+01, 1.28997345e+02,\n",
       "        1.48436852e+01, 7.48344421e+00, 7.12655106e+01, 6.68912277e+01,\n",
       "        1.64256554e+01, 1.27528587e+02, 2.99714661e+00, 1.90311737e+01,\n",
       "        8.85330200e+00, 2.87383614e+01, 1.60471954e+02, 1.14258194e+01,\n",
       "        5.88891602e+00, 2.43062248e+01, 3.17996483e+01, 1.91472168e+01,\n",
       "        4.70687332e+01, 2.34572601e+00],\n",
       "       [8.27437592e+00, 1.09418526e+01, 4.54709015e+01, 3.27686310e+01,\n",
       "        4.63479576e+01, 5.33663940e+00, 1.55339127e+01, 6.01579285e+00,\n",
       "        1.34398438e+02, 2.54855919e+01, 1.63467331e+01, 4.06003990e+01,\n",
       "        3.62500458e+01, 3.00483131e+01, 1.95006447e+01, 2.42203979e+01,\n",
       "        1.19881401e+01, 2.54267998e+01, 1.22088966e+01, 1.96909981e+01,\n",
       "        2.93595085e+01, 2.78788986e+01, 2.66243286e+01, 2.35847473e+01,\n",
       "        5.15517426e+01, 2.62486267e+00, 1.97118454e+01, 1.55142021e+01,\n",
       "        1.76138535e+01, 3.66908913e+01, 1.09212189e+01, 8.42331848e+01,\n",
       "        2.07624130e+01, 8.40367126e+00, 2.68850937e+01, 5.18583870e+01,\n",
       "        9.40924072e+00, 1.06134399e+02, 1.82486229e+01, 1.91522293e+01,\n",
       "        2.23290749e+01, 3.71458588e+01, 1.38663788e+02, 3.26115417e+00,\n",
       "        1.56756592e+01, 3.15874405e+01, 2.77805252e+01, 2.78112640e+01,\n",
       "        4.38284416e+01, 6.82585144e+00]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_all_four_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0662545c6fd30eceb00c46289e22b5a22aef9c4ebb29470f344626a3bc8eec96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
