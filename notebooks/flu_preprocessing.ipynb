{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import us as usStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def days(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%m/%d/%Y\")\n",
    "    d2 = datetime.strptime(d2, \"%m/%d/%Y\")\n",
    "    return abs((d2 - d1).days)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "def weeks(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%m/%d/%Y\")\n",
    "    d2 = datetime.strptime(d2, \"%m/%d/%Y\")\n",
    "    return (abs((d2 - d1).days)+1)//7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "# x,y = shuffle_in_unison(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = '../data/GLEAM'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "files_name = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        files_name.append(f)\n",
    "        # print(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks_1_df = pd.DataFrame()\n",
    "# weeks_2_df = pd.DataFrame()\n",
    "# weeks_3_df = pd.DataFrame()\n",
    "# weeks_4_df = pd.DataFrame()\n",
    "# for i in files_name:\n",
    "#     if i.endswith(\".csv\"):\n",
    "#         df = pd.read_csv(i)\n",
    "#         # if df['target'] == '1 wk ahead inc flu hosp':\n",
    "#         weeks_1_df = weeks_1_df.append(df[(df['target'] == '1 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "#         weeks_2_df = weeks_2_df.append(df[(df['target'] == '2 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "#         weeks_3_df = weeks_3_df.append(df[(df['target'] == '3 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "#         weeks_4_df = weeks_4_df.append(df[(df['target'] == '4 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "\n",
    "# weeks_1_df = weeks_1_df.drop(['target','type','quantile'],axis = 1)\n",
    "# weeks_2_df = weeks_2_df.drop(['target','type','quantile'],axis = 1)\n",
    "# weeks_3_df = weeks_3_df.drop(['target','type','quantile'],axis = 1)\n",
    "# weeks_4_df = weeks_4_df.drop(['target','type','quantile'],axis = 1)\n",
    "\n",
    "\n",
    "# weeks_1_df = weeks_1_df.sort_values(by=['target_end_date', 'location'])\n",
    "# weeks_2_df = weeks_2_df.sort_values(by=['target_end_date', 'location'])\n",
    "# weeks_3_df = weeks_3_df.sort_values(by=['target_end_date', 'location'])\n",
    "# weeks_4_df = weeks_4_df.sort_values(by=['target_end_date', 'location'])\n",
    "\n",
    "\n",
    "# # # weeks to drop\n",
    "# # lst_weeks_drop = ['2022-06-11','2022-06-18', '2022-06-25','2022-07-02','2022-07-09','2022-07-16', '2022-10-22', '2022-10-29', '2022-11-05', '2022-11-12','2022-11-19','2022-11-26', '2022-12-03','2022-12-10']\n",
    "# # # states to drop \n",
    "# states_to_drop = ['11','US']\n",
    "\n",
    "# # weeks_1_df = weeks_1_df[~weeks_1_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# # weeks_2_df = weeks_2_df[~weeks_2_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# # weeks_3_df = weeks_3_df[~weeks_3_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# # weeks_4_df = weeks_4_df[~weeks_4_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "\n",
    "# weeks_1_df = weeks_1_df[~weeks_1_df['location'].isin(states_to_drop)]\n",
    "# weeks_2_df = weeks_2_df[~weeks_2_df['location'].isin(states_to_drop)]\n",
    "# weeks_3_df = weeks_3_df[~weeks_3_df['location'].isin(states_to_drop)]\n",
    "# weeks_4_df = weeks_4_df[~weeks_4_df['location'].isin(states_to_drop)]\n",
    "\n",
    "\n",
    "# # 1 week ahead\n",
    "# week_1_df_2 = weeks_1_df.drop_duplicates(subset=['target_end_date'])\n",
    "# week_1_df_3 = weeks_1_df.drop_duplicates(subset=['location'])\n",
    "# list1_week1 = list(week_1_df_2['target_end_date'][3:])\n",
    "# list2_week1 = list(week_1_df_3['location'])\n",
    "\n",
    "# # 2 week ahead\n",
    "# week_2_df_2 = weeks_2_df.drop_duplicates(subset=['target_end_date'])\n",
    "# week_2_df_3 = weeks_2_df.drop_duplicates(subset=['location'])\n",
    "# list1_week2 = list(week_2_df_2['target_end_date'][2:-1])\n",
    "# list2_week2 = list(week_2_df_3['location'])\n",
    "\n",
    "# # 3 week ahead\n",
    "# week_3_df_2 = weeks_3_df.drop_duplicates(subset=['target_end_date'])\n",
    "# week_3_df_3 = weeks_3_df.drop_duplicates(subset=['location'])\n",
    "# list1_week3 = list(week_3_df_2['target_end_date'][1:-2])\n",
    "# list2_week3 = list(week_3_df_3['location'])\n",
    "\n",
    "\n",
    "# # 4 week ahead\n",
    "# week_4_df_2 = weeks_4_df.drop_duplicates(subset=['target_end_date'])\n",
    "# week_4_df_3 = weeks_4_df.drop_duplicates(subset=['location'])\n",
    "# list1_week4 = list(week_4_df_2['target_end_date'][:-3])\n",
    "# list2_week4 = list(week_4_df_3['location'])\n",
    "\n",
    "# # Find comman weeks in GLEAM pred\n",
    "# comman_weeks = sorted(list(set.intersection(*map(set, [list1_week1, list1_week2, list1_week3,list1_week4]))))\n",
    "\n",
    "# time_length = len(comman_weeks)\n",
    "# state_length = len(list2_week4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include Interpolation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_1_df = pd.DataFrame()\n",
    "weeks_2_df = pd.DataFrame()\n",
    "weeks_3_df = pd.DataFrame()\n",
    "weeks_4_df = pd.DataFrame()\n",
    "for i in files_name:\n",
    "    if i.endswith(\".csv\"):\n",
    "        df = pd.read_csv(i)\n",
    "        # if df['target'] == '1 wk ahead inc flu hosp':\n",
    "        weeks_1_df = weeks_1_df.append(df[(df['target'] == '1 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "        weeks_2_df = weeks_2_df.append(df[(df['target'] == '2 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "        weeks_3_df = weeks_3_df.append(df[(df['target'] == '3 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "        weeks_4_df = weeks_4_df.append(df[(df['target'] == '4 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "\n",
    "weeks_1_df = weeks_1_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_2_df = weeks_2_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_3_df = weeks_3_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_4_df = weeks_4_df.drop(['target','type','quantile'],axis = 1)\n",
    "\n",
    "\n",
    "weeks_1_df = weeks_1_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_2_df = weeks_2_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_3_df = weeks_3_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_4_df = weeks_4_df.sort_values(by=['target_end_date', 'location'])\n",
    "\n",
    "week1_after_interpolation = pd.read_csv('../data/Flu_Interpolation/week1_after_interpolation.csv')\n",
    "week2_after_interpolation = pd.read_csv('../data/Flu_Interpolation/week2_after_interpolation.csv')\n",
    "week3_after_interpolation = pd.read_csv('../data/Flu_Interpolation/week3_after_interpolation.csv')\n",
    "week4_after_interpolation = pd.read_csv('../data/Flu_Interpolation/week4_after_interpolation.csv')\n",
    "\n",
    "week1_after_interpolation.rename(columns = {\"value_a\": \"value\"}, inplace = True)\n",
    "week2_after_interpolation.rename(columns = {\"value_a\": \"value\"}, inplace = True)\n",
    "week3_after_interpolation.rename(columns = {\"value_a\": \"value\"}, inplace = True)\n",
    "week4_after_interpolation.rename(columns = {\"value_a\": \"value\"}, inplace = True)\n",
    "\n",
    "temp_merged_week1_df = pd.merge(weeks_1_df, week1_after_interpolation, on=['target_end_date', 'location'], how=\"outer\", validate=\"one_to_many\")[['target_end_date', 'location', 'value_x','value_y']]\n",
    "temp_merged_week1_df['value_x'] = temp_merged_week1_df['value_x'].fillna(temp_merged_week1_df['value_y'])\n",
    "weeks_1_df = temp_merged_week1_df.rename(columns = {\"value_x\": \"value\"}).sort_values(by=['target_end_date', 'location'])[['target_end_date', 'location', 'value']]\n",
    "\n",
    "temp_merged_week2_df = pd.merge(weeks_2_df, week2_after_interpolation, on=['target_end_date', 'location'], how=\"outer\", validate=\"one_to_many\")[['target_end_date', 'location', 'value_x','value_y']]\n",
    "temp_merged_week2_df['value_x'] = temp_merged_week2_df['value_x'].fillna(temp_merged_week2_df['value_y'])\n",
    "weeks_2_df = temp_merged_week2_df.rename(columns = {\"value_x\": \"value\"}).sort_values(by=['target_end_date', 'location'])[['target_end_date', 'location', 'value']]\n",
    "\n",
    "temp_merged_week3_df = pd.merge(weeks_3_df, week3_after_interpolation, on=['target_end_date', 'location'], how=\"outer\", validate=\"one_to_many\")[['target_end_date', 'location', 'value_x','value_y']]\n",
    "temp_merged_week3_df['value_x'] = temp_merged_week3_df['value_x'].fillna(temp_merged_week3_df['value_y'])\n",
    "weeks_3_df = temp_merged_week3_df.rename(columns = {\"value_x\": \"value\"}).sort_values(by=['target_end_date', 'location'])[['target_end_date', 'location', 'value']]\n",
    "\n",
    "temp_merged_week4_df = pd.merge(weeks_4_df, week4_after_interpolation, on=['target_end_date', 'location'], how=\"outer\", validate=\"one_to_many\")[['target_end_date', 'location', 'value_x','value_y']]\n",
    "temp_merged_week4_df['value_x'] = temp_merged_week4_df['value_x'].fillna(temp_merged_week4_df['value_y'])\n",
    "weeks_4_df = temp_merged_week4_df.rename(columns = {\"value_x\": \"value\"}).sort_values(by=['target_end_date', 'location'])[['target_end_date', 'location', 'value']]\n",
    "\n",
    "\n",
    "# # weeks to drop\n",
    "# lst_weeks_drop = ['2022-06-11','2022-06-18', '2022-06-25','2022-07-02','2022-07-09','2022-07-16', '2022-10-22', '2022-10-29', '2022-11-05', '2022-11-12','2022-11-19','2022-11-26', '2022-12-03','2022-12-10']\n",
    "# # states to drop \n",
    "states_to_drop = ['11','US']\n",
    "\n",
    "# weeks_1_df = weeks_1_df[~weeks_1_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# weeks_2_df = weeks_2_df[~weeks_2_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# weeks_3_df = weeks_3_df[~weeks_3_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "# weeks_4_df = weeks_4_df[~weeks_4_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "\n",
    "weeks_1_df = weeks_1_df[~weeks_1_df['location'].isin(states_to_drop)]\n",
    "weeks_2_df = weeks_2_df[~weeks_2_df['location'].isin(states_to_drop)]\n",
    "weeks_3_df = weeks_3_df[~weeks_3_df['location'].isin(states_to_drop)]\n",
    "weeks_4_df = weeks_4_df[~weeks_4_df['location'].isin(states_to_drop)]\n",
    "\n",
    "\n",
    "# 1 week ahead\n",
    "week_1_df_2 = weeks_1_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_1_df_3 = weeks_1_df.drop_duplicates(subset=['location'])\n",
    "list1_week1 = list(week_1_df_2['target_end_date'][3:])\n",
    "list2_week1 = list(week_1_df_3['location'])\n",
    "\n",
    "# 2 week ahead\n",
    "week_2_df_2 = weeks_2_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_2_df_3 = weeks_2_df.drop_duplicates(subset=['location'])\n",
    "list1_week2 = list(week_2_df_2['target_end_date'][2:-1])\n",
    "list2_week2 = list(week_2_df_3['location'])\n",
    "\n",
    "# 3 week ahead\n",
    "week_3_df_2 = weeks_3_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_3_df_3 = weeks_3_df.drop_duplicates(subset=['location'])\n",
    "list1_week3 = list(week_3_df_2['target_end_date'][1:-2])\n",
    "list2_week3 = list(week_3_df_3['location'])\n",
    "\n",
    "\n",
    "# 4 week ahead\n",
    "week_4_df_2 = weeks_4_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_4_df_3 = weeks_4_df.drop_duplicates(subset=['location'])\n",
    "list1_week4 = list(week_4_df_2['target_end_date'][:-3])\n",
    "list2_week4 = list(week_4_df_3['location'])\n",
    "\n",
    "# Find comman weeks in GLEAM pred\n",
    "comman_weeks = sorted(list(set.intersection(*map(set, [list1_week1, list1_week2, list1_week3,list1_week4]))))\n",
    "\n",
    "time_length = len(comman_weeks)\n",
    "state_length = len(list2_week4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth for flu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks to drop\n",
    "# lst_weeks_drop = lst_weeks_drop = ['2022-06-11','2022-06-18', '2022-06-25','2022-07-02','2022-07-09',\\\n",
    "#     '2022-07-16', '2022-10-22', '2022-10-29', '2022-11-05', '2022-11-12','2022-11-19','2022-11-26', '2022-12-03','2022-12-10']\n",
    "# states to drop \n",
    "states_to_drop = ['11','US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.read_csv('../data/CDC/truth-Incident Hospitalizations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg['date'] = pd.to_datetime(dfg['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfg_date_filtered = dfg[dfg['date'] >= '2022-02-26']\n",
    "dfg_date_filtered = dfg[(dfg['date'] >= '2022-02-26') & (dfg['date'] <= '2023-01-28')]\n",
    "# dfg_date_filtered = dfg_date_filtered[dfg_date_filtered['date'].isin(comman_weeks)]\n",
    "# dfg_date_filtered = dfg_date_filtered[~dfg_date_filtered['date'].isin(lst_weeks_drop)] # drop the weeks \n",
    "dfg_date_filtered = dfg_date_filtered[~dfg_date_filtered['location'].isin(states_to_drop)]\n",
    "# only get the states that are in the prediction\n",
    "dfg_date_filtered_temp = dfg_date_filtered[dfg_date_filtered['location'].isin(list2_week1)]\n",
    "# # only get the dates that are in the prediction\n",
    "# dfg_date_filtered_temp = dfg_date_filtered_temp[dfg_date_filtered_temp['date'].isin(list1_week4)]\n",
    "# dfg_date_filtered_temp = dfg_date_filtered_temp.sort_values(by=['date', 'location'])\n",
    "dfg_date_filtered_temp = dfg_date_filtered_temp.sort_values(by=['date', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_groundtruth = np.resize(dfg_date_filtered_temp.groupby(['date','location']).sum().to_numpy().T, (time_length,state_length)) # 32,50\n",
    "flu_groundtruth = flu_groundtruth.reshape(1,time_length,state_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flu weekly prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flu Prediction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "pred_week1 = np.zeros([time_length,state_length])\n",
    "no_value_1_week = {}\n",
    "# for id1,i in enumerate(list1_week1):\n",
    "for id1,i in enumerate(comman_weeks):\n",
    "    df4 = weeks_1_df.loc[weeks_1_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week1):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week1[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_1_week.keys():\n",
    "                no_value_1_week[i] = [j]\n",
    "            else:\n",
    "                no_value_1_week[i].append(j)\n",
    "print(pred_week1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "pred_week2 = np.zeros([time_length,state_length])\n",
    "no_value_2_week = {}\n",
    "# for id1,i in enumerate(list1_week2):\n",
    "for id1,i in enumerate(comman_weeks):\n",
    "    df4 = weeks_2_df.loc[weeks_2_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week2):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week2[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_2_week.keys():\n",
    "                no_value_2_week[i] = [j]\n",
    "            else:\n",
    "                no_value_2_week[i].append(j)\n",
    "print(pred_week2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "pred_week3 = np.zeros([time_length,state_length])\n",
    "no_value_3_week = {}\n",
    "# for id1,i in enumerate(list1_week3):\n",
    "for id1,i in enumerate(comman_weeks):\n",
    "    df4 = weeks_3_df.loc[weeks_3_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week3):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week3[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_3_week.keys():\n",
    "                no_value_3_week[i] = [j]\n",
    "            else:\n",
    "                no_value_3_week[i].append(j)\n",
    "print(pred_week3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "pred_week4 = np.zeros([time_length,state_length])\n",
    "no_value_4_week = {}\n",
    "# for id1,i in enumerate(list1_week4):\n",
    "for id1,i in enumerate(comman_weeks):\n",
    "    df4 = weeks_4_df.loc[weeks_4_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week4):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week4[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_4_week.keys():\n",
    "                no_value_4_week[i] = [j]\n",
    "            else:\n",
    "                no_value_4_week[i].append(j)\n",
    "print(pred_week4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flu_pred = np.stack([pred_week1,pred_week2,pred_week3,pred_week4],axis=0)\n",
    "flu_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "flu_residual = flu_pred - flu_groundtruth\n",
    "residual_trans = flu_residual.transpose(1,2,0)\n",
    "print(residual_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_offsets = np.sort(np.concatenate((np.arange(-6, 1, 1),)))\n",
    "# print(x_offsets)\n",
    "# y_offsets = np.array([7])\n",
    "# print(y_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 1.  1.  1. ...  1.  1.  1.]\n",
      " [ 2.  2.  2. ...  2.  2.  2.]\n",
      " ...\n",
      " [47. 47. 47. ... 47. 47. 47.]\n",
      " [48. 48. 48. ... 48. 48. 48.]\n",
      " [49. 49. 49. ... 49. 49. 49.]]\n"
     ]
    }
   ],
   "source": [
    "#x data\n",
    "\n",
    "x_num_samples, x_num_nodes = time_length, state_length\n",
    "x_data = residual_trans\n",
    "# x_data = residual.transpose(1,2,0)\n",
    "\n",
    "x_day = list(range(0,x_num_samples))\n",
    "\n",
    "x_day = np.tile(x_day, [1, x_num_nodes, 1]).transpose((2, 1, 0))\n",
    "x_data = np.concatenate([x_data,x_day], axis=-1)\n",
    "print(x_data[:,:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(x_data, axis=0)[:time_length-4]\n",
    "x1 = x1.reshape(time_length-4,1,state_length,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "list5 = []\n",
    "for i in range(time_length):\n",
    "    residual_sum4 = np.sum((flu_residual[:,i:i+1]),axis=1)\n",
    "    list5.append(residual_sum4)\n",
    "residual_sum = np.stack(list5,axis =1)\n",
    "residual_sum_trans = residual_sum.transpose(1,2,0)\n",
    "print(residual_sum_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 4, 50, 1)\n",
      "(1, 4, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# y_data = np.stack([residual_trans[1:20,:,0],residual_trans[2:21,:,1],\n",
    "#                         residual_trans[3:22,:,2],residual_trans[4:23,:,3]],axis=-1)\n",
    "y_data = np.stack([residual_sum_trans[1:time_length-3,:,0],residual_sum_trans[2:time_length-2,:,1],\n",
    "                        residual_sum_trans[3:time_length-1,:,2],residual_sum_trans[4:time_length,:,3]],axis=-1)\n",
    "y1 = np.expand_dims(y_data,axis=1)\n",
    "y1 = y1.transpose(0,3,2,1)\n",
    "y_test = np.zeros([1,4,state_length,1])\n",
    "\n",
    "print(y1.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(x_data, axis=0)[-1].reshape(1,1,state_length,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = np.stack(x_data, axis=0)[:12]\n",
    "# x1 = x1.reshape(12,1,50,5)\n",
    "\n",
    "# y_data = np.stack([residual_trans[0:12,:,0],residual_trans[1:13,:,1],\n",
    "#                         residual_trans[2:14,:,2],residual_trans[3:15,:,3]],axis=-1)\n",
    "# y1 = np.expand_dims(y_data,axis=1)\n",
    "# y1 = y1.transpose(0,3,2,1)\n",
    "\n",
    "# without shuffling \n",
    "num_val = 12\n",
    "num_train = 34\n",
    "# num_test = 11\n",
    "\n",
    "x_train, y_train = x1[:num_train], y1[:num_train]\n",
    "x_val, y_val = x1[num_train:num_train+num_val], y1[num_train:num_train+num_val]\n",
    "# x_test, y_test = x1[num_train+num_val:], y1[num_train+num_val:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_shuffle, y_shuffle = shuffle_in_unison(x1, y1)\n",
    "\n",
    "# # num_samples = x1.shape[0]\n",
    "# # num_val = int(num_samples*0.5)\n",
    "# # num_train = num_samples-num_val\n",
    "\n",
    "# num_val = 10\n",
    "# num_train = 11\n",
    "# num_test = 1\n",
    "# # train\n",
    "# x_train, y_train = x_shuffle[:num_train], y_shuffle[:num_train]\n",
    "\n",
    "# # val\n",
    "# x_val, y_val = (\n",
    "#     x_shuffle[num_train: num_train + num_val],\n",
    "#     y_shuffle[num_train: num_train + num_val],\n",
    "# )\n",
    "# # x_test = x_shuffle[num_train+num_val:]\n",
    "# # x_test, y_test = (x_shuffle[num_train+num_val:], y_shuffle[num_train+num_val:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x:  (34, 1, 50, 5) y: (34, 4, 50, 1)\n",
      "val x:  (12, 1, 50, 5) y: (12, 4, 50, 1)\n",
      "test x:  (1, 1, 50, 5) y: (1, 4, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "week_num = \"week50\"\n",
    "for cat in [\"train\", \"val\", \"test\"]:\n",
    "    _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "    print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(\"../data/shuffled_data/week50\", \"%s.npz\" % cat),\n",
    "        x=_x,\n",
    "        y=_y\n",
    "        # x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "        # y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference = np.absolute(y_test)\n",
    "difference[difference != difference] = 0\n",
    "mae = difference.mean()\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0662545c6fd30eceb00c46289e22b5a22aef9c4ebb29470f344626a3bc8eec96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
