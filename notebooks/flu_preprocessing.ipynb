{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import us as usStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def days(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%m/%d/%Y\")\n",
    "    d2 = datetime.strptime(d2, \"%m/%d/%Y\")\n",
    "    return abs((d2 - d1).days)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "def weeks(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%m/%d/%Y\")\n",
    "    d2 = datetime.strptime(d2, \"%m/%d/%Y\")\n",
    "    return (abs((d2 - d1).days)+1)//7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "# x,y = shuffle_in_unison(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ground truth for flu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weeks to drop\n",
    "lst_weeks_drop = ['2022-06-18', '2022-06-25','2022-07-02','2022-07-09','2022-07-16', \\\n",
    "    '2022-10-22', '2022-10-29', '2022-11-05', '2022-11-12']\n",
    "# states to drop \n",
    "states_to_drop = ['11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.read_csv('/Users/xiangyikong/Desktop/week73/truth-Incident Hospitalizations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg['date'] = pd.to_datetime(dfg['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_date_filtered = dfg[(dfg['date'] >= '2022-02-26') & (dfg['date'] <= '2022-11-12')]\n",
    "dfg_date_filtered = dfg_date_filtered[~dfg_date_filtered['date'].isin(lst_weeks_drop)]\n",
    "dfg_date_filtered = dfg_date_filtered[~dfg_date_filtered['location'].isin(states_to_drop)]\n",
    "# only get the states that are in the prediction\n",
    "dfg_date_filtered_temp = dfg_date_filtered[dfg_date_filtered['location'].isin(list2_week4)]\n",
    "# only get the dates that are in the prediction\n",
    "dfg_date_filtered_temp = dfg_date_filtered_temp[dfg_date_filtered_temp['date'].isin(list1_week4)]\n",
    "dfg_date_filtered_temp = dfg_date_filtered_temp.sort_values(by=['date', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_groundtruth = np.resize(dfg_date_filtered_temp.groupby(['date','location']).sum().to_numpy().T, (16,51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_groundtruth = flu_groundtruth.reshape(1,16,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022-02-26T00:00:00.000000000', '2022-03-05T00:00:00.000000000',\n",
       "       '2022-03-12T00:00:00.000000000', '2022-03-19T00:00:00.000000000',\n",
       "       '2022-03-26T00:00:00.000000000', '2022-04-02T00:00:00.000000000',\n",
       "       '2022-04-09T00:00:00.000000000', '2022-04-16T00:00:00.000000000',\n",
       "       '2022-04-23T00:00:00.000000000', '2022-04-30T00:00:00.000000000',\n",
       "       '2022-05-07T00:00:00.000000000', '2022-05-14T00:00:00.000000000',\n",
       "       '2022-05-21T00:00:00.000000000', '2022-05-28T00:00:00.000000000',\n",
       "       '2022-06-04T00:00:00.000000000', '2022-06-11T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_date_filtered_temp['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-02-26</th>\n",
       "      <th>01</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-06-11</th>\n",
       "      <th>53</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>2224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     value\n",
       "date       location       \n",
       "2022-02-26 01           14\n",
       "           02            1\n",
       "           04           72\n",
       "           05           42\n",
       "           06           34\n",
       "...                    ...\n",
       "2022-06-11 53           59\n",
       "           54           15\n",
       "           55           15\n",
       "           56            6\n",
       "           US         2224\n",
       "\n",
       "[816 rows x 1 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_date_filtered_temp.groupby(['date','location']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>location_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-02-26</th>\n",
       "      <th>Alabama</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022-10-15</th>\n",
       "      <th>Virginia</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1537 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          value\n",
       "date       location_name       \n",
       "2022-02-26 Alabama           14\n",
       "           Alaska             1\n",
       "           Arizona           72\n",
       "           Arkansas          42\n",
       "           California        34\n",
       "...                         ...\n",
       "2022-10-15 Virginia          52\n",
       "           Washington        17\n",
       "           West Virginia     12\n",
       "           Wisconsin          6\n",
       "           Wyoming            3\n",
       "\n",
       "[1537 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_date_filtered.groupby(['date','location_name']).aggregate('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2022-02-26T00:00:00.000000000', '2022-03-05T00:00:00.000000000',\n",
       "       '2022-03-12T00:00:00.000000000', '2022-03-19T00:00:00.000000000',\n",
       "       '2022-03-26T00:00:00.000000000', '2022-04-02T00:00:00.000000000',\n",
       "       '2022-04-09T00:00:00.000000000', '2022-04-16T00:00:00.000000000',\n",
       "       '2022-04-23T00:00:00.000000000', '2022-04-30T00:00:00.000000000',\n",
       "       '2022-05-07T00:00:00.000000000', '2022-05-14T00:00:00.000000000',\n",
       "       '2022-05-21T00:00:00.000000000', '2022-05-28T00:00:00.000000000',\n",
       "       '2022-06-04T00:00:00.000000000', '2022-06-11T00:00:00.000000000',\n",
       "       '2022-07-23T00:00:00.000000000', '2022-07-30T00:00:00.000000000',\n",
       "       '2022-08-06T00:00:00.000000000', '2022-08-13T00:00:00.000000000',\n",
       "       '2022-08-20T00:00:00.000000000', '2022-08-27T00:00:00.000000000',\n",
       "       '2022-09-03T00:00:00.000000000', '2022-09-10T00:00:00.000000000',\n",
       "       '2022-09-17T00:00:00.000000000', '2022-09-24T00:00:00.000000000',\n",
       "       '2022-10-01T00:00:00.000000000', '2022-10-08T00:00:00.000000000',\n",
       "       '2022-10-15T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_date_filtered['date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flu weekly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022-01-31-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-02-07-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-02-14-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-02-21-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-02-28-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-03-07-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-03-14-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-03-21-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-03-28-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-04-04-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-04-11-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-04-18-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-04-25-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-05-02-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-05-09-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-05-16-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-05-23-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-05-30-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-06-13-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-06-20-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-10-17-MOBS-GLEAM_FLUH.csv\n",
    "# 2022-10-24-MOBS-GLEAM_FLUH.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = '/Users/xiangyikong/Desktop/week73/GLEAM_preds'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "files_name = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        files_name.append(f)\n",
    "        # print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z5/qndjgmp94tgckrl38gcbw_9h0000gn/T/ipykernel_12551/1662951925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweeks_4_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# if df['target'] == '1 wk ahead inc flu hosp':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mweeks_1_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweeks_1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1 wk ahead inc flu hosp'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'point'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weeks_1_df = pd.DataFrame()\n",
    "weeks_2_df = pd.DataFrame()\n",
    "weeks_3_df = pd.DataFrame()\n",
    "weeks_4_df = pd.DataFrame()\n",
    "for i in files_name:\n",
    "    df = pd.read_csv(i)\n",
    "    # if df['target'] == '1 wk ahead inc flu hosp':\n",
    "    weeks_1_df = weeks_1_df.append(df[(df['target'] == '1 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "    weeks_2_df = weeks_2_df.append(df[(df['target'] == '2 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "    weeks_3_df = weeks_3_df.append(df[(df['target'] == '3 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "    weeks_4_df = weeks_4_df.append(df[(df['target'] == '4 wk ahead inc flu hosp') & (df['type'] == 'point')])\n",
    "\n",
    "weeks_1_df = weeks_1_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_2_df = weeks_2_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_3_df = weeks_3_df.drop(['target','type','quantile'],axis = 1)\n",
    "weeks_4_df = weeks_4_df.drop(['target','type','quantile'],axis = 1)\n",
    "\n",
    "\n",
    "weeks_1_df = weeks_1_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_2_df = weeks_2_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_3_df = weeks_3_df.sort_values(by=['target_end_date', 'location'])\n",
    "weeks_4_df = weeks_4_df.sort_values(by=['target_end_date', 'location'])\n",
    "\n",
    "\n",
    "# weeks to drop\n",
    "lst_weeks_drop = ['2022-06-18', '2022-06-25','2022-07-02','2022-07-09','2022-07-16', '2022-10-22', '2022-10-29', '2022-11-05', '2022-11-12', \"2022-11-19\"]\n",
    "# states to drop \n",
    "states_to_drop = ['11']\n",
    "\n",
    "weeks_1_df = weeks_1_df[~weeks_1_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "weeks_2_df = weeks_2_df[~weeks_2_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "weeks_3_df = weeks_3_df[~weeks_3_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "weeks_4_df = weeks_4_df[~weeks_4_df['target_end_date'].isin(lst_weeks_drop)]\n",
    "\n",
    "weeks_1_df = weeks_1_df[~weeks_1_df['location'].isin(states_to_drop)]\n",
    "weeks_2_df = weeks_2_df[~weeks_2_df['location'].isin(states_to_drop)]\n",
    "weeks_3_df = weeks_3_df[~weeks_3_df['location'].isin(states_to_drop)]\n",
    "weeks_4_df = weeks_4_df[~weeks_4_df['location'].isin(states_to_drop)]\n",
    "\n",
    "\n",
    "# 1 week ahead\n",
    "week_1_df_2 = weeks_1_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_1_df_3 = weeks_1_df.drop_duplicates(subset=['location'])\n",
    "list1_week1 = list(week_1_df_2['target_end_date'][3:])\n",
    "list2_week1 = list(week_1_df_3['location'])\n",
    "\n",
    "# 2 week ahead\n",
    "week_2_df_2 = weeks_2_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_2_df_3 = weeks_2_df.drop_duplicates(subset=['location'])\n",
    "list1_week2 = list(week_2_df_2['target_end_date'][2:-1])\n",
    "list2_week2 = list(week_2_df_3['location'])\n",
    "\n",
    "# 3 week ahead\n",
    "week_3_df_2 = weeks_3_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_3_df_3 = weeks_3_df.drop_duplicates(subset=['location'])\n",
    "list1_week3 = list(week_3_df_2['target_end_date'][1:-2])\n",
    "list2_week3 = list(week_3_df_3['location'])\n",
    "\n",
    "\n",
    "# 4 week ahead\n",
    "week_4_df_2 = weeks_4_df.drop_duplicates(subset=['target_end_date'])\n",
    "week_4_df_3 = weeks_4_df.drop_duplicates(subset=['location'])\n",
    "list1_week4 = list(week_4_df_2['target_end_date'][:-3])\n",
    "list2_week4 = list(week_4_df_3['location'])\n",
    "\n",
    "\n",
    "\n",
    "time_length = len(list1_week4)\n",
    "state_length = len(list2_week4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flu Prediction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 51)\n"
     ]
    }
   ],
   "source": [
    "pred_week1 = np.zeros([time_length,state_length])\n",
    "no_value_1_week = {}\n",
    "for id1,i in enumerate(list1_week1):\n",
    "    df4 = weeks_1_df.loc[weeks_1_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week1):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week1[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_1_week.keys():\n",
    "                no_value_1_week[i] = [j]\n",
    "            else:\n",
    "                no_value_1_week[i].append(j)\n",
    "print(pred_week1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 51)\n"
     ]
    }
   ],
   "source": [
    "pred_week2 = np.zeros([time_length,state_length])\n",
    "no_value_2_week = {}\n",
    "for id1,i in enumerate(list1_week2):\n",
    "    df4 = weeks_2_df.loc[weeks_2_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week2):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week2[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_2_week.keys():\n",
    "                no_value_2_week[i] = [j]\n",
    "            else:\n",
    "                no_value_2_week[i].append(j)\n",
    "print(pred_week2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 51)\n"
     ]
    }
   ],
   "source": [
    "pred_week3 = np.zeros([time_length,state_length])\n",
    "no_value_3_week = {}\n",
    "for id1,i in enumerate(list1_week3):\n",
    "    df4 = weeks_3_df.loc[weeks_3_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week3):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week3[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_3_week.keys():\n",
    "                no_value_3_week[i] = [j]\n",
    "            else:\n",
    "                no_value_3_week[i].append(j)\n",
    "print(pred_week3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 51)\n"
     ]
    }
   ],
   "source": [
    "pred_week4 = np.zeros([time_length,state_length])\n",
    "no_value_4_week = {}\n",
    "for id1,i in enumerate(list1_week4):\n",
    "    df4 = weeks_4_df.loc[weeks_4_df['target_end_date'] == i]\n",
    "    for id2,j in enumerate(list2_week4):\n",
    "        df5 = df4.loc[df4['location'] == j]\n",
    "        try:\n",
    "            pred_week4[id1,id2] = df5['value']\n",
    "        except:\n",
    "            # print('error')\n",
    "            if i not in no_value_4_week.keys():\n",
    "                no_value_4_week[i] = [j]\n",
    "            else:\n",
    "                no_value_4_week[i].append(j)\n",
    "print(pred_week4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16, 51)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flu_pred = np.stack([pred_week1,pred_week2,pred_week3,pred_week4],axis=0)\n",
    "flu_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 51, 4)\n"
     ]
    }
   ],
   "source": [
    "flu_residual = flu_pred - flu_groundtruth\n",
    "residual_trans = flu_residual.transpose(1,2,0)\n",
    "print(residual_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.]\n",
      " [ 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.]\n",
      " [ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.]\n",
      " [ 6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.\n",
      "   6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.\n",
      "   6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.]\n",
      " [ 7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.\n",
      "   7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.\n",
      "   7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.]\n",
      " [ 8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "   8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "   8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.]\n",
      " [ 9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n",
      "   9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n",
      "   9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
      " [11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.\n",
      "  11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.\n",
      "  11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.]\n",
      " [12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12.\n",
      "  12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12.\n",
      "  12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12.]\n",
      " [13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13.\n",
      "  13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13.\n",
      "  13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13. 13.]\n",
      " [14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14.\n",
      "  14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14.\n",
      "  14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14.]\n",
      " [15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15.\n",
      "  15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15.\n",
      "  15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "#x data\n",
    "\n",
    "x_num_samples, x_num_nodes = 16, 51\n",
    "x_data = residual_trans\n",
    "# x_data = residual.transpose(1,2,0)\n",
    "\n",
    "x_day = list(range(0,x_num_samples))\n",
    "\n",
    "x_day = np.tile(x_day, [1, x_num_nodes, 1]).transpose((2, 1, 0))\n",
    "x_data = np.concatenate([x_data,x_day], axis=-1)\n",
    "print(x_data[:,:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 51, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(x_data, axis=0)[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 51, 5)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(x_data, axis=0)[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 51, 5)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 and x test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 51, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.stack([residual_trans[1:13,:,0],residual_trans[2:14,:,1],\n",
    "                        residual_trans[3:15,:,2],residual_trans[4:16,:,3]],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 51)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_trans[1:2,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.expand_dims(y_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y1.transpose(0,3,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4, 51, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros([1,4,51,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shuffle, y_shuffle = shuffle_in_unison(x1, y1)\n",
    "\n",
    "num_samples = x1.shape[0]\n",
    "num_val = int(num_samples*0.5)\n",
    "num_train = num_samples-num_val\n",
    "# train\n",
    "x_train, y_train = x_shuffle[:num_train], y_shuffle[:num_train]\n",
    "\n",
    "# val\n",
    "x_val, y_val = (\n",
    "    x_shuffle[num_train: num_train + num_val],\n",
    "    y_shuffle[num_train: num_train + num_val],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x:  (6, 51, 5) y: (6, 4, 51, 1)\n",
      "val x:  (6, 51, 5) y: (6, 4, 51, 1)\n",
      "test x:  (1, 51, 5) y: (1, 4, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "for cat in [\"train\", \"val\", \"test\"]:\n",
    "    _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "    print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(\"week73/\", \"%s.npz\" % cat),\n",
    "        x=_x,\n",
    "        y=_y\n",
    "        # x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "        # y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 51, 5)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4, 51, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0662545c6fd30eceb00c46289e22b5a22aef9c4ebb29470f344626a3bc8eec96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
