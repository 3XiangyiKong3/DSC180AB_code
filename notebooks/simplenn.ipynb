{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_in_the_prediction = ['01','02','04','05','06','08','09','10','12','13','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40',\n",
    " '41','42','44','45','46','47','48','49','50','51','53','54','55','56']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_df = pd.read_csv(\"../data/CDC/truth-Incident Hospitalizations.csv\")\n",
    "truth_df = truth_df[truth_df['date'] >= '2022-01-01']\n",
    "truth_df = truth_df[truth_df['location'] != 'US']\n",
    "truth_df = truth_df[truth_df['location'].isin(states_in_the_prediction)]\n",
    "truth_df.sort_values(by=['date', 'location'], inplace=True)\n",
    "unique_dates = truth_df['date'].unique()\n",
    "unique_states = truth_df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardize the data individually for each state\n",
    "mean_arr = []\n",
    "std_arr = []\n",
    "for state in unique_states:\n",
    "    state_values = truth_df[truth_df['location'] == state]['value'].values\n",
    "    mean = state_values.mean()\n",
    "    mean_arr.append(mean)\n",
    "    std = state_values.std()\n",
    "    std_arr.append(std)\n",
    "    truth_df.loc[truth_df['location'] == state, 'norm_value'] = (state_values - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = np.zeros([len(unique_dates),len(unique_states)])\n",
    "for id1,i in enumerate(unique_dates):\n",
    "    for id2,j in enumerate(unique_states):\n",
    "        weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['value'].values\n",
    "        # weeks[id1,id2] = truth_df[(truth_df['date']==i) & (truth_df['location']==j)]['norm_value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 50)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and validation sets\n",
    "loader_temp = weeks.copy().transpose(1,0)\n",
    "train_data = loader_temp[:,:-2].copy() #train_data = loader_temp[:,:-4].copy()\n",
    "train_data = np.concatenate([train_data[:,i:i+10] for i in range(0, 46, 1)], axis = 0)\n",
    "np.random.shuffle(train_data)\n",
    "val_data = train_data[:500]\n",
    "train_data = train_data[500:]\n",
    "test_data = loader_temp[:,-8:].copy() # test_data = loader_temp[:,-10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to PyTorch tensors\n",
    "# train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "# train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "# val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "# test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "# test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "train_inputs = torch.tensor(train_data[:, :6], dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "val_inputs = torch.tensor(val_data[:, :6], dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "test_inputs = torch.tensor(test_data[:, :6], dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_data[:, 6:], dtype=torch.float32)\n",
    "\n",
    "mean = train_inputs.mean()\n",
    "std = train_inputs.std()\n",
    "\n",
    "train_inputs = (train_inputs - mean) / std\n",
    "val_inputs = (val_inputs - mean) / std\n",
    "train_labels = (train_labels - mean) / std\n",
    "val_labels = (val_labels - mean) / std\n",
    "test_inputs = (test_inputs  - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
    "val_dataset =torch.utils.data.TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "# create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMLP(nn.Module):\n",
    "    def __init__(self,input_length, output_length,hidden_length):\n",
    "        super(AutoMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, hidden_length),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_length, output_length),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoMLP(6, 4, 256).to(device) # 8, 16, 32, 64, 128, 256\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.9) # stepwise learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 0.138 Validation loss : 0.499\n",
      "Epoch 11 Training loss: 0.137 Validation loss : 0.492\n",
      "Epoch 21 Training loss: 0.135 Validation loss : 0.504\n",
      "Epoch 31 Training loss: 0.134 Validation loss : 0.494\n",
      "Epoch 41 Training loss: 0.134 Validation loss : 0.499\n",
      "Epoch 51 Training loss: 0.133 Validation loss : 0.513\n",
      "Epoch 61 Training loss: 0.132 Validation loss : 0.509\n",
      "Epoch 71 Training loss: 0.132 Validation loss : 0.513\n",
      "Epoch 81 Training loss: 0.131 Validation loss : 0.515\n",
      "Epoch 91 Training loss: 0.130 Validation loss : 0.509\n",
      "Epoch 101 Training loss: 0.130 Validation loss : 0.508\n",
      "Epoch 111 Training loss: 0.130 Validation loss : 0.509\n",
      "Epoch 121 Training loss: 0.129 Validation loss : 0.512\n",
      "Epoch 131 Training loss: 0.129 Validation loss : 0.509\n",
      "Epoch 141 Training loss: 0.129 Validation loss : 0.511\n",
      "Epoch 151 Training loss: 0.128 Validation loss : 0.511\n",
      "Epoch 161 Training loss: 0.128 Validation loss : 0.509\n",
      "Epoch 171 Training loss: 0.128 Validation loss : 0.513\n",
      "Epoch 181 Training loss: 0.128 Validation loss : 0.513\n",
      "Epoch 191 Training loss: 0.128 Validation loss : 0.514\n",
      "Epoch 201 Training loss: 0.127 Validation loss : 0.513\n",
      "Epoch 211 Training loss: 0.127 Validation loss : 0.513\n",
      "Epoch 221 Training loss: 0.127 Validation loss : 0.514\n",
      "Epoch 231 Training loss: 0.127 Validation loss : 0.512\n",
      "Epoch 241 Training loss: 0.127 Validation loss : 0.513\n",
      "Epoch 251 Training loss: 0.127 Validation loss : 0.514\n",
      "Epoch 261 Training loss: 0.127 Validation loss : 0.514\n",
      "Epoch 271 Training loss: 0.127 Validation loss : 0.514\n",
      "Epoch 281 Training loss: 0.127 Validation loss : 0.514\n",
      "Epoch 291 Training loss: 0.127 Validation loss : 0.515\n"
     ]
    }
   ],
   "source": [
    "best_val = 1e6\n",
    "for epoch in range(300):\n",
    "    running_loss = []\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    \n",
    "    # validate the model after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = []\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss.append(loss.item())\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d Training loss: %.3f Validation loss : %.3f' % (epoch + 1, np.mean(running_loss),  np.mean(val_running_loss)))\n",
    "    scheduler.step()\n",
    "    if np.mean(val_running_loss) < best_val:\n",
    "        best_val = np.mean(val_running_loss)\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.904602546691894"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryput_1 = (best_model(test_inputs.to(device))* std + mean)\n",
    "np.mean(np.abs(np.abs([item for sublist in tryput_1.detach().numpy() for item in sublist]) - np.array(truth_df[truth_df['date'] == '2023-01-28']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = best_model(test_inputs.to(device))[:,np.array([False, False, False,True,])]\n",
    "# result = []\n",
    "# for i in range(len(pred)):\n",
    "#     result.append(((pred[i] * std_arr[i])+ mean_arr[i]).tolist())\n",
    "# flat_list = [item for sublist in result for item in sublist]\n",
    "# p.mean(np.abs(flat_list - np.array(truth_df[truth_df['date'] == '2023-02-04']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.660641059875488"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "temp = test_preds[:,np.array([True, False, False,False,])]\n",
    "# flat_list = [item for sublist in temp for item in sublist]\n",
    "# flat_list.to_list()\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "np.mean(np.abs(np.abs(flat_list) - np.array(truth_df[truth_df['date'] == '2023-01-28']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.372677421569826"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "temp = test_preds[:,np.array([False, True, False,False,])]\n",
    "# flat_list = [item for sublist in temp for item in sublist]\n",
    "# flat_list.to_list()\n",
    "flat_list = [item for sublist in temp.detach().numpy() for item in sublist]\n",
    "np.mean(np.abs(np.abs(flat_list) - np.array(truth_df[truth_df['date'] == '2023-02-04']['value'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29.886135  ,  10.501411  ,   9.740902  ,  12.0020485 ,\n",
       "         2.3397179 ,  32.494225  ,   0.6734886 ,   4.4903793 ,\n",
       "       409.8579    ,  41.751476  ,   2.7727165 ,   2.7423897 ,\n",
       "        11.769562  ,   3.4826698 ,  10.590172  ,   6.6613884 ,\n",
       "        29.887796  ,  38.588455  ,   1.9614639 ,   9.76577   ,\n",
       "         3.7868843 ,  23.743057  ,   0.8619957 ,  40.73269   ,\n",
       "        35.113327  ,   6.3290825 ,  26.505194  ,   5.2713547 ,\n",
       "        28.366827  ,   6.78788   ,   2.1146393 , 100.33423   ,\n",
       "         2.4374504 ,  11.983963  ,  26.507687  ,  12.212769  ,\n",
       "        59.106144  , 110.23315   ,   3.5222511 ,  31.665165  ,\n",
       "        10.325104  ,  27.583416  , 212.01239   ,  11.617561  ,\n",
       "        13.109932  ,  13.721832  ,  54.408634  ,  22.691063  ,\n",
       "         1.3688316 ,   0.45460892], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58,  10, 109,  61, 198,  33,  31,   5, 522,  72,   0,  30,  62,\n",
       "        96,  43,  41,  30,  70,  31,  61, 122, 154,  25,  53,  94,  12,\n",
       "        57,  21,  20, 102,  30, 243,  60,  28, 107, 139,  25, 285,  14,\n",
       "        60,  11,  76, 454,  17,  11,  64,  80,  13,  95,  11])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(truth_df[truth_df['date'] == '2023-01-21']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.1138649 ,   7.50141144,  38.25909805,  17.99795151,\n",
       "       112.66028214,   2.49422455,  31.32651138,   0.50962067,\n",
       "        50.85791016,  26.24852371,   8.22728348,  14.25761032,\n",
       "        49.23043823,  62.51733017,  34.40982819,  10.3386116 ,\n",
       "         0.8877964 ,   3.4115448 ,   9.03853607,  27.23423004,\n",
       "        67.21311569,  62.25694275,  14.1380043 ,  10.7326889 ,\n",
       "        32.88667297,   3.67091751,   4.50519371,   5.72864532,\n",
       "         8.36682701,  63.21212006,  14.88536072,  53.66577148,\n",
       "        32.56254959,  10.01603699,  40.49231339,  83.78723145,\n",
       "        49.10614395,  27.76685333,   6.47774887,   2.33483505,\n",
       "         5.67489624,  10.41658401, 116.98760986,   6.61756134,\n",
       "         7.10993195,  39.27816772,   1.59136581,   3.69106293,\n",
       "        42.63116837,   4.54539108])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(np.abs(flat_list) - np.array(truth_df[truth_df['date'] == '2023-01-28']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z5/qndjgmp94tgckrl38gcbw_9h0000gn/T/ipykernel_40784/2859540066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "test_preds = best_model(test_inputs.to(device)) * std + mean\n",
    "print(\"test error:\", torch.mean(torch.abs(test_preds.cpu() - test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLEAM_01_23_pd = pd.read_csv(\"../data/GLEAM/2023-01-23-MOBS-GLEAM_FLUH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLEAM_02_04_pred = GLEAM_01_23_pd[(GLEAM_01_23_pd['target'] == '2 wk ahead inc flu hosp') & (GLEAM_01_23_pd['quantile'] == 0.5) & (GLEAM_01_23_pd['location'].isin(states_in_the_prediction))][['location','value']]['value'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportions = [.90, .10]\n",
    "# lengths = [int(p * len(train_loader)) for p in proportions]\n",
    "# lengths[-1] = len(train_loader) - sum(lengths[:-1])\n",
    "# tr_dataset, vl_dataset = torch.utils.data.random_split(train_loader, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AutoMLP(nn.Module):\n",
    "#     def __init__(self,input_length, hidden_length, output_length):\n",
    "#         super(AutoMLP, self).__init__()\n",
    "#         self.input_length = input_length\n",
    "#         self.hidden_length = hidden_length\n",
    "#         self.output_length = output_length\n",
    "#         self.fc1 = nn.Linear(self.input_length, self.hidden_length)\n",
    "#         self.fc2 = nn.Linear(self.hidden_length, self.output_length)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.Softmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0662545c6fd30eceb00c46289e22b5a22aef9c4ebb29470f344626a3bc8eec96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
